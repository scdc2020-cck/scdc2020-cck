{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython = get_ipython()\n",
    "\n",
    "def hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n",
    "                      exception_only=False, running_compiled_code=False):\n",
    "       etype, value, tb = sys.exc_info()\n",
    "       return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
    "\n",
    "ipython.showtraceback = hide_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "master_path = os.path.join(os.getcwd(), '데이터SET', f\"{'[Track1_데이터2] samp_train'}.csv\")\n",
    "master = pd.read_csv(master_path)\n",
    "master.MRC_ID_DI[master.MRC_ID_DI > 0 ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_0 = master[master['MRC_ID_DI'] == 0].sample(frac=1)\n",
    "master_1 = master[master['MRC_ID_DI'] == 1].sample(frac=1)\n",
    "\n",
    "sample_size = len(master_0) if len(master_0) < len(master_1) else len(master_1)\n",
    "\n",
    "master = pd.concat([master_0.head(sample_size), master_1.head(sample_size)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img_path = os.path.join(os.getcwd(), 'image')\n",
    "\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "for i in range(len(master)):\n",
    "    path = os.path.join(img_path, str(master.iloc[i, 0]) +'.png')\n",
    "    label = master.iloc[i, 1]\n",
    "    img = Image.open(path)\n",
    "    data = np.asarray(img)\n",
    "    X.append(data)\n",
    "    y.append(label)\n",
    "    \n",
    "    for ang in range(-20, 20, 5):\n",
    "        if ang != 0:\n",
    "            img2 = img.rotate(ang)\n",
    "            data = np.asarray(img2)\n",
    "            X.append(data)\n",
    "            y.append(label)\n",
    "        \n",
    "        #img2 = img2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        #data = np.asarray(img2)\n",
    "        #X.append(data)\n",
    "        #y.append(label)\n",
    "        \n",
    "X = np.array(X)       \n",
    "y = np.array(y)\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "img_path = os.path.join(os.getcwd(), 'image')\n",
    "\n",
    "y = []\n",
    "X_paths = []\n",
    "\n",
    "for i in range(len(master)):\n",
    "    X_paths.append( os.path.join(img_path, str(master.iloc[i, 0]) +'.png') )\n",
    "    y.append(master.iloc[i, 1])\n",
    "\n",
    "y = np.array(y)\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(X_paths, y, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train_paths, X_val_paths, y_train, y_val = train_test_split(X_train_paths, y_train, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train = []\n",
    "for file_path in X_train_paths:\n",
    "    #read image\n",
    "    img = cv2.imread(file_path)\n",
    "    X_train.append(img)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_val = []\n",
    "for file_path in X_val_paths:\n",
    "    #read image\n",
    "    img = cv2.imread(file_path)\n",
    "    X_val.append(img)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "X_test = []\n",
    "for file_path in X_test_paths:\n",
    "    #read image\n",
    "    img = cv2.imread(file_path)\n",
    "    X_test.append(img)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols, img_channel = 224, 224, 3\n",
    "\n",
    "base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, img_channel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 111, 111, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 111, 111, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 109, 109, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 109, 109, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 54, 54, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 54, 54, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 52, 52, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 25, 25, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 25, 25, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 25, 25, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 25, 25, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 25, 25, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 25, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 25, 25, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 25, 25, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 25, 25, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 25, 25, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 25, 25, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 25, 25, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 25, 25, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 25, 25, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 25, 25, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 25, 25, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 25, 25, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 25, 25, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 25, 25, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 25, 25, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 25, 25, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 25, 25, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 25, 25, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 25, 25, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 25, 25, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 25, 25, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 25, 25, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 25, 25, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 25, 25, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 25, 25, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 12, 12, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 12, 12, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 12, 12, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 12, 12, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 12, 12, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 12, 12, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 12, 12, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 12, 12, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 12, 12, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 12, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 12, 12, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 12, 12, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 12, 12, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 12, 12, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 12, 12, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 12, 12, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 12, 12, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 12, 12, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 12, 12, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 12, 12, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 12, 12, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 12, 12, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 12, 12, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 12, 12, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 12, 12, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 12, 12, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 12, 12, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 12, 12, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 12, 12, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 12, 12, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 12, 12, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 12, 12, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 12, 12, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 12, 12, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 12, 12, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 12, 12, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 12, 12, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 12, 12, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 12, 12, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 12, 12, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 12, 12, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 12, 12, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 12, 12, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 12, 12, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 5, 5, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 5, 5, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 5, 5, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 5, 5, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 5, 5, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 5, 5, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 5, 5, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 5, 5, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 5, 5, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 5, 5, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 5, 5, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 5, 5, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 5, 5, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 5, 5, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 5, 5, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 5, 5, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 5, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 5, 5, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 5, 5, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 5, 5, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 5, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 5, 5, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 5, 5, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 5, 5, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 5, 5, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 5, 5, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 5, 5, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 2)            102402      mixed10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,905,186\n",
      "Trainable params: 21,870,754\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "add_model = tf.keras.Sequential()\n",
    "add_model.add(tf.keras.layers.Flatten(input_shape=base_model.output_shape[1:]))\n",
    "add_model.add(tf.keras.layers.Dropout(rate = 0.5))\n",
    "#add_model.add(tf.keras.layers.Dropout(rate = 0.8))\n",
    "#add_model.add(tf.keras.layers.Dense(units=8, activation=tf.nn.relu))\n",
    "add_model.add(tf.keras.layers.Dense(units=2, activation=tf.nn.softmax))\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0000001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-105d894279b4>:27: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 12467 steps, validate for 1386 steps\n",
      "Epoch 1/2000\n",
      "12467/12467 [==============================] - 613s 49ms/step - loss: 0.9083 - accuracy: 0.5132 - val_loss: 0.8386 - val_accuracy: 0.5186oss: 0. - ETA: 39s - loss: 0.9087 - accuracy: 0.513 - ETA: 39s - loss: 0.9088 - accuracy: 0.513 - ETA: 39s - loss: 0.9087 - ETA: 37s - loss: 0.9083 - - ETA: 36s  - ETA: 33s - loss: 0.9079 - ETA: 6s - los - ETA: 4s - loss: 0.9078 - \n",
      "Epoch 2/2000\n",
      "12467/12467 [==============================] - 597s 48ms/step - loss: 0.8430 - accuracy: 0.5488 - val_loss: 0.8515 - val_accuracy: 0.5189\n",
      "Epoch 3/2000\n",
      "12467/12467 [==============================] - 598s 48ms/step - loss: 0.7856 - accuracy: 0.5781 - val_loss: 0.8450 - val_accuracy: 0.5200 - - ETA: 7s - loss: 0.7855  - ETA: 4s - loss: 0.7854 - accuracy:  - ETA: 0s - loss: 0.7856 - accu\n",
      "Epoch 4/2000\n",
      "12467/12467 [==============================] - 597s 48ms/step - loss: 0.7376 - accuracy: 0.6076 - val_loss: 0.8327 - val_accuracy: 0.5337\n",
      "Epoch 5/2000\n",
      "12467/12467 [==============================] - 599s 48ms/step - loss: 0.6905 - accuracy: 0.6368 - val_loss: 0.8018 - val_accuracy: 0.5402- loss - ETA: 34s - ETA: 31s - loss: 0.6914  - ETA\n",
      "Epoch 6/2000\n",
      "12467/12467 [==============================] - 600s 48ms/step - loss: 0.6498 - accuracy: 0.6591 - val_loss: 0.8219 - val_accuracy: 0.5319 - ac - ETA: 55s - l - ETA: 44s - loss: 0.6507 - accuracy:  - ETA: 43s - loss: 0.6509 - accuracy: 0.65 - ETA: 43s - loss: 0.6509 - accurac - ETA: 42s - loss: 0.6509 - accurac - ETA: 41s - loss: 0.6508 - accuracy: - ETA: 41s - loss: 0.6510 - accuracy: 0.65 - ETA: 40s - loss: 0. - ETA: 38s - loss - ETA: 36s - loss: 0.6506 - a - ETA: 35s - loss: 0.6506 - accuracy - ETA: 34s - loss: 0.650 - ETA: 32s - loss: 0.6503 - - ETA: 26s - loss: 0.6498 - ac - ETA: 25s - loss: 0.6499 - accurac - ETA: 24s  - ETA: 21s - loss: 0.6494 - accu - ETA: 20s - loss: 0.6496 - accuracy:  - E - ETA - ETA: 12s - loss: 0.6496 - accuracy: 0. - ETA: 12s - loss: 0.6496 - accu - ETA: 11s - loss: 0.6496 - accuracy: 0 -  - - ETA: 2s - loss: 0.6498 - \n",
      "Epoch 7/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.6127 - accuracy: 0.6863 - val_loss: 0.8239 - val_accuracy: 0.5366ETA: 31s - loss: 0.6147 -  - ETA: 26s - loss: 0.6147 - - ETA: 20s - loss: 0.6142  - ETA: 19s - loss: 0.6142 - - ETA: 17s - loss: 0.6141 - accuracy: - E - ETA: 1s\n",
      "Epoch 8/2000\n",
      "12467/12467 [==============================] - 605s 49ms/step - loss: 0.5743 - accuracy: 0.7077 - val_loss: 0.8142 - val_accuracy: 0.5435- loss - E - ETA: 30s - loss: 0.5746 - accuracy - ETA - ETA: 25s - loss: 0.5744 - ac - ETA: 20s - loss: 0.574 - - ETA:  - ETA: 0s - loss: 0.5743 - \n",
      "Epoch 9/2000\n",
      "12467/12467 [==============================] - 605s 49ms/step - loss: 0.5384 - accuracy: 0.7319 - val_loss: 0.8123 - val_accuracy: 0.5561 51s - loss: 0.5392 - accurac - ETA:  - ETA: 47s - loss: 0.5389 - accuracy: 0 - ETA: 46s - loss: 0.5389 - accuracy:  - ETA: 46s - loss:  - ETA: 31s - loss: 0.5381 - accu - ETA: 30s - loss: 0.5380 - accu - E - ETA: 25 - ETA: - ETA: 18s - lo  - ETA: 1s - loss: 0\n",
      "Epoch 10/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.5116 - accuracy: 0.7470 - val_loss: 0.8094 - val_accuracy: 0.5568y: 0. - ETA: 1s - los\n",
      "Epoch 11/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.4825 - accuracy: 0.7671 - val_loss: 0.8520 - val_accuracy: 0.5576- ETA: 1:03 - ETA: 1:02 - loss: 0.4835 - accurac - ETA: 55s - loss: 0.4841 - accuracy: 0.766 -  - ETA: 39s -  - ETA: 28s - loss: 0.4829 - - ETA: 2s - loss: 0.4821 - accura - ETA: 1s - loss: 0.4821 - accuracy:  - ETA: 1s - l\n",
      "Epoch 12/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.4558 - accuracy: 0.7860 - val_loss: 0.7966 - val_accuracy: 0.5633 loss: 0.4574 - accuracy: - ETA: 38s - loss: 0.45 - ETA: 36s - loss: 0.45 - ETA: 30s - loss: 0.457 - ETA: 28s - loss: 0.4572 - accuracy: 0.7 - ETA: 28s - loss: 0.457 - ETA: 10s - loss: 0.4565 - ac - ETA: 9s\n",
      "Epoch 13/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.4345 - accuracy: 0.7973 - val_loss: 0.8264 - val_accuracy: 0.5460- loss: 0.4336 - ac - ETA: 1:32 - los - ETA - ETA: 1:27 - loss: 0.433 - ETA: 1:26 - loss: 0.4336 - ac - ETA: 1:21 - loss: 0.4340 - accuracy:  - ETA: 1:21 - loss: 0.4341 - accu - ETA - ETA: 48s - loss: 0.4344 - accuracy: - ETA: 35s -  - ETA: 32s - loss: 0.4345 - accuracy: 0.797 - ETA: 32s - loss: 0.4346 - accuracy: 0.797 - ETA: 32s - lo - ETA: 30s - loss: 0.4349 - ac - ETA: 24s  - ETA: 21s - - ETA: 18s - loss: 0.4345 - - ETA: 17s - loss: 0.4 - ETA: 15s - loss: 0.4341 - accuracy: 0.797 - ETA: 15s - loss: 0.4340 - accuracy: 0. - ETA: 14s - loss: 0.4340 - accuracy - ETA: 14s - los - ETA: 11s - loss: 0.4 - ETA: 9s - los - ETA: 6s - l - ETA: 5s - l - ETA: 1s - loss: 0.434 - ETA: 0s - loss: 0.434\n",
      "Epoch 14/2000\n",
      "12467/12467 [==============================] - 604s 48ms/step - loss: 0.4091 - accuracy: 0.8152 - val_loss: 0.8206 - val_accuracy: 0.5550loss: 0.40 - ETA: 48s - loss: 0.4087 - accu - ETA: 47s - loss: 0.4088 - accuracy: - ETA: 46s - loss: 0.4087 - accuracy:  - ETA: 46s - loss: 0. - ETA: 44s - loss: 0.4084 - accura - ETA: 43s - loss: 0.4085 - accuracy: 0.8 - ETA: 30s - loss: 0.40 - ETA: 28s - loss: 0.4082 -  - ETA: 23s - loss: 0.4085 - accura - ETA: 22s - loss: 0.40 - ETA: 20s - loss: 0.4087 - accuracy: 0.81 - ETA: 20s - loss: 0.4088 - accuracy:  - ETA: 19s - loss - ETA: 17s - loss: 0.4090 - accura - ETA: 16s - loss: 0.4090 - acc - ETA: 15s - loss: 0.4090 - accuracy: 0.81 - ETA: 14s - loss: 0.4090 - accuracy:  - ETA: 10s - loss: 0.4089 - accuracy: - ETA: 7s - loss: 0.4087 - ac - ETA: 7s - loss: 0.4089 - accuracy: 0. - ETA: 6s - loss: 0.4 - ETA: 6s - loss: 0.4089 -  - ETA: 1s - los\n",
      "Epoch 15/2000\n",
      "12467/12467 [==============================] - 604s 48ms/step - loss: 0.3846 - accuracy: 0.8282 - val_loss: 0.8057 - val_accuracy: 0.549654 - accura - ETA: 31s - loss: 0.3848 - accurac - ETA: 30s - loss: 0.3849 - accura - ETA: 29s - loss: 0.3849  - ETA: 28s - loss: 0.3849 - accura - ETA: 27s - loss: 0.3849 - accuracy: 0.828 - ETA: 27s - loss: 0.3849 - accuracy - ETA: 26s - loss: 0.3850 - accuracy: 0.828 - ETA: 26s - loss: 0.3850 - accur - ETA: 3s -\n",
      "Epoch 16/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.3584 - accuracy: 0.8480 - val_loss: 0.8351 - val_accuracy: 0.5550l - ETA: 1:25 - loss: 0.3 - ETA: 1:22 - loss: 0.3584 - ac - ETA: 1:21 - loss: 0.3586 - ac - ETA: 1:19 - loss: 0.3584 - accura - ETA: 1:18 - - ETA: 1:17 - loss: 0.358 - ETA: 1:16 - loss: 0.3583 - ac - ETA: 1:15 - loss: 0.3584  - ETA: 1:14 - loss: 0.3585 - accuracy - ETA - ETA: 1:12 - loss: 0.3583 - ac - ETA: 1:12 - loss: 0.3 - ETA: 1:11 - ETA: 1:09 - ETA: 1:07 - loss: 0.3582  - ETA: 1:07 - los - ETA: 1:05 - los - ETA: 1:04 - l - ETA: 58s - loss: 0.35 - ETA: 56s - loss: 0 - ETA: 53s - loss: - ETA: 51s - loss: 0.3574 - accu - ETA: 50s - loss: 0.3574 - acc - ETA: 45s -  - ETA: 42s - loss: 0.3568 - accura - ETA: 41s - loss: 0.3568 - accuracy: 0.8 - ETA: 41s - loss: 0.3568 - ac - ETA: 39s - loss: 0.3566 - accuracy: 0.848 - ETA: 39s - loss: 0.3567  - ETA: 38s - lo - ETA: 35s - loss: 0.3569 -  - ETA: 34s - loss: 0.3569 - accuracy: 0.849 - ETA: 33s - loss: 0. - ETA: 31s - loss: 0.356 - ETA: 30s - loss: 0. - ETA: 27s - loss: 0.3572 - accuracy: 0.84 - ETA: 27s - loss: 0.3572 -  - ETA: 26s -  - ETA: 0s - loss: 0.3584 - accuracy\n",
      "Epoch 17/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12467/12467 [==============================] - 606s 49ms/step - loss: 0.3360 - accuracy: 0.8625 - val_loss: 0.8257 - val_accuracy: 0.5583 2:12 - ETA: 2:11 - loss: 0.337 - ETA: 2:08 - loss: 0 - ETA: 2:07 - ETA: 2: - ETA: 1:55 - l - ETA - ETA: 1:04 - loss: 0.3371 - accuracy: 0.86 - ETA: 1:04 - loss: 0.3 - ETA: 1:03 - loss: 0.3370 - accura - ETA: 1:02 - loss: - ETA:  - ETA: 51s - loss: 0.3371 - a - ET -\n",
      "Epoch 18/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.3147 - accuracy: 0.8752 - val_loss: 0.8278 - val_accuracy: 0.5630 - ETA: 49s - loss: - ETA: 47s - loss: 0. - ETA:  - ETA:  - ETA: 22s - loss: 0.31 - ETA: 16s - loss: 0\n",
      "Epoch 19/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.2988 - accuracy: 0.8840 - val_loss: 0.8137 - val_accuracy: 0.5586 32s - loss: 0.2998 - accuracy - ETA: 31s - loss: 0.2998 - accuracy:  - ETA: 31s  - ETA: 24s - loss: 0.2998 - accuracy: - ETA: 23s - loss: 0 - ETA: 21s - loss: 0.2996 - a - ETA: 19s - loss: 0.2996 -\n",
      "Epoch 20/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.2801 - accuracy: 0.8947 - val_loss: 0.8510 - val_accuracy: 0.5489a - ETA: 41s - loss: 0.2813 - a - ETA: 40s - loss: 0.281 - ETA: 22s - loss: 0.2810 - accuracy: 0.894 - E - ETA: 18s - l - ETA: 11s - loss: 0.2804 - accuracy:  - ETA: 11s - loss: 0.2804 - accuracy:  - ETA: 10s - loss: 0.2803 - accu - ETA: 9s - loss: 0 - ETA: 8s - loss: 0.2803  - ETA: 7s - loss: 0.2803 - ac - ETA: 7s - ETA: 5s - loss: 0.280 - ETA: 4s - loss: 0.2804 - accuracy:  - ETA: 4s - l - ETA: 3s - loss: 0.2801 - accuracy: 0. - ETA: 2s - loss: 0.2\n",
      "Epoch 21/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.2590 - accuracy: 0.9080 - val_loss: 0.8527 - val_accuracy: 0.559790  - ETA: 1: - ETA: 1:03 - loss: - ETA: 1:02 - ETA: 1:00 - loss: 0 - ETA: 58s - loss:  - E - ETA: 48s - loss: 0.2589 - accur - ETA: 47s - loss: 0.2591 - accuracy: 0.907 - ETA: 47s - loss: 0.2591 - a - ETA: 46 - ETA - ETA: 36 - ETA: 32s - loss - ETA: 30s - loss: 0.2591 - accuracy: - ETA: - ETA: 22s - loss: 0.2590 - accuracy: 0.908 - ETA: 22s - loss: 0.2590 - accuracy: 0.9 - ETA: 21s - ETA: 18s - loss: 0.2588  - ETA: 17s - \n",
      "Epoch 22/2000\n",
      "12467/12467 [==============================] - 600s 48ms/step - loss: 0.2403 - accuracy: 0.9191 - val_loss: 0.8441 - val_accuracy: 0.5619 8:58 - loss: 0.2385 - accuracy: 0. - ETA: 8:57 - ETA: 8:56 - loss: 0.2400 - accura - ETA: 8: - ETA: 8:53 - loss: 0.2421 -  - ETA: 8:52 - loss: 0 - ETA: 8:49 - loss: 0.2405 - accuracy: 0. - ETA: 8:49 - loss: 0.240 - ETA: 7:52 - loss: 0.2403 - accu - E - ETA: 7:28 - loss: 0 - ETA: 7:27 - loss: 0.2 - ETA: 7:13 - loss: 0.2406 - accuracy: 0. - ETA: 7:13 - loss: - ETA: 7:12 - los - ETA: 6:32 - loss: 0.2410 - accuracy: 0.91 - ETA: 6:32 - loss: 0.2409 - accuracy: 0. - ETA: 6:32 - loss: 0.2410 - accu - ETA: 6:31 - loss: 0.2414 -  - ETA - ETA: 6:29 - loss: 0.2 - ETA: 6:23 - loss: 0.2409 - ac - ETA: 6:21 - loss: 0.2 - ETA: 6: - ETA: 6:16 - loss: 0.2409 - accu - ETA: 6:15 - ETA: 6:12 - loss: 0.240 - ETA: 6:11 - loss: 0.2410 - accura - ETA: 6:10 - loss: 0.2407 - ac - ETA: 6:08 - loss: 0.2410 -  - ETA: 6:07 - loss: 0.2 - ETA: 6:06 - loss: 0.2411 - accuracy:  - ETA: 6:06 - loss: 0.2411 - ac - ETA: 6:05 - loss: 0.2411 - accuracy: 0.91 - ETA: 6:05 - ETA: 6:03 - loss: - ETA: 6: - ETA: 5:29 - loss: 0.2415 - accura - ETA: 5:28 - loss: 0.2416  - ETA:  - ETA: 5:22 - loss: 0 - ETA:  - ETA: 5:19 - loss: 0.2427 - accuracy: 0. - ETA: 5:19 - - ETA: 5:17 - loss: 0.2425 -  - ETA: 5:16 - loss: - ETA: 5:03 - loss: 0.2415  - ETA: 4:24 - loss: 0.2414  - ETA: 4:23 - loss: 0.2415 - accura - ETA: 4:23 - ETA: 4:21 - loss: 0.241 - ETA: 4:20 - l - ETA - ETA: 4:15 - loss: 0.2414 -  - - ETA: 4:12 - loss: 0.2 - ETA: 4:11 - loss: 0.2419 - accuracy: 0.91 - ETA: 4:11 - loss: 0.2419 - accuracy: 0. - ETA: 4:11 - loss: 0.2419 - accuracy: 0. - - ETA: 3:54 - loss: 0 - ETA: 3:53 - loss: 0.2408  - ETA - ETA: 3:30 - l - ETA: 3:22 - loss: - ETA: 2:41 - loss: 0.2405 - accuracy:  - - ETA: 2:16 - l - ETA: 2: - ETA: 2:11 - loss: 0.2398 - accuracy: 0. - ETA: 2:10 - loss: - ETA: 1:27 - loss: 0.2400 - accura - ETA - ETA: 1:08 - loss: 0 - ETA:  - ETA: 1:01 - loss: 0.2402 - accura - ETA: 1: - ETA: 59s - loss: 0.2402 - accuracy: 0.91 - ETA: 59s - loss: 0.2402 -  - ETA: 57s - loss: 0.2402 - accuracy: 0.9 - ETA: 57s - ETA: \n",
      "Epoch 23/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.2229 - accuracy: 0.9284 - val_loss: 0.8652 - val_accuracy: 0.5586- ETA: 1:18 - loss: 0.2217 -  - ETA: 1:08 - l - ETA: 1:05 - loss: 0.221 - ETA:  - ETA: 1:00 - loss:  - ETA: 59s - loss: 0.2223 - accurac - ETA: 58s - loss: 0.2222 - accurac - ETA: 0s - loss: 0.2\n",
      "Epoch 24/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.2064 - accuracy: 0.9373 - val_loss: 0.8864 - val_accuracy: 0.5586 loss: 0.2067 - accuracy:  - ETA: 57s - - ETA: 54s - loss: 0.2066 - accuracy:  - ETA: 54s - loss: 0.2067 - accuracy: 0. - ETA: 53s - loss: 0.2067 - accur  - ETA: 44s - l - ETA: 42s - loss: 0.2063 - - ETA: 40s - - ETA: 37s - loss: 0.2065 -  - ETA: 36s - loss: 0 - ETA: 33s - loss: 0.2065 - accuracy - ETA: 33s - loss: 0.2064 - accur - ETA: 32s - loss: 0.2063 - accuracy:  - ETA: 31s - loss: 0 - ETA: 25s - loss: 0.2062 - accuracy: 0.93 - ETA: 25s - loss: 0.2062 - accu - ETA: 24s - loss: 0.2063 - accuracy - ETA: 23s - loss: 0.2063 - accurac - ETA: 22s - loss: 0 - ETA: 20s - loss: 0. - ETA: 18s - loss: - ETA: 0s - loss: 0.2064 - \n",
      "Epoch 25/2000\n",
      "12467/12467 [==============================] - 608s 49ms/step - loss: 0.1913 - accuracy: 0.9447 - val_loss: 0.8801 - val_accuracy: 0.5727\n",
      "Epoch 26/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.1780 - accuracy: 0.9523 - val_loss: 0.9113 - val_accuracy: 0.5601A: 1:36 - los - ETA: 1:32 - loss: 0.1778 - accu - ETA: 1:32 - loss: 0.178 - ETA: 1:31 - loss: 0.178 - E - ETA: 1:18 - l - ETA: 1: - ETA: 1:12 - ETA: 1:11 - - ETA: 1:03 - loss:  - ETA: 57s - loss: 0.17 - ETA: 43s - loss: 0.1781 - accurac - ETA: 42s - loss: 0.1782 - accuracy: 0.95 - ETA: 42s - loss: 0.1782 - accu - ETA: 28s - loss: 0.1 - ETA: 26s - E - ETA: 20s - loss: 0.1781 - accura - ETA: 19s - loss: 0.1781 - accuracy - ETA: 18s - loss: 0.1780 - ac - ETA: 17s - loss: 0.1780 - accura - ETA: 16s - loss: 0.1780 - accuracy:  - ETA: 15s - loss: 0.1780 - accuracy: - ETA: 15s - loss: 0.17 - ETA: 13s - loss: 0.178 - ETA: 8s - loss: - ETA: 3s - loss: 0.1781 - accu - ETA:  - ETA: 1s - los\n",
      "Epoch 27/2000\n",
      "12467/12467 [==============================] - 608s 49ms/step - loss: 0.1656 - accuracy: 0.9568 - val_loss: 0.9044 - val_accuracy: 0.5695A: 9: - ETA: 8:31 - l - E - ETA: 8:18 - loss: 0.1621 -  - ETA: 8: - ETA: 8:05 - loss: 0.1640 - accura - ETA: 8:04 - loss: 0.1637 -  - ETA: 59s - loss: 0.1644 - accuracy: 0.9 - ETA: 59 - ETA: 26s - loss: 0.1653 - acc - ETA: 25s - los \n",
      "Epoch 28/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.1511 - accuracy: 0.9640 - val_loss: 0.9179 - val_accuracy: 0.5713 loss: 0.1515 - acc - ETA: 56s - loss: 0.1515 - accurac - ETA: 55s - loss: 0.1514 - accuracy - ETA: 54s - loss: 0.1515 - accura - ETA: 53 - ETA: 46s - loss: 0.1513 - accuracy - ETA: 45s - los - ETA: 43s - loss: 0.1512 - a - ETA: 41s - loss: 0.1512 - ac - ETA: 40s - loss: 0 - ETA: 34s - loss: 0.1509 - accuracy: 0.96 - ETA: 34s - loss: 0.1509 - ETA: 32s - loss: 0.1507 - accuracy: 0 - - ETA: 28s - loss: 0.1507 - accuracy: 0 - ETA: 27s - loss: 0.1507 - accuracy - ETA: 26s - loss: 0.1508 - accuracy: 0.96 - ETA: 26s - loss:  - ETA: 24s - los - ETA: 21s - loss: 0.1512 - accuracy - ETA: 17s - loss - ETA: 4s - loss: 0.1513 - accuracy:  - ETA: 3s - loss: 0.1513 - accuracy: 0. - E\n",
      "Epoch 29/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.1384 - accuracy: 0.9700 - val_loss: 0.9314 - val_accuracy: 0.5615 - ETA: 47s - loss - ETA: 17s - loss: 0.1386 - accuracy: - ETA: 16s - loss\n",
      "Epoch 30/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.1246 - accuracy: 0.9740 - val_loss: 0.9348 - val_accuracy: 0.54890.1253 - accura - - ETA: 1:04 - loss: 0.1253 - accuracy - - ETA: 59s - loss: 0.1 - ETA: 57s - loss: 0.1253  - ETA: 36s - loss: 0.1248 - accuracy: 0.97 - ETA: 35s - loss: 0.1249 - accu - ETA: 34s  - ETA:  - ETA: 7s - loss: 0\n",
      "Epoch 31/2000\n",
      "12467/12467 [==============================] - 604s 48ms/step - loss: 0.1121 - accuracy: 0.9801 - val_loss: 0.9489 - val_accuracy: 0.5550: 0.1113 - accuracy: 0.98 - ETA: 2:52 - - ETA: 2:44 - loss: 0.1112 - accuracy:  - ETA: 2:44 - loss: 0.1112 - accura - ETA: 2:43 - loss: 0 - ETA: 2:42 - loss: 0.1111 - ac - ETA: 2:31 - loss: 0.1115 - ac - ETA: 2:31 - ETA: 2:25 - - ETA: 2:19 - loss: 0.1 - ETA: 2:18 - loss: 0.1113 - ac - ETA: 2:18 - ETA: 58s - loss: 0.1116 - accuracy: 0.980 - ETA: 58s - loss: 0.1116 - - ETA: 44s - loss:  - ETA: 42s -   - ETA: 31s - loss: 0.11 - ETA: 22s - ETA: 19s - loss - ETA: 12s - l - ETA: 7s - l - ETA: 6s - - ETA: 5s - loss: 0.1120 -  - ETA: 4s - los - ETA: 0s - loss: 0.1122 - accura - ETA: 0s - loss: 0.1122 - accu\n",
      "Epoch 32/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.1038 - accuracy: 0.9827 - val_loss: 0.9437 - val_accuracy: 0.5644ss: 0.1 - ETA: 59s - loss: 0.1034 - accuracy: 0.9830 - ETA:  - ETA: 56s - los - ETA: 41s - loss: 0.1036 - accura - ETA: 40s - loss: 0 - E\n",
      "Epoch 33/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0923 - accuracy: 0.9867 - val_loss: 0.9606 - val_accuracy: 0.5543loss: 0.0884 - ac - ETA: 9:18 - loss: 0.0892 - accu - ETA: 9:15 - loss: 0.0872 - accuracy:  - ETA: 9:14 - loss: 0.0879 - accuracy: 0. - ETA: 9:14 - loss: 0.0879 - accuracy:  - ETA: 9:14 - - ETA: 9:09 - loss: - ETA: 9:02 - loss: 0 - ETA: 9:01 - loss: 0.0890 - ac - ETA: 9:00 - loss: 0.0 - ETA: 8:56 - loss: 0.0870 - ac - ETA: 8:55 - l - ETA: 8:54 - loss: 0.0 - ETA: 8:52 - loss: 0.0 - ETA: 8:32 - loss: 0.0 - ETA - ETA: 8:23 - - ETA: 8:21 - loss: 0.0895 -  - ETA: 8:20 - loss: 0.0892 - accuracy: 0.98 - ETA: 8:20 - ETA: 8:19 - loss: 0.0890  - ETA: 8:18 - loss: - ETA: 5:45 - loss: - ETA: 5:44 - loss: 0.0919 - accuracy - ETA: 5:43 - loss: 0.091 - E - ETA: 4:52 - loss: 0.0923 - accuracy: 0.98 - ETA: 4:52 - ETA: 4:50 - loss: 0.0922 - accuracy - E - ETA: 4:44 - l - ETA: 4:42 - loss: 0.0919  - ETA: 4:42 - los - ETA: 4:40 - loss: 0.0919 - accuracy: 0.98 - ETA: 4:26 - loss: 0.0918 - ac - ETA: 4:25 - loss: 0.0917  - ETA: 4:18 - los - ETA: 4:17 - loss: 0 - ETA: 4:15 - loss: 0.0913 - accuracy: 0. - ETA: 4: - ETA: 4:12 - ETA: 4:10 - loss: 0.0913 - accuracy - ETA: 4:10 - loss: 0.0913 - accuracy: 0. - - ETA: 4:07 - loss: - ETA: 4:06 - loss: 0.0915 - accura - - ETA: 4:04 - loss: 0.0 - ETA: 4:03 - loss: 0.0916 - ac - ETA: 4:02 - loss: 0.0917 - accuracy: 0. - ETA: 4:02 - loss: 0.0917 - ac - ETA: 4:01 - ETA: 4:00 - l - ETA - ETA: 3:36 - l - ETA: 3:34 - ETA: 3:33 - - ETA:  - ETA: 3:27 - loss: 0.0 - ETA: 3:26 - loss: 0.0914 - accuracy: 0. - ETA: 3:26 - loss: 0.0914 - ac - ETA: 3:26 - loss: 0.091 - ETA:  - ETA: 2: - ETA: 1:14 - loss: 0.0 - ETA: 1:13 - loss: 0.0925 - accuracy:  - ETA: 1:13 - ETA: 1:11 - loss: 0.0 - ETA: 1: - ETA: 1:00 - loss: 0.0926  - ETA: 59s  - ETA: 56s - loss: 0.0926 - a - ETA: 55s - - ETA: 52s - loss: 0 - ETA: 46s - loss: 0.0926 - accurac - ETA: 45s - loss: 0.0926 - accuracy: 0.98 - ETA: 45s - loss:  - ETA: 42s - loss: 0.0 - ETA: 40s - l - ETA: 38s - loss: 0.0924 - accur - ETA: 37s - lo - ETA: 34s - loss: 0.0923 - accu - ETA: 33s - lo - ETA: 30s - loss: 0.0923 - accuracy: 0.9 - ETA: 30s  - ETA: 27s - loss: 0.0924 - acc - ETA: 26s - loss: 0.0924 - ac - ETA: 25s - los - ETA: 22s - loss: 0.0923 - accuracy: 0.98 - ETA: 18s - loss: 0.0922 - a -  - ETA: 13s -\n",
      "Epoch 34/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.0829 - accuracy: 0.9895 - val_loss: 0.9751 - val_accuracy: 0.55900.0831 - acc - ETA: 23s - loss: 0.0831 - a - ETA: 3s - loss: 0.0829  - ETA: 2s - l - ETA: 1s - ETA: 0s - loss: 0.0829 - accuracy: 0.98\n",
      "Epoch 35/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.0752 - accuracy: 0.9904 - val_loss: 1.0162 - val_accuracy: 0.5630s - loss: 0.0749 - a - ETA: 51s - loss: 0.0749 - a - ETA: 50s - loss: 0.0749 - accuracy: 0. - ETA: 50s -\n",
      "Epoch 36/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.0655 - accuracy: 0.9932 - val_loss: 1.0291 - val_accuracy: 0.5597ETA - ETA: 1:07 - loss: 0.065 - ETA: 1:01 - - ETA: 1:00 - - ETA: 58s - loss: 0.0649 - accuracy: 0.993 - ETA: 58s - loss: 0.0649 - accur - ETA: 57s - loss: 0.06 - ETA: 55s - loss: 0.0650 - accuracy: 0.99 - ETA: 55s - loss: 0.0650 - accur - ETA: 54s - loss: 0.0650 - accuracy:  - ETA: 53s - loss: 0.0650 - accuracy: 0 - ETA: 53s - loss: 0.0650 - accuracy:  - ETA: 52s - loss: 0.0650 - accu - ETA: 51s - loss: 0.0650 - ETA: 49 - ETA: 30s - loss: 0.06 - ETA: 28s - loss: 0.0653 - accuracy: 0. - ETA: 28s - loss: 0.0653 - acc - ETA: 18s  - ETA: 15s - loss: 0 - ETA: 13s - loss: 0.0  - ETA: 2s - loss: 0.0655 - accuracy:  - ETA: 2s - loss: 0.065 - ETA: 1s\n",
      "Epoch 37/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0579 - accuracy: 0.9944 - val_loss: 1.0671 - val_accuracy: 0.5561- loss: 0.0576 - accuracy: 0. - ETA: 36s - - ETA: 34s - loss: 0. - ETA: 2s - loss: 0.0577 - ac\n",
      "Epoch 38/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0511 - accuracy: 0.9961 - val_loss: 1.0664 - val_accuracy: 0.560155s - loss: 0.0512 - a - ETA: 46s - loss: 0.0512 - accuracy - ETA: 45s - loss: - ETA: 43s - los - ETA: 40s - ETA: 1s - loss:\n",
      "Epoch 39/2000\n",
      "12467/12467 [==============================] - 604s 48ms/step - loss: 0.0447 - accuracy: 0.9969 - val_loss: 1.1164 - val_accuracy: 0.5590\n",
      "Epoch 40/2000\n",
      "12467/12467 [==============================] - 604s 48ms/step - loss: 0.0393 - accuracy: 0.9978 - val_loss: 1.1180 - val_accuracy: 0.5514ss: 0.0393 - accuracy: 0 - ETA: 31s - loss: 0.0  - ETA: 17s - loss: 0.0393 - accuracy: 0.997 - ETA: 16s - loss: 0.0393 - accu - ETA: 15s - loss: 0.03  - ETA: 9s - loss: 0.0393 - accuracy: 0.997 - ETA: 9s - loss: 0.0393  - ETA: 3s - loss: 0\n",
      "Epoch 41/2000\n",
      "12467/12467 [==============================] - 606s 49ms/step - loss: 0.0338 - accuracy: 0.9986 - val_loss: 1.1656 - val_accuracy: 0.5608337 - a\n",
      "Epoch 42/2000\n",
      "12467/12467 [==============================] - 599s 48ms/step - loss: 0.0294 - accuracy: 0.9994 - val_loss: 1.1408 - val_accuracy: 0.5507\n",
      "Epoch 43/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0240 - accuracy: 0.9995 - val_loss: 1.1890 - val_accuracy: 0.5576- ETA: 44s - loss: 0.0240  - ETA: 43s - loss: - ETA: 40s - loss: 0.0240 -  - ETA: 39s - loss - ETA: 32s - loss: 0 - ETA: 30s - loss: 0.0239 - accu - ETA: 25s - loss: 0.0240 - accur - ETA: 24s - loss: 0.0240 - accuracy: 0.99 - ETA: 24s - loss: 0.0 - ETA: 22s - loss: 0.0240 - - ETA: 20s - loss: 0.0 - ETA: 0s - loss: 0.0240 - accuracy: 0.99\n",
      "Epoch 44/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0208 - accuracy: 0.9995 - val_loss: 1.2252 - val_accuracy: 0.5590- ETA: 1:27 - loss: 0.0209 -  - ETA: 1:27 - loss: 0.0209 - accura - ETA: 1:26 - loss: 0.0208 - accuracy:  - ETA: 1:26 - \n",
      "Epoch 45/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.0176 - accuracy: 0.9996 - val_loss: 1.2803 - val_accuracy: 0.55073:06 - loss: 0.0197 - accuracy: 1.00 - ETA: 12:46 - loss: 0.0202 - accurac - ETA:  - ETA: 8:49 - loss: 0.0177 - accuracy:  - E - E - ETA: 7:10 - loss: 0.0180 - accu - ETA: 7:10 - ETA - ETA: 7:04 - loss: 0.0182 - ac - ETA: 6:40 - loss: 0.0181 -  - ETA - ETA: 6:35 - loss: 0.0 - E - ETA: 6:00 - loss: 0.0 - ETA: 5:28 - loss: 0.018 - ETA: 5:27 - loss: 0.0180 - accuracy: 0.99 - ETA: 5:27 - loss: 0.0180  - ETA:  - ETA: 5:22 - loss: 0 - ETA: 5:21 - loss: 0.0179 - accuracy:  - ETA: 5:21 - los - ETA: 5: - ETA: 5:18 - loss: 0.017 - ETA: 5:17 - - ETA: 5:15 - loss: 0.0179 - accura - ETA: 5:15 - loss: 0.017 - ETA: 5:14 - l - ETA:  - ETA: 5:09 - loss: 0.017 - ETA: 5: - - ETA - ETA: 5:00 - - - ETA: 4:40 - loss: 0.0177 - ac - ETA: 4:39 - l - ETA: 2:53 - - ETA: 2:50 - loss: - ETA: 2:44 - loss: 0.0177 -  - ETA: 2:43 - loss: 0.0177 - ac - ETA: 2:43 - - ETA: 2:33 - loss: 0.0178 - accuracy - ETA - ETA: 1:32 - loss: 0.0177 - ac - ETA: 1:10 - loss: 0.0177 - ac - - - ETA: 1:02 - loss: 0.0176 - accuracy:  - ETA: 1:01 - loss: 0.0176 - accura - ETA - ETA: 59s - loss: 0.0176 - accurac -  - ETA: 50s - loss: - ETA: 48s - loss: 0.01 - ETA: 46s - loss - ETA:  - ETA: 40s - loss: 0.0176 - ETA: 38s - loss: 0.0176 - accura - ETA: 37s - loss: 0.0176 -  - ETA: 36s - loss: 0.0176 - a\n",
      "Epoch 46/2000\n",
      "12467/12467 [==============================] - 604s 48ms/step - loss: 0.0151 - accuracy: 0.9997 - val_loss: 1.3491 - val_accuracy: 0.5572- ETA: 2:34 - loss: 0.0151 - accuracy - ETA: 2:34 - loss: - ETA: 2:33 - loss: 0.0151 - ac - E - ETA: 2:28 - loss: 0.0151 -  - ETA: 1:06 - loss: - ETA: 1:05 - l - ETA: 59s - loss: 0.0152 - accuracy: 0 - ETA: 58s - loss: 0.0152 - accu - ETA: 53s - loss: 0.0151 - accu - ETA: 52s - - ETA: 49s - loss: 0.0151 - ac - ETA: 48s - loss: 0.0151 - accuracy:  - ET - ETA: 0s - loss: 0.0151 \n",
      "Epoch 47/2000\n",
      "12467/12467 [==============================] - 605s 49ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 1.3627 - val_accuracy: 0.55368s - loss: 0 - ETA: 28s - loss: - ETA: 25s  - ETA: 20s - loss: 0.0122 -  - ETA: 0s - loss: 0.0122 - accuracy\n",
      "Epoch 48/2000\n",
      "12467/12467 [==============================] - 605s 49ms/step - loss: 0.0099 - accuracy: 0.9999 - val_loss: 1.3169 - val_accuracy: 0.5532 - loss: 0.0099 - accuracy: - ETA: 12s -  - ETA: 7s - loss: 0.0099 - accuracy - ETA: 5s - loss: 0.0099 - ac - - ETA: 0s - loss: 0.0099 - \n",
      "Epoch 49/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 1.3671 - val_accuracy: 0.5496 - - ETA: 46s - loss: 0.00 - ETA: 44s - los - ETA: 41s - loss: 0.0085 - accu - ETA: 32s - loss: 0.0085 - accura - ETA: 31s - loss: 0.0085 - accuracy: - ETA: 31s - loss: 0.0085 - accur - ETA: 30s - loss: 0.0085 - a - E - ETA: 16s - loss: 0.0085 - accuracy: - ETA: 1 - ETA: 12s - loss: 0.0085 - accuracy: 1 - ETA: 12s - loss: 0.0085 - a - ETA: 11s - loss - ETA: 3s - loss: 0.0085 - accuracy: 1.00 - ETA: 3s - loss: 0.0085 - accuracy:  - ETA: 2s - loss: 0.008\n",
      "Epoch 50/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.0070 - accuracy: 0.9999 - val_loss: 1.3840 - val_accuracy: 0.55540074 - accuracy - ETA: 7:58 - loss: 0.0075 - accuracy - ETA: 7:58 - - ETA: 6:37 - loss: - ETA:  - ETA: 6:34 - l - ETA: 6:33 - loss: 0.0075 - accuracy: 0. - ETA: 6:33 - loss: 0.0075 - accura - ETA: 6:32 - loss: 0.0075 - accuracy:  - ETA: 6:32 - loss: 0.0075 - accuracy - ETA: 6:25 - loss: 0.0 - ETA: 2: - ETA: 2:06 - ETA: 2:05 - los - ETA: 1:57 - loss: 0.0070 - accu - ETA: 1:56 - loss: 0.0070 - accuracy: 0.99 - ETA: 1:56 - loss: 0.0070 -  - ETA: 1:56 - l - ETA: 1:44 - loss: 0.0 - ETA: 1: - ETA: 1:24 - loss: 0.00 - ETA: 33s - loss: 0.0070 - accuracy: 0 - ETA: 33s - loss: 0.0070 - accura - ETA: 32s - loss: 0.0070 - accuracy: - ETA: 31s - loss: 0.0070 - accuracy: 0 - ETA: 31s - loss: - ETA: - ETA: 25s - loss: 0.0070 - accuracy:  - ETA: 24s - loss: 0.0070 - accuracy: - ETA: 24s - loss: 0.0070 - acc - ETA: 14s - loss: 0.0 - ETA: 12s - loss: 0.0070 - a - ETA: 11s  - ETA: 3s - los - ETA: 1s - loss: 0.0070 - accura - ETA: 1s -\n",
      "Epoch 51/2000\n",
      "12467/12467 [==============================] - 599s 48ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.5002 - val_accuracy: 0.5453 - loss: 0.0058 - accura - ETA: 39s - loss: 0.005 - ETA: 33s - los - ETA: 30s - loss: 0.0058 - accuracy: 1.0 - ETA: 30s - loss: 0.0058 - accuracy:  - ETA: 29s  - ETA: 0s - loss: 0.0058 - accuracy: 1.00\n",
      "Epoch 52/2000\n",
      "12467/12467 [==============================] - 602s 48ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.5345 - val_accuracy: 0.5612accuracy: 1.00 - ETA: 46s - loss: - ETA: 44s - loss: 0.0045 - accuracy: 1.0 - ETA: 43s - loss: 0.0045 - a - ETA: 38s - loss:  - ETA: 36s - loss: 0.0046 - acc - ETA: 35s - loss: 0.0046 - accu - ETA: 34s - loss:  - ETA: 27s - - ETA: 20s - loss: \n",
      "Epoch 53/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.5523 - val_accuracy: 0.5489 0.0038 - accuracy: 1.000 - ETA: 58s - loss: 0.0038 - accuracy: 1.0 - ETA: 58s - loss: 0.0038 - acc - ETA: 57s - lo - ETA: 54s  - ETA: 4s - loss: 0.0038 - accuracy: 1.00 - ETA: 4s - loss: 0.0038 - accuracy\n",
      "Epoch 54/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.5913 - val_accuracy: 0.5475\n",
      "Epoch 55/2000\n",
      "12467/12467 [==============================] - 608s 49ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.6340 - val_accuracy: 0.5464 - loss: 0.0023 - accuracy: 1.000 - ETA: 42s -  - ETA: 39s - loss: 0.0023 - accu - ETA: 38s - loss: 0.0023 - a - ETA: 36s - loss: 0.002 - ETA: 35s - l - ETA: 32s - loss: 0. - ETA: 26s  - ETA: 23s - loss: 0. - ETA: 21s - loss: - ETA: 18s - loss: 0.00 - ETA: 17s  - E\n",
      "Epoch 56/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.6223 - val_accuracy: 0.5489A: 6:20 - l - - ETA: 6:17 - loss: 0.001 - ETA - ETA: 5:53 - loss: 0.0018 - accura - ETA: 5:52 - loss: 0.0018 - accuracy - ETA: 5:52 - loss: 0.0018 - ac - ETA: 5:51 - loss: 0.0018 - accuracy - ETA: 5: - ETA - ETA: 5:48 - loss: 0 - ETA: 5:32 - loss: 0 - ETA - ETA:  - ETA: 4:07 - loss: 0.0 - ETA: 1:29 - los - ET - ETA: 50s -  - ETA: 48s - loss: 0 - ETA: 46s - loss: - E - E - ETA: 32s - loss:  - ETA: 29s - loss: 0.0018 - accuracy:  - ETA: 1s - loss: 0.0018 - accuracy: 1. - ETA: 1s - loss: 0.001 - ETA: 0s - loss: 0.0018 - accuracy: 1.00\n",
      "Epoch 57/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.8060 - val_accuracy: 0.550758s - loss: 0.0014 - a - ETA: 57s - loss: 0.0014 - - ETA: 55s - loss: 0 - ETA: 53s - loss: 0.00  - ETA: 47s - loss: 0.0014 - accuracy: 1.0 - ETA: 47s - loss: 0.0014 - accuracy: - ETA: 46s - loss: 0.0014  - ETA: 37s - loss: 0.0014 - accuracy: 1.0 - ETA: 36s - loss: 0.0014 - accuracy: 1.0 - ETA: 36s - loss: 0.0014 - accurac - ETA: 35s - loss: 0.0 - ETA: 33s - loss: 0.0014 - acc - ETA: 32s - loss: - ETA: 30s - los - ETA: 27s - loss: 0.0014 - acc - ETA: 26s - loss: 0.001 - ETA: 20s - ETA: 17s - loss: 0.0014 - accuracy - ETA: 16s - loss: 0.0014 - ac - ETA: 15s  - E - ETA: 3s - loss: 0.0014 - accuracy:  - ETA: 2s - los - ETA: 1s - loss: 0.0014 - accuracy: 1. - ETA: 1s - loss: 0.0014 - accuracy: 1.00 - ETA: 1s - l\n",
      "Epoch 58/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12467/12467 [==============================] - 604s 48ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.8496 - val_accuracy: 0.547111 - accuracy:  - ETA: 5:43 - l - ETA: 5:42 - loss: 0.0011 - accu - ETA: 5:41 - los - ETA: 5:40 - los - ETA: 5:28 - loss: 0 - ETA: 5:27 - los - ETA: 5:20 - ETA: 5:18 - loss: 0.001 - ETA: 5:03 - loss: 0.0 - ETA - ETA: 3:44 - loss: 0.0011 - accuracy - ETA: 3:44 - loss: 0.0011 - ac - ETA: 1:00 - loss: 0.0011  - ETA: 59s - loss: 0.00 - ETA: 49s - loss: 0.0011 -  - ETA: 48s - loss: 0.0011 - accuracy: 1. - ETA: 48s - loss: 0.0011 - accuracy: 1.00 - ETA: 47s - loss - ETA: 45s - loss - ETA: 43s - loss:  - ETA: 3 - ETA: 29s - loss: 0.0011 - accurac - ETA: 28s - loss: - ETA: 26s - loss: 0.0011 - accu - ETA:  - ETA\n",
      "Epoch 59/2000\n",
      "12467/12467 [==============================] - 609s 49ms/step - loss: 8.3849e-04 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 0.5402uracy: 1. - ETA: 1:54 - loss: 8 - - ETA: 1:14 - - ETA: 1:12 - loss: 8.4166e - ETA: 1:11 - loss: 8.4102e-0 - ETA: 1:10 - loss: 8.4079e-04 - accu - ETA - ETA: 1:08 - los - ETA: 1:02 - los - ETA: 52s - loss: 8.4103e-04 - accuracy: 1.000 - ETA: 52s - loss: 8.4113e-04 - accuracy: 1.00 - ETA: 52s - loss: 8.4115e-04 - accuracy: 1.000 - ETA: 52s - ETA: 48s - lo - ETA: 45s - loss: 8.4075e-04 - accuracy: 1 - ETA: 45s - loss: 8.4041e-04 - accuracy: 1.00 - ETA: 45s - loss - ETA: 37s - loss: 8.3758e-04 - accuracy: 1.00 - ETA: 37s - loss: 8.3 - ETA: 35s - loss: 8.3667e-04 - accuracy: 1. - ETA: 34s - loss: 8.3704e-04 - accuracy:  - ETA: 34s - loss: 8.3695e-04 - accuracy: 1.000 - ETA: 34s - loss: 8.3688e-04 - accuracy: 1 - ETA: 33s - loss: 8.3669e-04 - accuracy - ETA: 33s - loss: 8.3702e-04 - - ETA: 31s  - ETA: 2 - ETA: 20s - loss: 8.3573e-04 - - ETA:  - ETA: 1 - ETA: 2s - loss: 8.381 - ETA: 1s - loss: 8.384\n",
      "Epoch 60/2000\n",
      "12467/12467 [==============================] - 605s 49ms/step - loss: 6.0688e-04 - accuracy: 1.0000 - val_loss: 1.9248 - val_accuracy: 0.5543- ETA: 2:22 - loss: 6.2211e-04 - accu - ETA: 2:21 - - E - ETA: 2:02 - loss: 6.2223e-04 - accuracy: 1. - E - - ETA: 1:0 - ETA: 33s - loss: 6.10 - ETA: 30s - loss: 6.1035e-04 - accuracy: 1.000 - ETA: 30s - loss: 6.1040e-04 - accura - ETA: 29s - loss: 6.0991e-04 - ac - ETA: 28s - loss: 6.0975e - ETA: 26s - l - ETA: 23s - loss: 6.1007e-04 - accuracy:  - ETA: 22s - loss: 6.1015e-04 - accuracy: 1 - ETA: 22s - loss: 6.1034e-04 - accuracy: 1.0 - ETA: 21s - loss: 6.1015e-04 - accuracy: 1.00 - ETA: 21s - loss: 6.1031e-04 - accuracy: 1 - ETA: 21s - loss: 6.1026e - ETA: 19s - l - E\n",
      "Epoch 61/2000\n",
      "12467/12467 [==============================] - 607s 49ms/step - loss: 4.6152e-04 - accuracy: 1.0000 - val_loss: 2.0945 - val_accuracy: 0.5547curacy - ETA: - ETA: 54s - loss:  - ETA: 43s - loss: 4.6301e-04 - accuracy:  - ETA: 43s - loss: 4.6372e-04 - accuracy - ETA - ETA: 34s - loss: 4.6304e-04 - ac - ETA: 3 - ETA: \n",
      "Epoch 62/2000\n",
      "12467/12467 [==============================] - 600s 48ms/step - loss: 3.2473e-04 - accuracy: 1.0000 - val_loss: 2.0469 - val_accuracy: 0.553604 -  - - ETA:  - ETA - ETA: 3: - ETA: 2:49 - los - ETA: 2:48 - loss: 3.3289e-04 - ac - ETA: 2:45 - loss: 3.3297e-04 - ac - ETA: 2:44 - l - ETA: 2:34 - loss: 3.318 - ETA: 2:26 - l - ETA: 2:24 - ETA: 1:05 - ETA: 50s - loss: 3.2541 - ETA: 47s - loss:  - ETA: 45s - loss: 3.2667e-04 - a - ETA: 43s - loss: 3.263 - ETA: 41s - loss: 3.2665e-04 - accuracy: 1  - ETA:  - ETA: 33s -\n",
      "Epoch 63/2000\n",
      "12467/12467 [==============================] - 603s 48ms/step - loss: 2.4720e-04 - accuracy: 1.0000 - val_loss: 2.1551 - val_accuracy: 0.5503\n",
      "Epoch 64/2000\n",
      "12467/12467 [==============================] - 606s 49ms/step - loss: 1.7712e-04 - accuracy: 1.0000 - val_loss: 2.1687 - val_accuracy: 0.5467- ETA: 17s - loss: 1. - ETA: 15s - loss: 1.7788e-04 -  - ETA: 5s - loss: 1.7737e-04 - ac - ETA: 4s - loss: 1.7734e-04 - accura - E - ETA: 2s - loss: 1.7719e - ETA: 1s - loss: 1.770\n",
      "Epoch 65/2000\n",
      "12467/12467 [==============================] - 609s 49ms/step - loss: 1.2849e-04 - accuracy: 1.0000 - val_loss: 2.3363 - val_accuracy: 0.54381.2811e-04 - accuracy: 1. - ETA: 6:23 - loss: - ETA: 4:50 - loss: 1.2730e-04 - accuracy:  - ETA: 4:49 - l - ETA: 4:45 - l - ETA: 4:44 - loss: 1.2733e-04 - accura - ETA: 4:43 - loss: 1.2734e-04 - accura - ETA: 4:43 - loss: 1.2721e-04  - ETA: 4:42 - loss: 1.2717e-04  - ETA: 4:41 - loss: 1.2699e-04 - accuracy: 1. - ETA: 4:41 - l - ETA: 4:40 - los - ETA: 4:31 - loss: 1.2737e-04 - ac - ETA: 4:31 - loss: 1.2725e-04 - accuracy:  - ETA: 4:30 - loss: 1.2715e-04 - accuracy: 1.00 - E - ETA: 4:28 - los - ETA: 3:59 - loss: 1.2700e - ETA: 3:58 - loss: 1.2688e-04 - accuracy: 1.00 - ETA: 3:58 - loss: - ETA: 3:57 - loss: 1.2684e - ETA: 3:56 - loss: 1.268 - ETA: 3:53 - loss: 1.2716e-04 -  - ETA: 3:52 - loss: 1.2707e - ETA: 3:51 - los - ETA: 3:31 - loss: 1 - ETA: 3:21 - loss: 1.3062e-04 - accuracy - ETA: 3: - ETA: 3:05 - - ETA: 3:03 - loss: 1.3014e-04 - accuracy: 1. - ETA: 3:03 - loss: 1.3010e-0 - ETA: 2:42 - loss: 1.2926e - ETA:  - ETA: 2:27 - loss: 1.2869e-04 - accuracy: 1.00 - ETA: 2:27 - loss: 1.2873e-04 - accuracy: 1. - ETA: 2: - ETA: 2:21 - loss: - ETA: 2:19 - los - ETA: 2:18 - loss: 1.2853e-04 - accu - ETA: 2:17 - loss: 1.2851e-04 - ac - ETA: 2:17 - loss: 1.2845e-04 - accuracy - ETA: 2:16 - l - ETA: 2:15 - loss: 1.2835e-04 - accura - ETA: 2:14 - loss: 1.2825e-04 - accuracy - E - ETA: 1:49 - loss: 1.2719e-04 - accuracy: 1.00 - ETA: 1:49 - loss: 1.2718e-04 - accuracy: 1. - ETA: 1:49 - loss: 1.2716e-04 - accuracy: 1.00 - ETA: 1:49 - loss: 1 - ETA: 1:00 - loss: 1.2837e-04 - accu - ETA: 59s - loss: 1.2828e-04 - accuracy: 1.000 - ETA: 59s - loss: 1.2829e-04 - accuracy: - ETA: 59s - loss: 1.2835e-04 - ac - ETA: 57s - loss: 1.2842e-04 - accuracy: 1.0 - ETA: 57s - loss: 1.2840e-04 - ac - ETA: 56s - loss: 1.2822e-04 - acc - ETA: 55s - loss: - ETA: 52s - loss: 1.2821e-04 - accuracy: - ETA: 51s - loss: - ET - ETA: 45s - loss: 1.2861e - ETA: 43s - loss: 1. - ETA: 4 - ETA: 32s - lo - ETA: 29s - loss: 1.284 - ETA: 27s - loss\n",
      "Epoch 66/2000\n",
      "12467/12467 [==============================] - 601s 48ms/step - loss: 9.6641e-05 - accuracy: 1.0000 - val_loss: 2.3875 - val_accuracy: 0.5554 7:45 - loss: 1.1064e-0 - ETA: 7:44 - loss: 1.1052e-04 - accuracy:  - E - ETA: 7:18 - l - ETA: 7:14 - l - ETA: 7:08 - loss: 1.0767e-04 - accuracy: 1. - ETA: 7:07 - loss: 1.0758e-04 - accuracy: 1. - ETA: 7:07 - loss: 1.0746e-04 - ac - ETA: 7:07 - ETA: 7:05 - ETA: 7:03 - loss: 1.0652e-0 - ETA:  - ETA: 7:00 - loss: 1.0594e-04 -  - ETA: 6:59 - loss: 1.0586e-04 - ac - E - ETA: 6:43 - ETA: 6:34 - ETA: 6:32 - loss: 1.0328e-04 - accu - ETA: 6:32 - - ETA: 6:30 - loss: 1.030 - ETA: 6:27 - loss: 1.0243e-04 -  - ETA: 6:24 - loss: 1.023 - ETA: 6:18 - loss: 1.0239e - ETA: 6:15 - loss: 1.023 - ETA: 5:53 - loss: 1.0239e-04 - ac - ETA:  - ETA: 5:46 - loss: 1.0199e-04 - ac - ETA: 5:45 - loss: 1.0206e-04 - accuracy: 1. - ETA - ETA: 4: - ETA: 4:37 - loss: 1.0 - ETA: 4:31 - loss: 1.0161e-04 - accuracy: 1.00 - ETA: 4:31 - - ETA: 4:29 - loss: 1.0146e-04  - ETA: 4:24 - - ETA: 1:07 - loss: 9.8146e-05 - accuracy: 1. - ETA: 1: - ETA: 1:05 - ETA: 1:03 - loss: 9.804 - ETA: 1:02 - loss: 9.7978e-05 - accuracy:  - E - ETA: 1:00 - loss: 9.7809e-05 - ac - ETA: 59s - loss: 9. - ETA: 56s - loss: 9.76 - ETA: 40s - loss: 9.7478e-05 - accuracy: 1 - ETA: 40s - loss: 9.7510e-05 - accur - ETA: 39s - loss: 9.7497e-05 - accuracy: 1 - ET - E - ETA: 7s - loss: 9.6904e-05 -  - ETA: 1s - l - ETA: 0s - loss: 9.6687e-05 - accura\n",
      "Epoch 67/2000\n",
      "12467/12467 [==============================] - 606s 49ms/step - loss: 6.6709e-05 - accuracy: 1.0000 - val_loss: 2.2702 - val_accuracy: 0.5431TA: 59s - loss: 6.8325e-05 - ETA: 58s - loss: 6.8243e-05 - accuracy: 1.00 -  - ETA: 40s - loss: 6.7765e- - ETA: 38s - loss: 6.7833e-05 - accuracy: 1 - ETA: 37s - loss: 6.7793e-05  - ETA: 36s - loss: 6.7724e-05 - accuracy: 1.000 - ET - ETA: 15s - loss: - ETA: 12s - loss: 6.7119e-05 - accurac - ETA: 11s - loss: 6.7083e-05 - a -  - ETA: 5s - loss: 6.687 - ETA: 4s - loss: 6.6829e-05 -  - ETA: 1s -\n",
      "Epoch 68/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12467/12467 [==============================] - 602s 48ms/step - loss: 4.6834e-05 - accuracy: 1.0000 - val_loss: 2.4966 - val_accuracy: 0.53844.5572e-05 - ac - - ETA: 45s - loss - ETA: 42s - loss: 4 - ETA: 35s - loss: 4.6640e-05  - ETA: 34s - los - ETA: 31s - loss: 4.6604 - \n",
      "Epoch 69/2000\n",
      "12467/12467 [==============================] - 608s 49ms/step - loss: 3.1816e-05 - accuracy: 1.0000 - val_loss: 2.5338 - val_accuracy: 0.5442A: 1:00 - loss: 3.2013e-05 - accuracy: 1.00 - ETA: 1: - ETA: 58s - lo -  - ETA: 50s - loss: 3.2008e-05 - accuracy:  - ETA - ETA\n",
      "Epoch 70/2000\n",
      " 7074/12467 [================>.............] - ETA: 4:08 - loss: 2.2574e-05 - accuracy: 1.0000 ETA: 4:52 - loss: 2.2665e-05 -  - ETA: 4:46 - loss: 2.2781e - ETA: 4:41 - loss: 2.2727e - ETA: 4:40 - loss: 2.2759e - ETA: 4:39 - loss: 2.276 - - ETA: 4:36 - loss: 2 - ETA: 4:19 - loss: 2.2626e-05 - accuracy: 1. - ETA: 4:19 - ETA: 4:17 - loss: 2.2608e - ETA: 4:16 - loss: 2.2 - ETA: 4:13 - ETA: 4:09 - loss: 2.2566e-05 - accuracy: WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-105d894279b4>\", line 27, in <module>\n",
      "    callbacks=[callback]\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 324, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1306, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 126, in run_one_epoch\n",
      "    step=step, mode=mode, size=current_batch_size) as batch_logs:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 112, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 781, in on_batch\n",
      "    mode, 'begin', step, batch_logs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\", line 242, in _call_batch_hook\n",
      "    delta_t_median = np.median(self._delta_ts[hook_name])\n",
      "  File \"<__array_function__ internals>\", line 6, in median\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3495, in median\n",
      "    overwrite_input=overwrite_input)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3403, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3528, in _median\n",
      "    part = partition(a, kth, axis=axis)\n",
      "  File \"<__array_function__ internals>\", line 6, in partition\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 746, in partition\n",
      "    a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3434, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"<ipython-input-2-22b089780bea>\", line 7, in hide_traceback\n",
      "    return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in _showtraceback\n",
      "    sys.stdout.flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 344, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    self._event_pipe.send(b'')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\", line 400, in send\n",
      "    return super(Socket, self).send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 728, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 775, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 242, in zmq.backend.cython.socket._send_copy\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-105d894279b4>\", line 27, in <module>\n",
      "    callbacks=[callback]\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 324, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1306, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 126, in run_one_epoch\n",
      "    step=step, mode=mode, size=current_batch_size) as batch_logs:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 112, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 781, in on_batch\n",
      "    mode, 'begin', step, batch_logs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\", line 242, in _call_batch_hook\n",
      "    delta_t_median = np.median(self._delta_ts[hook_name])\n",
      "  File \"<__array_function__ internals>\", line 6, in median\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3495, in median\n",
      "    overwrite_input=overwrite_input)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3403, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 3528, in _median\n",
      "    part = partition(a, kth, axis=axis)\n",
      "  File \"<__array_function__ internals>\", line 6, in partition\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 746, in partition\n",
      "    a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3434, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"<ipython-input-2-22b089780bea>\", line 7, in hide_traceback\n",
      "    return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in _showtraceback\n",
      "    sys.stdout.flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 344, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    self._event_pipe.send(b'')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\", line 400, in send\n",
      "    return super(Socket, self).send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 728, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 775, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 242, in zmq.backend.cython.socket._send_copy\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2922, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3356, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"<ipython-input-2-22b089780bea>\", line 7, in hide_traceback\n",
      "    return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 823, in get_exception_only\n",
      "    return ListTB.structured_traceback(self, etype, value)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 702, in structured_traceback\n",
      "    + out_list)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "epochs = 2000\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1/255.)#,\n",
    "        #rotation_range=30, \n",
    "        #width_shift_range=0.1,\n",
    "        #height_shift_range=0.1, \n",
    "        #horizontal_flip=True)\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1/255.)#,\n",
    "        #rotation_range=30, \n",
    "        #width_shift_range=0.1,\n",
    "        #height_shift_range=0.1, \n",
    "        #horizontal_flip=True)\n",
    "val_datagen.fit(X_val)\n",
    "\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "history = model.fit_generator(\n",
    "    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_datagen.flow(X_val, y_val, batch_size=batch_size),\n",
    "    callbacks=[callback]\n",
    ")\n",
    "# callbacks=[tf.keras.callbacks.ModelCheckpoint('VGG16-transferlearning.model', monitor='val_acc', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1/255.)#,\n",
    "        #rotation_range=30, \n",
    "        #width_shift_range=0.1,\n",
    "        #height_shift_range=0.1, \n",
    "        #horizontal_flip=True)\n",
    "test_datagen.fit(X_test)\n",
    "\n",
    "score = model.evaluate(test_datagen.flow(X_test, y_test, batch_size=batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
