{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_name = os.path.join(os.getcwd(), '데이터SET', f\"{'[Track1_데이터3] samp_cst_feat'}.csv\")\n",
    "df_x = pd.read_csv(x_name, index_col=0)\n",
    "\n",
    "y_name = os.path.join(os.getcwd(), '데이터SET', f\"{'[Track1_데이터2] samp_train'}.csv\")\n",
    "df_y = pd.read_csv(y_name, index_col=0)\n",
    "\n",
    "df = pd.merge(df_x, df_y, on='cst_id_di')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.MRC_ID_DI[df.MRC_ID_DI > 0 ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_name = os.path.join(os.getcwd(), '데이터SET', f\"{'[Track1_데이터4] variable_dtype'}.xlsx\")\n",
    "nc = pd.read_excel(nc_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10124, 227)\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.preprocessing import scale\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "\n",
    "def find_outliers_kde(x):\n",
    "    x_scaled = scale(list(map(float, x)))\n",
    "    kde = KDEUnivariate(x_scaled)\n",
    "    kde.fit(bw='scott', fft=True)\n",
    "    pred = kde.evaluate(x_scaled)\n",
    "    \n",
    "    n = sum(pred < 0.001)\n",
    "    outlier_ind = np.asarray(pred).argsort()[:n]\n",
    "    outlier_value = np.asarray(x)[outlier_ind]\n",
    "    \n",
    "    return outlier_ind, outlier_value\n",
    "\n",
    "for i in df.columns[0:-1]:\n",
    "    if nc.loc[i, 'dType'] == 'numerical':\n",
    "        kde_indices, kde_values = find_outliers_kde(df[i])\n",
    "        df_01 = df[i].quantile(0.01)\n",
    "        df_99 = df[i].quantile(0.99)\n",
    "        df_50 = df[i].quantile(0.50)\n",
    "        for j in range(len(df)):\n",
    "            if df[i].values[j] in kde_values:\n",
    "                df.at[df.index[j], i] = df_50\n",
    "\"\"\"\n",
    "for i in df.columns[0:-1]:\n",
    "    if nc.loc[i, 'dType'] == 'numerical':\n",
    "        d_90 = df[i].quantile(0.99)\n",
    "        d_10 = df[i].quantile(0.01)\n",
    "        d_50 = df[i].quantile(0.50)\n",
    "        df[i] = np.where(df[i] > d_90, d_90, df[i])\n",
    "        df[i] = np.where(df[i] < d_10, d_10, df[i])\n",
    "\"\"\"\n",
    "\n",
    "print(df.shape)\n",
    "#(10124, 227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython = get_ipython()\n",
    "\n",
    "def hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n",
    "                      exception_only=False, running_compiled_code=False):\n",
    "       etype, value, tb = sys.exc_info()\n",
    "       return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
    "\n",
    "ipython.showtraceback = hide_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df['MRC_ID_DI'] == 0].sample(frac=1)\n",
    "df_1 = df[df['MRC_ID_DI'] == 1].sample(frac=1)\n",
    "\n",
    "sample_size = len(df_0) if len(df_0) < len(df_1) else len(df_1)\n",
    "\n",
    "df_h = pd.concat([df_0.head(sample_size), df_1.head(sample_size)]).sample(frac=1)\n",
    "df_t = pd.concat([df_0.tail(sample_size), df_1.head(sample_size)]).sample(frac=1)\n",
    "df_f = pd.concat([df_h, df_t]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_f.drop(columns = ['MRC_ID_DI'], axis=1)\n",
    "#X = df_f.drop(columns = ['MRC_ID_DI', 'VAR021', 'VAR046'], axis=1)\n",
    "#y = tf.keras.utils.to_categorical(df_f['MRC_ID_DI'])\n",
    "y = df_f['MRC_ID_DI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num, cat = list(), list()\n",
    "for i in X.columns:\n",
    "    if nc.loc[i, 'dType'] == 'numerical':\n",
    "        num.append(i)\n",
    "    else:\n",
    "        cat.append(i)\n",
    "\n",
    "\n",
    "X1_train = X_train[num]\n",
    "X1_test = X_test[num]\n",
    "\n",
    "X2_train = X_train[cat]\n",
    "X2_test = X_test[cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def add_interactions(df1, df2):\n",
    "    df = pd.concat([df1, df2])\n",
    "    combos = list(combinations(list(df.columns), 2))\n",
    "    colnames = list(df.columns) + ['_'.join(x) for x in combos]\n",
    "    \n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    df = poly.fit_transform(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = colnames\n",
    "    \n",
    "    noint_indicies = [i for i, x in enumerate(list((df == 0).all())) if x]\n",
    "    df = df.drop(df.columns[noint_indicies], axis = 1)\n",
    "    return df[:len(df1)], df[len(df1):]\n",
    "\n",
    "X1_train, X1_test = add_interactions(X1_train, X1_test)\n",
    "X2_train, X2_test = add_interactions(X2_train, X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sklearn.feature_selection\\n\\nselect = sklearn.feature_selection.SelectKBest(k=len(X1_train.columns)//8)\\nselected_features = select.fit(X1_train, y_train)\\nindices_selected = selected_features.get_support(indices=True)\\ncolnames_selected = [X1_train.columns[i] for i in indices_selected]\\n\\nX1_train = X1_train[colnames_selected]\\nX1_test = X1_test[colnames_selected]\\n\\nselect = sklearn.feature_selection.SelectKBest(k=len(X2_train.columns)//8)\\nselected_features = select.fit(X2_train, y_train)\\nindices_selected = selected_features.get_support(indices=True)\\ncolnames_selected = [X2_train.columns[i] for i in indices_selected]\\n\\nX2_train = X2_train[colnames_selected]\\nX2_test = X2_test[colnames_selected]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sklearn.feature_selection\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k=len(X1_train.columns)//8)\n",
    "selected_features = select.fit(X1_train, y_train)\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [X1_train.columns[i] for i in indices_selected]\n",
    "\n",
    "X1_train = X1_train[colnames_selected]\n",
    "X1_test = X1_test[colnames_selected]\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k=len(X2_train.columns)//8)\n",
    "selected_features = select.fit(X2_train, y_train)\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [X2_train.columns[i] for i in indices_selected]\n",
    "\n",
    "X2_train = X2_train[colnames_selected]\n",
    "X2_test = X2_test[colnames_selected]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=min(len(pd.concat([X1_train, X1_test])), len(X1_train.columns)//8))\n",
    "X_t = pd.DataFrame(pca.fit_transform(pd.concat([X1_train, X1_test])))\n",
    "X1_train, X1_test = X_t[:len(X1_train)], X_t[len(X1_train):]\n",
    "\n",
    "pca = PCA(n_components=len(X2_train.columns)//8)\n",
    "X_t = pd.DataFrame(pca.fit_transform(pd.concat([X2_train, X2_test])))\n",
    "X2_train, X2_test = X_t[:len(X2_train)], X_t[len(X2_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tf.keras.Input(dtype = tf.float32, shape = (len(X1_train.columns),))\n",
    "input_2 = tf.keras.Input(dtype = tf.float32, shape = (len(X2_train.columns),))\n",
    "\n",
    "dense_layer_1_1 = tf.keras.layers.Dense(units = 64, activation = tf.nn.relu)(input_1)\n",
    "dropout_1_5 = tf.keras.layers.Dropout(rate = 0.2)(dense_layer_1_1)\n",
    "\n",
    "\n",
    "dense_layer_2_1 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(input_2)\n",
    "dropout_2_5 = tf.keras.layers.Dropout(rate = 0.2)(dense_layer_2_1)\n",
    "\n",
    "concat_layer = tf.keras.layers.Concatenate()([dropout_1_5, dropout_2_5])\n",
    "\n",
    "\n",
    "dense_layer_3 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(concat_layer)\n",
    "dropout_3_5 = tf.keras.layers.Dropout(rate = 0.8)(dense_layer_3)\n",
    "\n",
    "#output = tf.keras.layers.Dense(units = 2, activation = tf.nn.softmax)(dense_layer_3)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.softmax)(dropout_3_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.00005), metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(x=X_train, y=y_train, batch_size=32, epochs=400, verbose=1, validation_split=0.1)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40)\n",
    "history = model.fit(x=[X1_train, X2_train], y=y_train, batch_size=32, epochs=2000, verbose=1, validation_split=0.1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test ACC:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-c0042600a459>:24: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-16-c0042600a459>:25: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Learning Started!\n",
      "Epoch: 0001 cost = [0.69290214 0.70077731 0.69969636 0.71201616 0.68392486 0.70386796\n",
      " 0.71650612 0.69721202 0.70229658 0.70417356]\n",
      "Epoch: 0002 cost = [0.68112946 0.68748743 0.68469867 0.69597056 0.6707163  0.68537659\n",
      " 0.69408024 0.68714272 0.68814823 0.68721747]\n",
      "Epoch: 0003 cost = [0.66913488 0.67518549 0.67226866 0.68293151 0.66150814 0.67230107\n",
      " 0.67572839 0.680974   0.67762983 0.67205154]\n",
      "Epoch: 0004 cost = [0.65751194 0.66615875 0.66399635 0.66763701 0.6550242  0.6598266\n",
      " 0.66729791 0.67268445 0.67310835 0.66333692]\n",
      "Epoch: 0005 cost = [0.64880452 0.65721556 0.65460837 0.65973745 0.6462252  0.64901053\n",
      " 0.65331925 0.66585299 0.66477225 0.65038127]\n",
      "Epoch: 0006 cost = [0.64580229 0.64978614 0.6418056  0.65299928 0.64019485 0.64202337\n",
      " 0.65024481 0.66026023 0.65760798 0.64419747]\n",
      "Epoch: 0007 cost = [0.63471451 0.64239732 0.6390816  0.64175463 0.633105   0.63819062\n",
      " 0.63987742 0.65433641 0.6558568  0.63779755]\n",
      "Epoch: 0008 cost = [0.63500252 0.63918967 0.63580952 0.64056511 0.62995084 0.63352871\n",
      " 0.63266222 0.64942061 0.65239497 0.63333195]\n",
      "Epoch: 0009 cost = [0.62866921 0.6343094  0.63228823 0.63699251 0.62616782 0.62616084\n",
      " 0.63031864 0.64837367 0.64528793 0.62255   ]\n",
      "Epoch: 0010 cost = [0.62179758 0.6262385  0.6261316  0.62857117 0.62116598 0.62349596\n",
      " 0.62154619 0.63878111 0.64177698 0.61985126]\n",
      "Epoch: 0011 cost = [0.61982503 0.62275068 0.62062719 0.62149466 0.61804037 0.6168613\n",
      " 0.61697219 0.63227043 0.63727549 0.61357722]\n",
      "Epoch: 0012 cost = [0.6182024  0.61389778 0.62001049 0.6228286  0.61272754 0.61698224\n",
      " 0.60835347 0.62948962 0.6312051  0.60951219]\n",
      "Epoch: 0013 cost = [0.60748336 0.6146745  0.60820489 0.61142615 0.60998378 0.60670918\n",
      " 0.60772506 0.62604378 0.62681778 0.60782043]\n",
      "Epoch: 0014 cost = [0.60852274 0.61380527 0.61007785 0.61340079 0.6077219  0.60491574\n",
      " 0.60340202 0.61920987 0.62463197 0.60200723]\n",
      "Epoch: 0015 cost = [0.60006627 0.60606392 0.60581716 0.60778033 0.60332805 0.60172976\n",
      " 0.59851895 0.6169088  0.62244408 0.59773139]\n",
      "Epoch: 0016 cost = [0.59991044 0.60069975 0.59998324 0.6055264  0.60236145 0.59920201\n",
      " 0.5926179  0.61055133 0.61736435 0.59648051]\n",
      "Epoch: 0017 cost = [0.59784959 0.59901698 0.59949389 0.60040242 0.59879068 0.59251099\n",
      " 0.59340454 0.60752262 0.6090221  0.59480087]\n",
      "Epoch: 0018 cost = [0.59281176 0.59806451 0.5963893  0.59138941 0.59504583 0.59037739\n",
      " 0.59318512 0.60001015 0.60842989 0.59160396]\n",
      "Epoch: 0019 cost = [0.58901575 0.59188019 0.59279429 0.5937546  0.59562805 0.58909233\n",
      " 0.59012544 0.60061785 0.60569317 0.59204438]\n",
      "Epoch: 0020 cost = [0.59105955 0.58993514 0.59019192 0.5874025  0.5939101  0.58596516\n",
      " 0.58434929 0.59611097 0.60001488 0.58653918]\n",
      "Epoch: 0021 cost = [0.58815567 0.58466671 0.58562249 0.5839095  0.58680626 0.58450994\n",
      " 0.58155591 0.58930677 0.60210468 0.58408985]\n",
      "Epoch: 0022 cost = [0.58240792 0.58256749 0.58560187 0.58210215 0.58858348 0.5791062\n",
      " 0.57894268 0.58692849 0.59861931 0.58350734]\n",
      "Epoch: 0023 cost = [0.57757482 0.58152446 0.57977526 0.5813724  0.58541666 0.58197991\n",
      " 0.57348416 0.58302145 0.59725688 0.58195226]\n",
      "Epoch: 0024 cost = [0.58306112 0.58155979 0.57702164 0.57781142 0.58378896 0.5758672\n",
      " 0.57384023 0.58238696 0.59706809 0.57673141]\n",
      "Epoch: 0025 cost = [0.57442902 0.57683447 0.57623307 0.57790231 0.58248581 0.57344707\n",
      " 0.57023874 0.57943803 0.59249086 0.57425998]\n",
      "Epoch: 0026 cost = [0.57295453 0.57260304 0.57262576 0.57373514 0.57734524 0.57014231\n",
      " 0.56920923 0.58121239 0.58895276 0.57226581]\n",
      "Epoch: 0027 cost = [0.57051752 0.57358647 0.57458757 0.57196936 0.57678079 0.57073284\n",
      " 0.56601047 0.57771554 0.58999519 0.56904548]\n",
      "Epoch: 0028 cost = [0.56727505 0.56913879 0.57192868 0.56803327 0.57496449 0.56976772\n",
      " 0.56256972 0.57278341 0.58359877 0.56889304]\n",
      "Epoch: 0029 cost = [0.56914267 0.57186727 0.57161202 0.5685207  0.56737534 0.56709658\n",
      " 0.56130611 0.5712078  0.58545476 0.56592456]\n",
      "Epoch: 0030 cost = [0.56738861 0.5655383  0.5694358  0.56626271 0.57012397 0.56260159\n",
      " 0.5600004  0.56668324 0.57854707 0.56547946]\n",
      "Epoch: 0031 cost = [0.56279185 0.55937385 0.56560822 0.56381023 0.5654907  0.56186202\n",
      " 0.55563183 0.56694696 0.58016169 0.56449194]\n",
      "Epoch: 0032 cost = [0.56017274 0.56181862 0.56307978 0.56132404 0.56527221 0.56068884\n",
      " 0.55850075 0.56693736 0.58004264 0.56313206]\n",
      "Epoch: 0033 cost = [0.55832058 0.56325966 0.56282255 0.56151993 0.56436706 0.55946029\n",
      " 0.55553066 0.55981207 0.57704966 0.55744345]\n",
      "Epoch: 0034 cost = [0.55919421 0.55765686 0.55969017 0.56172888 0.56007078 0.55317753\n",
      " 0.55128958 0.55911686 0.57774591 0.55738989]\n",
      "Epoch: 0035 cost = [0.55464038 0.55403903 0.55806924 0.55521554 0.55907591 0.5533445\n",
      " 0.55044618 0.56194434 0.57197643 0.55174058]\n",
      "Epoch: 0036 cost = [0.55235534 0.55717201 0.55743006 0.55295751 0.55781328 0.55280729\n",
      " 0.54778535 0.5542725  0.57069271 0.55445604]\n",
      "Epoch: 0037 cost = [0.55347104 0.55414617 0.55392643 0.55164009 0.55833649 0.55264465\n",
      " 0.54671679 0.55685433 0.56567593 0.55064681]\n",
      "Epoch: 0038 cost = [0.5534483  0.55307391 0.55190926 0.54973505 0.55682664 0.54936978\n",
      " 0.54365469 0.55511186 0.56513319 0.5532935 ]\n",
      "Epoch: 0039 cost = [0.54753317 0.54953505 0.55141929 0.55068833 0.55382444 0.54759109\n",
      " 0.54165677 0.55521697 0.56718987 0.5480023 ]\n",
      "Epoch: 0040 cost = [0.54194327 0.54994033 0.55049706 0.54862515 0.55376368 0.5473316\n",
      " 0.5400715  0.55086154 0.56344277 0.54405275]\n",
      "Epoch: 0041 cost = [0.54599988 0.54812813 0.54421758 0.54896894 0.55044622 0.54212166\n",
      " 0.53768578 0.54767074 0.55989203 0.54308097]\n",
      "Epoch: 0042 cost = [0.54626704 0.54655581 0.54751519 0.54178509 0.54762792 0.54222173\n",
      " 0.53868169 0.54799907 0.55741021 0.54202793]\n",
      "Epoch: 0043 cost = [0.54215926 0.54019492 0.54349832 0.54200244 0.54636582 0.54170957\n",
      " 0.53860383 0.54312111 0.55753326 0.5472281 ]\n",
      "Epoch: 0044 cost = [0.54112071 0.54435422 0.54659928 0.53899537 0.54272948 0.53875324\n",
      " 0.53762929 0.54211634 0.55536819 0.54231023]\n",
      "Epoch: 0045 cost = [0.5360322  0.54452259 0.54281956 0.54005171 0.54346151 0.5402494\n",
      " 0.53111403 0.54333147 0.55163266 0.53498385]\n",
      "Epoch: 0046 cost = [0.53609818 0.54392177 0.54033957 0.54122409 0.54048405 0.53347862\n",
      " 0.53151137 0.54320763 0.55490538 0.53461675]\n",
      "Epoch: 0047 cost = [0.53653997 0.53797463 0.53797496 0.54137494 0.54037479 0.53354825\n",
      " 0.52966033 0.54006982 0.54693222 0.53171453]\n",
      "Epoch: 0048 cost = [0.53477924 0.54029233 0.53871684 0.53470604 0.53945914 0.53089579\n",
      " 0.52899496 0.53800022 0.54646459 0.53030678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0049 cost = [0.53398887 0.53348193 0.54029558 0.53544319 0.5382489  0.53403349\n",
      " 0.53177598 0.53789038 0.54949732 0.53238617]\n",
      "Epoch: 0050 cost = [0.53567303 0.53387713 0.53386273 0.53697107 0.53557947 0.53074154\n",
      " 0.52628286 0.53406516 0.54452564 0.53500682]\n",
      "Epoch: 0051 cost = [0.5315575  0.53130186 0.53539332 0.52984133 0.53129887 0.52975204\n",
      " 0.52704829 0.53423632 0.54224601 0.53102097]\n",
      "Epoch: 0052 cost = [0.53331183 0.52576164 0.53227623 0.53055986 0.53255547 0.52362634\n",
      " 0.5216162  0.5350201  0.54072423 0.53186964]\n",
      "Epoch: 0053 cost = [0.5285862  0.52961124 0.53154158 0.53412167 0.53070401 0.52638137\n",
      " 0.5232323  0.53146497 0.54332202 0.52857869]\n",
      "Epoch: 0054 cost = [0.53043178 0.5277407  0.53262965 0.52691561 0.52739856 0.52428138\n",
      " 0.5177243  0.53009793 0.53815647 0.522832  ]\n",
      "Epoch: 0055 cost = [0.52352408 0.52604633 0.52978847 0.53269976 0.52880425 0.52249611\n",
      " 0.52067244 0.52757865 0.53543986 0.52118501]\n",
      "Epoch: 0056 cost = [0.52797451 0.52374834 0.52860888 0.52489396 0.52178697 0.5210863\n",
      " 0.52017018 0.52797015 0.53939159 0.52058922]\n",
      "Epoch: 0057 cost = [0.52501798 0.52004678 0.5240516  0.52565596 0.52445785 0.51975171\n",
      " 0.51646366 0.5265955  0.53979228 0.52468276]\n",
      "Epoch: 0058 cost = [0.52392694 0.52353275 0.52702915 0.52440579 0.52199963 0.52106534\n",
      " 0.51760504 0.52554656 0.52825046 0.52570662]\n",
      "Epoch: 0059 cost = [0.52188683 0.52116645 0.52261419 0.52464899 0.52141132 0.5161661\n",
      " 0.51219812 0.52627483 0.5340725  0.51762031]\n",
      "Epoch: 0060 cost = [0.51885802 0.51473438 0.52288192 0.51937217 0.51838613 0.51445543\n",
      " 0.51576302 0.52199874 0.53322318 0.51956783]\n",
      "Epoch: 0061 cost = [0.52415852 0.52020501 0.52254963 0.51947483 0.51835806 0.51338131\n",
      " 0.51293778 0.52284388 0.53006064 0.51460576]\n",
      "Epoch: 0062 cost = [0.51719217 0.51605938 0.52433379 0.51878218 0.51679016 0.51482868\n",
      " 0.51010826 0.51937087 0.529793   0.51819609]\n",
      "Epoch: 0063 cost = [0.5179091  0.51555057 0.5208944  0.51911491 0.52025714 0.51759799\n",
      " 0.5069866  0.51829179 0.53062559 0.51308964]\n",
      "Epoch: 0064 cost = [0.51735137 0.51782006 0.51796394 0.51469426 0.51325312 0.51281403\n",
      " 0.50805447 0.51732896 0.52569491 0.51886145]\n",
      "Epoch: 0065 cost = [0.51721689 0.51535943 0.51755847 0.51287423 0.51203203 0.51299112\n",
      " 0.50830836 0.516338   0.52504834 0.51562496]\n",
      "Epoch: 0066 cost = [0.5155312  0.51032537 0.51578758 0.50930274 0.50939039 0.50999146\n",
      " 0.50768606 0.51737165 0.52096654 0.51137203]\n",
      "Epoch: 0067 cost = [0.51427507 0.51084566 0.51941917 0.51752063 0.50751878 0.51200301\n",
      " 0.51038137 0.51485987 0.52411499 0.51462578]\n",
      "Epoch: 0068 cost = [0.51322313 0.51345384 0.51353502 0.51009817 0.50980561 0.50678675\n",
      " 0.50292345 0.51366915 0.52353455 0.51306237]\n",
      "Epoch: 0069 cost = [0.50972476 0.50389552 0.51213114 0.51258393 0.50826171 0.50481436\n",
      " 0.50191493 0.51374226 0.51964821 0.50821334]\n",
      "Epoch: 0070 cost = [0.51123687 0.50939134 0.50912555 0.50974913 0.5071984  0.50341316\n",
      " 0.50403706 0.51296323 0.51567188 0.50895501]\n",
      "Epoch: 0071 cost = [0.50664246 0.50602175 0.51159157 0.50556464 0.50280234 0.5034313\n",
      " 0.50173421 0.51102303 0.5164812  0.50819394]\n",
      "Epoch: 0072 cost = [0.50628967 0.50493852 0.50927284 0.50670783 0.50212335 0.50044716\n",
      " 0.49748739 0.5084644  0.5158437  0.51108366]\n",
      "Epoch: 0073 cost = [0.51252693 0.50834752 0.50467253 0.50483207 0.49956987 0.49716245\n",
      " 0.49893993 0.50968273 0.5158818  0.50338474]\n",
      "Epoch: 0074 cost = [0.50718061 0.5053121  0.50995492 0.50673511 0.49841318 0.50096862\n",
      " 0.49651058 0.50875212 0.51191229 0.50721403]\n",
      "Epoch: 0075 cost = [0.50563221 0.49980134 0.50402615 0.5026379  0.50280476 0.49553247\n",
      " 0.49819881 0.50798052 0.50812933 0.50133901]\n",
      "Epoch: 0076 cost = [0.50449092 0.50081052 0.50379844 0.5024586  0.4979393  0.49860097\n",
      " 0.4958648  0.50630299 0.51127972 0.50686461]\n",
      "Epoch: 0077 cost = [0.50330979 0.50045058 0.50908894 0.50414239 0.49732097 0.49731393\n",
      " 0.50033666 0.5062232  0.51211017 0.50117241]\n",
      "Epoch: 0078 cost = [0.50441591 0.49918757 0.50604186 0.49712435 0.49236497 0.49114363\n",
      " 0.49433281 0.50895313 0.50844399 0.50019184]\n",
      "Epoch: 0079 cost = [0.50552697 0.49815893 0.50074102 0.49944005 0.4960889  0.48806547\n",
      " 0.4959106  0.50263228 0.50277496 0.49832673]\n",
      "Epoch: 0080 cost = [0.50561868 0.49807113 0.50542434 0.50055483 0.4897431  0.4943905\n",
      " 0.49227942 0.50480109 0.50581872 0.49779655]\n",
      "Epoch: 0081 cost = [0.50007249 0.50064775 0.50589795 0.50075404 0.49020499 0.49160686\n",
      " 0.49318896 0.5022794  0.50656964 0.50056926]\n",
      "Epoch: 0082 cost = [0.49923742 0.49755355 0.49834979 0.49415301 0.48728339 0.48813839\n",
      " 0.49055207 0.50276483 0.50678873 0.49661443]\n",
      "Epoch: 0083 cost = [0.4952961  0.4935654  0.4968573  0.49894716 0.48402765 0.48738557\n",
      " 0.4873182  0.50149075 0.5019243  0.48996289]\n",
      "Epoch: 0084 cost = [0.49866174 0.49276494 0.49657648 0.49890675 0.48770337 0.48918381\n",
      " 0.4916102  0.4974926  0.49980178 0.49680913]\n",
      "Epoch: 0085 cost = [0.49508894 0.49180079 0.49514784 0.49435889 0.48689761 0.4900463\n",
      " 0.49096284 0.49782238 0.5016963  0.49089829]\n",
      "Epoch: 0086 cost = [0.49466488 0.49335883 0.49733986 0.49195197 0.48437759 0.49251484\n",
      " 0.49161045 0.49787697 0.49788558 0.49195367]\n",
      "Epoch: 0087 cost = [0.49303581 0.4930247  0.49273649 0.49291878 0.48554043 0.48826614\n",
      " 0.49066607 0.49673175 0.50075834 0.49006089]\n",
      "Epoch: 0088 cost = [0.49191505 0.49185726 0.49407116 0.49319697 0.48489789 0.48467435\n",
      " 0.48729145 0.49747259 0.50120062 0.491638  ]\n",
      "Epoch: 0089 cost = [0.49129322 0.4888975  0.49658477 0.48653403 0.48372356 0.48238246\n",
      " 0.48570275 0.49390351 0.49416773 0.48833136]\n",
      "Epoch: 0090 cost = [0.49072616 0.48929747 0.49009138 0.49329213 0.48504971 0.4826165\n",
      " 0.48334794 0.49409693 0.49615312 0.49272802]\n",
      "Epoch: 0091 cost = [0.48820838 0.48773826 0.49176916 0.48529374 0.48202182 0.47957787\n",
      " 0.48623127 0.4948827  0.49174731 0.48840425]\n",
      "Epoch: 0092 cost = [0.48621632 0.48825425 0.49301324 0.48852585 0.48258313 0.48001446\n",
      " 0.48160376 0.49431247 0.48846954 0.4897491 ]\n",
      "Epoch: 0093 cost = [0.49217017 0.48701845 0.49497032 0.48785695 0.47546434 0.48280635\n",
      " 0.48082854 0.48805977 0.4898373  0.48716845]\n",
      "Epoch: 0094 cost = [0.48976841 0.48547895 0.48995269 0.49097789 0.47586371 0.47733997\n",
      " 0.48038601 0.49247768 0.49420307 0.48417297]\n",
      "Epoch: 0095 cost = [0.49119106 0.48374642 0.48807805 0.48928443 0.47823211 0.47614767\n",
      " 0.47890796 0.49417525 0.48885509 0.48665009]\n",
      "Epoch: 0096 cost = [0.48707952 0.48566262 0.48900672 0.48471513 0.47455551 0.47647802\n",
      " 0.47975141 0.49071234 0.49070218 0.48427956]\n",
      "Epoch: 0097 cost = [0.48909532 0.48399456 0.48639589 0.48463615 0.47625879 0.47678999\n",
      " 0.47981735 0.48653859 0.48610635 0.48572871]\n",
      "Epoch: 0098 cost = [0.48392828 0.48279322 0.48668078 0.48514642 0.47046085 0.47296386\n",
      " 0.47730571 0.48858814 0.48430154 0.48319284]\n",
      "Epoch: 0099 cost = [0.48911712 0.48401017 0.48725278 0.48679897 0.47391238 0.4748092\n",
      " 0.47366364 0.48362707 0.48713827 0.48241026]\n",
      "Epoch: 0100 cost = [0.48417841 0.48062716 0.48866996 0.48376986 0.47322535 0.47129241\n",
      " 0.47468412 0.48912968 0.48459835 0.4829689 ]\n",
      "Epoch: 0101 cost = [0.4845483  0.47983895 0.47933748 0.48200418 0.46853181 0.47114078\n",
      " 0.47243514 0.48815635 0.48760167 0.48389504]\n",
      "Epoch: 0102 cost = [0.4845518  0.48135754 0.48382675 0.48548463 0.46935196 0.47302009\n",
      " 0.47338917 0.48408059 0.48366167 0.4866015 ]\n",
      "Epoch: 0103 cost = [0.48312431 0.47732025 0.4854433  0.48124589 0.4690423  0.47129008\n",
      " 0.4757245  0.48540147 0.48437779 0.48173506]\n",
      "Epoch: 0104 cost = [0.48227135 0.47629781 0.48382848 0.4816428  0.47120386 0.47316353\n",
      " 0.47270635 0.48298885 0.47757612 0.48131781]\n",
      "Epoch: 0105 cost = [0.48515214 0.48130667 0.48104442 0.4828319  0.46806201 0.47145844\n",
      " 0.47503396 0.48513241 0.48253514 0.48233836]\n",
      "Epoch: 0106 cost = [0.48108355 0.47680178 0.48170513 0.47407201 0.46328513 0.46661654\n",
      " 0.47436479 0.48104558 0.4757872  0.47589627]\n",
      "Epoch: 0107 cost = [0.48202256 0.47824269 0.48202825 0.47740567 0.45898747 0.46861517\n",
      " 0.47189391 0.48342568 0.47871836 0.47840268]\n",
      "Epoch: 0108 cost = [0.47966338 0.47743904 0.48205038 0.48101966 0.46248688 0.47009579\n",
      " 0.4705365  0.48086701 0.48197332 0.47367119]\n",
      "Epoch: 0109 cost = [0.47720339 0.47704155 0.48168887 0.47540596 0.46699432 0.47116092\n",
      " 0.47115373 0.47704732 0.47842513 0.47066331]\n",
      "Epoch: 0110 cost = [0.48130145 0.47795155 0.47342219 0.47573096 0.4572739  0.47051927\n",
      " 0.46687185 0.47613079 0.47645275 0.47548104]\n",
      "Epoch: 0111 cost = [0.47497994 0.47533711 0.47556659 0.47686495 0.46357511 0.4672186\n",
      " 0.47097645 0.47714582 0.47200917 0.47267435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0112 cost = [0.47896437 0.47210181 0.47801108 0.47307933 0.46167362 0.46537473\n",
      " 0.46967098 0.47874144 0.47247725 0.47303509]\n",
      "Epoch: 0113 cost = [0.47669476 0.47386489 0.47643104 0.47305562 0.45919205 0.46534093\n",
      " 0.47116318 0.47777431 0.47480772 0.47510023]\n",
      "Epoch: 0114 cost = [0.47813607 0.47374498 0.48017322 0.47543628 0.4581125  0.46772613\n",
      " 0.46947149 0.479521   0.47698351 0.4786892 ]\n",
      "Epoch: 0115 cost = [0.48094335 0.47104852 0.47614996 0.47171223 0.46234321 0.46646959\n",
      " 0.46948045 0.47628488 0.46963535 0.47266435]\n",
      "Epoch: 0116 cost = [0.47413409 0.47304308 0.47582968 0.47364502 0.45805873 0.46408316\n",
      " 0.46727002 0.47898649 0.47285338 0.47008379]\n",
      "Epoch: 0117 cost = [0.47487624 0.47228101 0.4721834  0.47073113 0.45139172 0.46401695\n",
      " 0.46639367 0.47590913 0.46942835 0.47447557]\n",
      "Epoch: 0118 cost = [0.47289527 0.47185367 0.47547312 0.46883969 0.45770253 0.4605684\n",
      " 0.47023912 0.47382017 0.47090737 0.46994583]\n",
      "Epoch: 0119 cost = [0.47507383 0.46735929 0.4719259  0.47256837 0.46041064 0.46115739\n",
      " 0.46522146 0.47467597 0.47102619 0.46903324]\n",
      "Epoch: 0120 cost = [0.47528278 0.46738626 0.47311662 0.46896104 0.45317435 0.45740639\n",
      " 0.46541445 0.47235673 0.46621348 0.46858745]\n",
      "Epoch: 0121 cost = [0.46940615 0.46633378 0.46960864 0.47148568 0.45175015 0.46188428\n",
      " 0.46354069 0.4711109  0.4693177  0.46891101]\n",
      "Epoch: 0122 cost = [0.47536837 0.4676748  0.47353817 0.4711767  0.45752819 0.46212753\n",
      " 0.4636757  0.47272442 0.46480972 0.46722864]\n",
      "Epoch: 0123 cost = [0.46840314 0.46909028 0.47200996 0.47005381 0.45453854 0.46091361\n",
      " 0.46369756 0.4726406  0.46805598 0.47035765]\n",
      "Epoch: 0124 cost = [0.47416486 0.46959891 0.47309567 0.47560264 0.45510656 0.4608097\n",
      " 0.46291283 0.47626712 0.46536973 0.46950273]\n",
      "Epoch: 0125 cost = [0.47457274 0.46479543 0.46983423 0.46217484 0.44960357 0.45920488\n",
      " 0.4662346  0.47522843 0.46404615 0.46489733]\n",
      "Epoch: 0126 cost = [0.46847582 0.46199052 0.46955079 0.46864761 0.45018149 0.45966217\n",
      " 0.46421066 0.47473988 0.46699698 0.46986996]\n",
      "Epoch: 0127 cost = [0.4715217  0.46349196 0.46851387 0.46509102 0.44983432 0.45944331\n",
      " 0.46537317 0.47339596 0.46643105 0.465689  ]\n",
      "Epoch: 0128 cost = [0.47008551 0.46469016 0.46877419 0.46788218 0.44979852 0.46206979\n",
      " 0.46615367 0.4723875  0.46884889 0.47308099]\n",
      "Epoch: 0129 cost = [0.47018348 0.46153059 0.46762669 0.46235607 0.44682721 0.45428558\n",
      " 0.46173332 0.47104981 0.46414732 0.46629772]\n",
      "Epoch: 0130 cost = [0.46813373 0.46574972 0.47185332 0.46562544 0.45006885 0.4518981\n",
      " 0.4624784  0.47209999 0.46212043 0.46706139]\n",
      "Epoch: 0131 cost = [0.46986838 0.46694138 0.4729142  0.46559454 0.44971286 0.45562627\n",
      " 0.46300293 0.47281895 0.46469685 0.46957764]\n",
      "Epoch: 0132 cost = [0.47094966 0.46410358 0.46703411 0.46605252 0.44702718 0.45221372\n",
      " 0.45861514 0.47245768 0.46186109 0.46475582]\n",
      "Epoch: 0133 cost = [0.46692108 0.46212258 0.46590703 0.46754732 0.45010281 0.45430647\n",
      " 0.46155244 0.4638706  0.46384949 0.46342542]\n",
      "Epoch: 0134 cost = [0.46523545 0.46052505 0.47140031 0.46499457 0.44760882 0.45484933\n",
      " 0.46109192 0.4720346  0.46037633 0.4656321 ]\n",
      "Epoch: 0135 cost = [0.46807752 0.45898598 0.46664024 0.46558198 0.44823696 0.45341901\n",
      " 0.45891742 0.46787926 0.46291738 0.45919595]\n",
      "Epoch: 0136 cost = [0.46864475 0.4617309  0.46205074 0.46108899 0.44106024 0.45347928\n",
      " 0.46329708 0.46781868 0.46020215 0.45932174]\n",
      "Epoch: 0137 cost = [0.46880672 0.46165655 0.46956507 0.46545711 0.44610369 0.45302433\n",
      " 0.46089952 0.46821809 0.46079125 0.4644171 ]\n",
      "Epoch: 0138 cost = [0.46637495 0.45997509 0.4691014  0.46180558 0.4418715  0.44798658\n",
      " 0.46036284 0.46976417 0.45611132 0.45935138]\n",
      "Epoch: 0139 cost = [0.46442137 0.46320826 0.4678376  0.46394016 0.44622138 0.4547869\n",
      " 0.46208569 0.46292917 0.45841625 0.45869711]\n",
      "Epoch: 0140 cost = [0.46649616 0.4606287  0.46633171 0.46265178 0.43981062 0.45256381\n",
      " 0.45685698 0.46736733 0.45221728 0.45898702]\n",
      "Epoch: 0141 cost = [0.46850087 0.4631229  0.46403024 0.4627889  0.44160408 0.45237797\n",
      " 0.45805608 0.4672246  0.4636573  0.45969142]\n",
      "Epoch: 0142 cost = [0.46834442 0.45998276 0.46281783 0.46621909 0.4443035  0.44885306\n",
      " 0.46034568 0.4720559  0.46054869 0.45788377]\n",
      "Epoch: 0143 cost = [0.4649053  0.46210191 0.46625209 0.46068592 0.43840989 0.44956589\n",
      " 0.45909772 0.4654122  0.45365772 0.46049656]\n",
      "Epoch: 0144 cost = [0.4651819  0.45888349 0.46538616 0.46284983 0.4447969  0.45276361\n",
      " 0.46173079 0.4658477  0.45356666 0.45659714]\n",
      "Epoch: 0145 cost = [0.46574383 0.46023194 0.46218711 0.4660264  0.43748853 0.45209774\n",
      " 0.45850364 0.47001932 0.45992547 0.46011311]\n",
      "Epoch: 0146 cost = [0.46168129 0.45604306 0.46618135 0.46082491 0.44731456 0.45092709\n",
      " 0.45827741 0.46767711 0.46059208 0.45622888]\n",
      "Epoch: 0147 cost = [0.46090207 0.46292745 0.46511881 0.45810739 0.44295335 0.44503987\n",
      " 0.45559531 0.46490873 0.45026121 0.46275958]\n",
      "Epoch: 0148 cost = [0.46010919 0.45744781 0.4613891  0.46308015 0.43701141 0.44768495\n",
      " 0.45524393 0.4639628  0.45486409 0.46014285]\n",
      "Epoch: 0149 cost = [0.46469892 0.46114279 0.46781041 0.45693078 0.44658455 0.44780938\n",
      " 0.45598709 0.46235133 0.45651185 0.45846246]\n",
      "Epoch: 0150 cost = [0.45995689 0.45874185 0.46564829 0.46165716 0.4357726  0.44982348\n",
      " 0.45526971 0.46446891 0.45404418 0.46100572]\n",
      "Epoch: 0151 cost = [0.46463341 0.45730433 0.4657893  0.46094688 0.4392146  0.44715286\n",
      " 0.4617938  0.46647227 0.45573617 0.45731631]\n",
      "Epoch: 0152 cost = [0.46544557 0.45774487 0.46101482 0.46277119 0.43479972 0.44804293\n",
      " 0.45629825 0.46699833 0.45421945 0.45284236]\n",
      "Epoch: 0153 cost = [0.46189199 0.46019201 0.46022522 0.45905037 0.43902182 0.45169592\n",
      " 0.45521655 0.46473206 0.44790796 0.45921979]\n",
      "Epoch: 0154 cost = [0.46277282 0.458351   0.46518416 0.45621717 0.44107224 0.4450311\n",
      " 0.45606921 0.46356958 0.45274784 0.45699449]\n",
      "Epoch: 0155 cost = [0.46231955 0.45548384 0.45998357 0.45543061 0.43755071 0.44405467\n",
      " 0.45570852 0.46527083 0.45518436 0.45361465]\n",
      "Epoch: 0156 cost = [0.46538989 0.45456051 0.46222635 0.45652908 0.43373064 0.44475759\n",
      " 0.45255642 0.46440008 0.45265451 0.45174513]\n",
      "Epoch: 0157 cost = [0.46302612 0.45566395 0.46057877 0.4549512  0.43469251 0.4467101\n",
      " 0.45483667 0.46253399 0.453263   0.45586769]\n",
      "Epoch: 0158 cost = [0.46164822 0.45858845 0.46431587 0.45670684 0.43833212 0.44640062\n",
      " 0.45780441 0.46255919 0.44821008 0.46001651]\n",
      "Epoch: 0159 cost = [0.46214986 0.45595196 0.4633423  0.45677105 0.43548888 0.44702975\n",
      " 0.45357962 0.46248223 0.44783023 0.45570226]\n",
      "Epoch: 0160 cost = [0.4613385  0.45242444 0.46287851 0.45383178 0.43187765 0.44475362\n",
      " 0.4523235  0.46402738 0.45022406 0.45677241]\n",
      "Epoch: 0161 cost = [0.46231298 0.46071274 0.46125367 0.453751   0.4376318  0.44486575\n",
      " 0.45436081 0.46160196 0.44898598 0.4556941 ]\n",
      "Epoch: 0162 cost = [0.45599358 0.45262471 0.46104441 0.46291711 0.43683936 0.44384385\n",
      " 0.45551232 0.46025519 0.45104127 0.45309728]\n",
      "Epoch: 0163 cost = [0.45844066 0.45774212 0.45917676 0.45977813 0.43164615 0.44803657\n",
      " 0.45392146 0.46023296 0.44781133 0.45833196]\n",
      "Epoch: 0164 cost = [0.4555646  0.45988795 0.45951541 0.46219949 0.43472224 0.44414001\n",
      " 0.45534036 0.45945721 0.45585649 0.4546349 ]\n",
      "Epoch: 0165 cost = [0.4608452  0.4573651  0.46144226 0.45431353 0.43697078 0.44443109\n",
      " 0.45858371 0.46177456 0.44754746 0.45362216]\n",
      "Epoch: 0166 cost = [0.46039513 0.45572428 0.46041826 0.45812936 0.43439392 0.44418731\n",
      " 0.45728335 0.4603704  0.45235535 0.45302011]\n",
      "Epoch: 0167 cost = [0.46018754 0.45746757 0.45949635 0.4591596  0.43398294 0.44431956\n",
      " 0.45203522 0.4654722  0.4462227  0.45121049]\n",
      "Epoch: 0168 cost = [0.45793166 0.45473704 0.46291365 0.45443832 0.43403856 0.4373921\n",
      " 0.45151834 0.4670349  0.44728535 0.45193881]\n",
      "Epoch: 0169 cost = [0.45931726 0.45210437 0.45893797 0.45731178 0.43545233 0.44781622\n",
      " 0.45366511 0.46284711 0.44277655 0.45264578]\n",
      "Epoch: 0170 cost = [0.46138239 0.45486371 0.4600338  0.4577582  0.42934587 0.44554034\n",
      " 0.45269574 0.46138417 0.45054797 0.45467107]\n",
      "Epoch: 0171 cost = [0.45875766 0.45914014 0.46049859 0.4544981  0.43607156 0.44064617\n",
      " 0.45061647 0.45937392 0.44453122 0.45451342]\n",
      "Epoch: 0172 cost = [0.46231821 0.45372102 0.4552522  0.45316457 0.43039331 0.44375524\n",
      " 0.45559918 0.45772551 0.44646417 0.45465412]\n",
      "Epoch: 0173 cost = [0.45938632 0.45317212 0.46497581 0.45409658 0.4300896  0.44079472\n",
      " 0.44915018 0.46027006 0.45122428 0.45197228]\n",
      "Epoch: 0174 cost = [0.45775595 0.45992484 0.45783873 0.45446743 0.43018773 0.44304298\n",
      " 0.45327817 0.45950627 0.44318332 0.45249358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0175 cost = [0.45800869 0.45557434 0.45869373 0.44926304 0.43802626 0.44250137\n",
      " 0.4531522  0.46149645 0.44784778 0.45498108]\n",
      "Epoch: 0176 cost = [0.46016828 0.45498525 0.45838836 0.45479559 0.42726159 0.44099867\n",
      " 0.45262205 0.45970168 0.4496265  0.45521011]\n",
      "Epoch: 0177 cost = [0.45735168 0.45703485 0.45917525 0.45239103 0.43490089 0.43970273\n",
      " 0.45178668 0.45835353 0.44431401 0.44744707]\n",
      "Epoch: 0178 cost = [0.459879   0.45466807 0.45657056 0.45172805 0.43122456 0.44561883\n",
      " 0.45261957 0.46112518 0.44358116 0.45136885]\n",
      "Epoch: 0179 cost = [0.46185442 0.45287004 0.4616524  0.45056566 0.42431782 0.43915604\n",
      " 0.45216425 0.46061983 0.44668503 0.45274072]\n",
      "Epoch: 0180 cost = [0.46002318 0.45439432 0.46089127 0.4513991  0.43345657 0.437619\n",
      " 0.45613375 0.45709702 0.44815903 0.45228192]\n",
      "Epoch: 0181 cost = [0.45823601 0.45351975 0.45716876 0.45363747 0.43297109 0.43989629\n",
      " 0.4493969  0.45845696 0.44836093 0.45118344]\n",
      "Epoch: 0182 cost = [0.45738608 0.44967527 0.46224171 0.45220947 0.42739071 0.44534515\n",
      " 0.44999812 0.46149585 0.44501783 0.45387583]\n",
      "Epoch: 0183 cost = [0.45975576 0.45366566 0.4576073  0.45369344 0.43123855 0.43709896\n",
      " 0.45220372 0.45759219 0.43937754 0.4519629 ]\n",
      "Epoch: 0184 cost = [0.46063292 0.44863044 0.45950268 0.4492513  0.42582386 0.4412186\n",
      " 0.44789218 0.45863339 0.44474656 0.45087166]\n",
      "Epoch: 0185 cost = [0.46188065 0.45546518 0.45795804 0.45195171 0.43180931 0.43319811\n",
      " 0.45160277 0.45768471 0.44460646 0.45124543]\n",
      "Epoch: 0186 cost = [0.45562239 0.44957587 0.46093366 0.45401376 0.42968474 0.44107478\n",
      " 0.4471985  0.46102605 0.44333308 0.44922149]\n",
      "Epoch: 0187 cost = [0.4573659  0.45078985 0.45597764 0.45182605 0.43548223 0.44145717\n",
      " 0.44595747 0.45732789 0.442984   0.44995877]\n",
      "Epoch: 0188 cost = [0.45920694 0.45369625 0.45853877 0.45269825 0.43172296 0.4357596\n",
      " 0.44821284 0.45839404 0.44080916 0.45114643]\n",
      "Epoch: 0189 cost = [0.45876495 0.45521467 0.45893268 0.45297395 0.43099782 0.43043775\n",
      " 0.44914429 0.45857999 0.44655262 0.45363657]\n",
      "Epoch: 0190 cost = [0.454581   0.45125036 0.4565151  0.45602758 0.42867534 0.440684\n",
      " 0.45020081 0.45996146 0.44407066 0.45412052]\n",
      "Epoch: 0191 cost = [0.45501486 0.44967549 0.4589024  0.45198772 0.43197336 0.43943963\n",
      " 0.44902792 0.45761224 0.44346875 0.45256392]\n",
      "Epoch: 0192 cost = [0.45911547 0.45424829 0.45743024 0.45084232 0.42546691 0.43535664\n",
      " 0.44697799 0.46282084 0.44302014 0.45118984]\n",
      "Epoch: 0193 cost = [0.45425849 0.45397995 0.46286591 0.4487348  0.42594005 0.44232868\n",
      " 0.45240069 0.45819831 0.44300298 0.44941464]\n",
      "Epoch: 0194 cost = [0.45690648 0.4510113  0.45614637 0.45112567 0.42648755 0.44033202\n",
      " 0.45066098 0.45817698 0.44160403 0.452568  ]\n",
      "Epoch: 0195 cost = [0.45637908 0.4513341  0.4599818  0.45495582 0.42938003 0.4381736\n",
      " 0.45540764 0.46247194 0.44145589 0.45259756]\n",
      "Epoch: 0196 cost = [0.45860814 0.44740871 0.45792343 0.45018758 0.42866944 0.43446863\n",
      " 0.44901317 0.45540471 0.44212809 0.45127015]\n",
      "Epoch: 0197 cost = [0.45655653 0.45483481 0.45864352 0.45248694 0.42551966 0.44180497\n",
      " 0.44865599 0.45725624 0.4408718  0.454183  ]\n",
      "Epoch: 0198 cost = [0.45759621 0.44988351 0.45437605 0.45168496 0.42962469 0.43752484\n",
      " 0.44845458 0.45541006 0.44737359 0.44903793]\n",
      "Epoch: 0199 cost = [0.45643827 0.45234274 0.45730114 0.45067611 0.41783182 0.43761875\n",
      " 0.45557482 0.46118286 0.44245254 0.45057703]\n",
      "Epoch: 0200 cost = [0.45554072 0.45040608 0.45639221 0.45037978 0.42635844 0.44273479\n",
      " 0.45267921 0.45808325 0.44330118 0.44804094]\n",
      "Learning Finished!\n",
      "0 Accuracy: 0.88701296\n",
      "1 Accuracy: 0.8896104\n",
      "2 Accuracy: 0.88701296\n",
      "3 Accuracy: 0.8922078\n",
      "4 Accuracy: 0.8805195\n",
      "5 Accuracy: 0.88441557\n",
      "6 Accuracy: 0.8805195\n",
      "7 Accuracy: 0.8922078\n",
      "8 Accuracy: 0.8818182\n",
      "9 Accuracy: 0.8909091\n",
      "Ensemble accuracy: 0.8909091\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00005\n",
    "training_epochs = 200\n",
    "batch_size = 32\n",
    "num_models = 10\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            \n",
    "            self.training = tf.keras.backend.placeholder(dtype = tf.bool)\n",
    "\n",
    "            self.X1 = tf.keras.backend.placeholder(dtype = tf.float32, shape=(None,len(X1_train.columns)))\n",
    "            self.X2 = tf.keras.backend.placeholder(dtype = tf.float32, shape=(None,len(X2_train.columns)))            \n",
    "            self.Y = tf.keras.backend.placeholder(dtype = tf.float32, shape=(None,2))\n",
    "\n",
    "            dense1 = tf.compat.v1.layers.dense(inputs=self.X1, units = 64, activation=tf.nn.relu)\n",
    "            dropout1 = tf.compat.v1.layers.dropout(inputs=dense1, rate=0.2, training=self.training)\n",
    "            \n",
    "            dense2 = tf.compat.v1.layers.dense(inputs=self.X2, units = 10, activation=tf.nn.relu)\n",
    "            dropout2 = tf.compat.v1.layers.dropout(inputs=dense2, rate=0.2, training=self.training)\n",
    "            \n",
    "            concat_layer = tf.keras.layers.Concatenate()([dropout1, dropout2])\n",
    "\n",
    "            dense3 = tf.compat.v1.layers.dense(inputs=concat_layer, units = 10, activation=tf.nn.relu)\n",
    "            dropout3 = tf.compat.v1.layers.dropout(inputs=dense3, rate=0.8, training=self.training)\n",
    "            \n",
    "            \n",
    "            self.logits = tf.keras.layers.Dense(units = 2, activation = tf.nn.softmax)(dropout3)\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x1_test, x2_test, training=False):\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X1: x1_test, self.X2: x2_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x1_test, x2_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X1: x1_test, self.X2: x2_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x1_data, x2_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X1: x1_data, self.X2: x2_data, self.Y: y_data, self.training: training})\n",
    "\n",
    "    \n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "\n",
    "models = []\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    "\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(len(X1_train)/ batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs1, batch_xs2, batch_ys = X1_train[i * batch_size:(i+1) * batch_size], X2_train[i * batch_size:(i+1) * batch_size], y_train[i * batch_size:(i+1) * batch_size]\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs1, batch_xs2, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "\n",
    "test_size = len(y_test)\n",
    "predictions = np.zeros([test_size, 2])\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
    "        X1_test, X2_test, y_test))\n",
    "    p = m.predict(X1_test, X2_test) \n",
    "    predictions += p\n",
    "\n",
    "ensemble_correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y_test, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct_prediction,  tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))\n",
    "\n",
    "#dir = os.path.dirname(os.path.realpath(__file__))\n",
    "#all_saver = tf.train.Saver() \n",
    "#all_saver.save(sess, dir + '/0909.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
