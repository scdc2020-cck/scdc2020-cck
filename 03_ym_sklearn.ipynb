{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_name = os.path.join(os.getcwd(), '데이터SET', f\"{'[Track1_데이터3] samp_cst_feat'}.csv\")\n",
    "df_x = pd.read_csv(x_name, index_col=0)\n",
    "\n",
    "y_name = os.path.join(os.getcwd(), '데이터SET', f\"{'[Track1_데이터2] samp_train'}.csv\")\n",
    "df_y = pd.read_csv(y_name, index_col=0)\n",
    "\n",
    "df = pd.merge(df_x, df_y, on='cst_id_di')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.MRC_ID_DI[df.MRC_ID_DI > 0 ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_name = os.path.join(os.getcwd(), '데이터SET', f\"{'[Track1_데이터4] variable_dtype'}.xlsx\")\n",
    "nc = pd.read_excel(nc_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10124, 227)\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.preprocessing import scale\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "\n",
    "def find_outliers_kde(x):\n",
    "    x_scaled = scale(list(map(float, x)))\n",
    "    kde = KDEUnivariate(x_scaled)\n",
    "    kde.fit(bw='scott', fft=True)\n",
    "    pred = kde.evaluate(x_scaled)\n",
    "    \n",
    "    n = sum(pred < 0.001)\n",
    "    outlier_ind = np.asarray(pred).argsort()[:n]\n",
    "    outlier_value = np.asarray(x)[outlier_ind]\n",
    "    \n",
    "    return outlier_ind, outlier_value\n",
    "\n",
    "for i in df.columns[0:-1]:\n",
    "    if nc.loc[i, 'dType'] == 'numerical':\n",
    "        kde_indices, kde_values = find_outliers_kde(df[i])\n",
    "        df_01 = df[i].quantile(0.01)\n",
    "        df_99 = df[i].quantile(0.99)\n",
    "        df_50 = df[i].quantile(0.50)\n",
    "        for j in range(len(df)):\n",
    "            if df[i].values[j] in kde_values:\n",
    "                df.at[df.index[j], i] = df_50\n",
    "\"\"\"\n",
    "for i in df.columns[0:-1]:\n",
    "    if nc.loc[i, 'dType'] == 'numerical':\n",
    "        d_90 = df[i].quantile(0.99)\n",
    "        d_10 = df[i].quantile(0.01)\n",
    "        d_50 = df[i].quantile(0.50)\n",
    "        df[i] = np.where(df[i] > d_90, d_90, df[i])\n",
    "        df[i] = np.where(df[i] < d_10, d_10, df[i])\n",
    "\"\"\"\n",
    "\n",
    "print(df.shape)\n",
    "#(10124, 227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ipython = get_ipython()\n",
    "\n",
    "def hide_traceback(exc_tuple=None, filename=None, tb_offset=None,\n",
    "                      exception_only=False, running_compiled_code=False):\n",
    "       etype, value, tb = sys.exc_info()\n",
    "       return ipython._showtraceback(etype, value, ipython.InteractiveTB.get_exception_only(etype, value))\n",
    "\n",
    "ipython.showtraceback = hide_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df[df['MRC_ID_DI'] == 0].sample(frac=1)\n",
    "df_1 = df[df['MRC_ID_DI'] == 1].sample(frac=1)\n",
    "\n",
    "sample_size = len(df_0) if len(df_0) < len(df_1) else len(df_1)\n",
    "\n",
    "df_h = pd.concat([df_0.head(sample_size), df_1.head(sample_size)]).sample(frac=1)\n",
    "df_t = pd.concat([df_0.tail(sample_size), df_1.head(sample_size)]).sample(frac=1)\n",
    "df_f = pd.concat([df_h, df_t]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_f.drop(columns = ['MRC_ID_DI'], axis=1)\n",
    "#X = df_f.drop(columns = ['MRC_ID_DI', 'VAR021', 'VAR046'], axis=1)\n",
    "#y = tf.keras.utils.to_categorical(df_f['MRC_ID_DI'])\n",
    "y = df_f['MRC_ID_DI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "num, cat = list(), list()\n",
    "for i in X.columns:\n",
    "    if nc.loc[i, 'dType'] == 'numerical':\n",
    "        num.append(i)\n",
    "    else:\n",
    "        cat.append(i)\n",
    "\n",
    "\n",
    "X1_train = X_train[num]\n",
    "X1_test = X_test[num]\n",
    "\n",
    "X2_train = X_train[cat]\n",
    "X2_test = X_test[cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def add_interactions(df1, df2):\n",
    "    df = pd.concat([df1, df2])\n",
    "    combos = list(combinations(list(df.columns), 2))\n",
    "    colnames = list(df.columns) + ['_'.join(x) for x in combos]\n",
    "    \n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    df = poly.fit_transform(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = colnames\n",
    "    \n",
    "    noint_indicies = [i for i, x in enumerate(list((df == 0).all())) if x]\n",
    "    df = df.drop(df.columns[noint_indicies], axis = 1)\n",
    "    return df[:len(df1)], df[len(df1):]\n",
    "\n",
    "X1_train, X1_test = add_interactions(X1_train, X1_test)\n",
    "X2_train, X2_test = add_interactions(X2_train, X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sklearn.feature_selection\\n\\nselect = sklearn.feature_selection.SelectKBest(k=len(X1_train.columns)//8)\\nselected_features = select.fit(X1_train, y_train)\\nindices_selected = selected_features.get_support(indices=True)\\ncolnames_selected = [X1_train.columns[i] for i in indices_selected]\\n\\nX1_train = X1_train[colnames_selected]\\nX1_test = X1_test[colnames_selected]\\n\\nselect = sklearn.feature_selection.SelectKBest(k=len(X2_train.columns)//8)\\nselected_features = select.fit(X2_train, y_train)\\nindices_selected = selected_features.get_support(indices=True)\\ncolnames_selected = [X2_train.columns[i] for i in indices_selected]\\n\\nX2_train = X2_train[colnames_selected]\\nX2_test = X2_test[colnames_selected]\\n'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sklearn.feature_selection\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k=len(X1_train.columns)//8)\n",
    "selected_features = select.fit(X1_train, y_train)\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [X1_train.columns[i] for i in indices_selected]\n",
    "\n",
    "X1_train = X1_train[colnames_selected]\n",
    "X1_test = X1_test[colnames_selected]\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k=len(X2_train.columns)//8)\n",
    "selected_features = select.fit(X2_train, y_train)\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [X2_train.columns[i] for i in indices_selected]\n",
    "\n",
    "X2_train = X2_train[colnames_selected]\n",
    "X2_test = X2_test[colnames_selected]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=min(len(pd.concat([X1_train, X1_test])), len(X1_train.columns)//8))\n",
    "X_t = pd.DataFrame(pca.fit_transform(pd.concat([X1_train, X1_test])))\n",
    "X1_train, X1_test = X_t[:len(X1_train)], X_t[len(X1_train):]\n",
    "\n",
    "pca = PCA(n_components=len(X2_train.columns)//8)\n",
    "X_t = pd.DataFrame(pca.fit_transform(pd.concat([X2_train, X2_test])))\n",
    "X2_train, X2_test = X_t[:len(X2_train)], X_t[len(X2_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tf.keras.Input(dtype = tf.float32, shape = (len(X1_train.columns),))\n",
    "input_2 = tf.keras.Input(dtype = tf.float32, shape = (len(X2_train.columns),))\n",
    "\n",
    "dense_layer_1_1 = tf.keras.layers.Dense(units = 64, activation = tf.nn.relu)(input_1)\n",
    "dropout_1_5 = tf.keras.layers.Dropout(rate = 0.2)(dense_layer_1_1)\n",
    "\n",
    "\n",
    "dense_layer_2_1 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(input_2)\n",
    "dropout_2_5 = tf.keras.layers.Dropout(rate = 0.2)(dense_layer_2_1)\n",
    "\n",
    "concat_layer = tf.keras.layers.Concatenate()([dropout_1_5, dropout_2_5])\n",
    "\n",
    "\n",
    "dense_layer_3 = tf.keras.layers.Dense(units = 10, activation = tf.nn.relu)(concat_layer)\n",
    "dropout_3_5 = tf.keras.layers.Dropout(rate = 0.8)(dense_layer_3)\n",
    "\n",
    "#output = tf.keras.layers.Dense(units = 2, activation = tf.nn.softmax)(dense_layer_3)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.softmax)(dropout_3_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           [(None, 2462)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           [(None, 41)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 64)           157632      input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 10)           420         input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 64)           0           dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 10)           0           dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 74)           0           dropout_48[0][0]                 \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 10)           750         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 10)           0           dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 2)            22          dropout_50[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 158,824\n",
      "Trainable params: 158,824\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00001), metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6233 samples, validate on 693 samples\n",
      "Epoch 1/2000\n",
      "6233/6233 [==============================] - 1s 162us/sample - loss: 0.9133 - acc: 0.4964 - val_loss: 0.7318 - val_acc: 0.4834\n",
      "Epoch 2/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.9224 - acc: 0.4868 - val_loss: 0.7166 - val_acc: 0.5036\n",
      "Epoch 3/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.8987 - acc: 0.4847 - val_loss: 0.7029 - val_acc: 0.5310\n",
      "Epoch 4/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.8610 - acc: 0.4937 - val_loss: 0.6912 - val_acc: 0.5440\n",
      "Epoch 5/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.8550 - acc: 0.4877 - val_loss: 0.6801 - val_acc: 0.5628\n",
      "Epoch 6/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.8195 - acc: 0.5140 - val_loss: 0.6705 - val_acc: 0.5830\n",
      "Epoch 7/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.8107 - acc: 0.5150 - val_loss: 0.6622 - val_acc: 0.6089\n",
      "Epoch 8/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.7914 - acc: 0.5235 - val_loss: 0.6546 - val_acc: 0.6133\n",
      "Epoch 9/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.7807 - acc: 0.5286 - val_loss: 0.6476 - val_acc: 0.6248\n",
      "Epoch 10/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.7792 - acc: 0.5274 - val_loss: 0.6415 - val_acc: 0.6522\n",
      "Epoch 11/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.7647 - acc: 0.5246 - val_loss: 0.6363 - val_acc: 0.6696\n",
      "Epoch 12/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.7509 - acc: 0.5381 - val_loss: 0.6315 - val_acc: 0.6941\n",
      "Epoch 13/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.7458 - acc: 0.5299 - val_loss: 0.6272 - val_acc: 0.6970\n",
      "Epoch 14/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.7266 - acc: 0.5363 - val_loss: 0.6231 - val_acc: 0.7042\n",
      "Epoch 15/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.7289 - acc: 0.5436 - val_loss: 0.6200 - val_acc: 0.7071\n",
      "Epoch 16/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.7207 - acc: 0.5466 - val_loss: 0.6168 - val_acc: 0.7100\n",
      "Epoch 17/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.7092 - acc: 0.5464 - val_loss: 0.6140 - val_acc: 0.7258\n",
      "Epoch 18/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.7052 - acc: 0.5551 - val_loss: 0.6115 - val_acc: 0.7273\n",
      "Epoch 19/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.7035 - acc: 0.5458 - val_loss: 0.6092 - val_acc: 0.7287\n",
      "Epoch 20/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6996 - acc: 0.5562 - val_loss: 0.6070 - val_acc: 0.7359\n",
      "Epoch 21/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6957 - acc: 0.5530 - val_loss: 0.6050 - val_acc: 0.7417\n",
      "Epoch 22/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6919 - acc: 0.5570 - val_loss: 0.6029 - val_acc: 0.7489\n",
      "Epoch 23/2000\n",
      "6233/6233 [==============================] - 1s 86us/sample - loss: 0.6829 - acc: 0.5673 - val_loss: 0.6010 - val_acc: 0.7460\n",
      "Epoch 24/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.6906 - acc: 0.5654 - val_loss: 0.5994 - val_acc: 0.7460\n",
      "Epoch 25/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6788 - acc: 0.5675 - val_loss: 0.5982 - val_acc: 0.7518\n",
      "Epoch 26/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.6738 - acc: 0.5712 - val_loss: 0.5967 - val_acc: 0.7561\n",
      "Epoch 27/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6715 - acc: 0.5784 - val_loss: 0.5949 - val_acc: 0.7605\n",
      "Epoch 28/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.6619 - acc: 0.5803 - val_loss: 0.5933 - val_acc: 0.7619\n",
      "Epoch 29/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6694 - acc: 0.5792 - val_loss: 0.5922 - val_acc: 0.7633\n",
      "Epoch 30/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.6561 - acc: 0.5965 - val_loss: 0.5903 - val_acc: 0.7662\n",
      "Epoch 31/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6585 - acc: 0.5890 - val_loss: 0.5890 - val_acc: 0.7662\n",
      "Epoch 32/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.6562 - acc: 0.5896 - val_loss: 0.5873 - val_acc: 0.7662\n",
      "Epoch 33/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6503 - acc: 0.5996 - val_loss: 0.5857 - val_acc: 0.7662\n",
      "Epoch 34/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.6576 - acc: 0.5952 - val_loss: 0.5843 - val_acc: 0.7662\n",
      "Epoch 35/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6534 - acc: 0.5936 - val_loss: 0.5831 - val_acc: 0.7677\n",
      "Epoch 36/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.6511 - acc: 0.5920 - val_loss: 0.5819 - val_acc: 0.7691\n",
      "Epoch 37/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6483 - acc: 0.5941 - val_loss: 0.5804 - val_acc: 0.7662\n",
      "Epoch 38/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.6536 - acc: 0.6015 - val_loss: 0.5798 - val_acc: 0.7633\n",
      "Epoch 39/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6451 - acc: 0.6016 - val_loss: 0.5782 - val_acc: 0.7648\n",
      "Epoch 40/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.6417 - acc: 0.6084 - val_loss: 0.5770 - val_acc: 0.7648\n",
      "Epoch 41/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6388 - acc: 0.6117 - val_loss: 0.5754 - val_acc: 0.7662\n",
      "Epoch 42/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6349 - acc: 0.6092 - val_loss: 0.5737 - val_acc: 0.7691\n",
      "Epoch 43/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.6436 - acc: 0.6061 - val_loss: 0.5726 - val_acc: 0.7691\n",
      "Epoch 44/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6464 - acc: 0.6039 - val_loss: 0.5713 - val_acc: 0.7706\n",
      "Epoch 45/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6308 - acc: 0.6154 - val_loss: 0.5698 - val_acc: 0.7706\n",
      "Epoch 46/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.6394 - acc: 0.6122 - val_loss: 0.5687 - val_acc: 0.7706\n",
      "Epoch 47/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6365 - acc: 0.6071 - val_loss: 0.5674 - val_acc: 0.7720\n",
      "Epoch 48/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6366 - acc: 0.6101 - val_loss: 0.5663 - val_acc: 0.7706\n",
      "Epoch 49/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6377 - acc: 0.6174 - val_loss: 0.5651 - val_acc: 0.7734\n",
      "Epoch 50/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6319 - acc: 0.6177 - val_loss: 0.5637 - val_acc: 0.7749\n",
      "Epoch 51/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6335 - acc: 0.6084 - val_loss: 0.5624 - val_acc: 0.7763\n",
      "Epoch 52/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6304 - acc: 0.6092 - val_loss: 0.5612 - val_acc: 0.7734\n",
      "Epoch 53/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.6269 - acc: 0.6207 - val_loss: 0.5596 - val_acc: 0.7749\n",
      "Epoch 54/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6238 - acc: 0.6215 - val_loss: 0.5582 - val_acc: 0.7720\n",
      "Epoch 55/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6212 - acc: 0.6209 - val_loss: 0.5566 - val_acc: 0.7749\n",
      "Epoch 56/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.6222 - acc: 0.6252 - val_loss: 0.5550 - val_acc: 0.7763\n",
      "Epoch 57/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.6220 - acc: 0.6284 - val_loss: 0.5537 - val_acc: 0.7749\n",
      "Epoch 58/2000\n",
      "6233/6233 [==============================] - 1s 85us/sample - loss: 0.6267 - acc: 0.6217 - val_loss: 0.5521 - val_acc: 0.7792\n",
      "Epoch 59/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6246 - acc: 0.6238 - val_loss: 0.5508 - val_acc: 0.7792\n",
      "Epoch 60/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.6200 - acc: 0.6288 - val_loss: 0.5495 - val_acc: 0.7807\n",
      "Epoch 61/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.6160 - acc: 0.6345 - val_loss: 0.5483 - val_acc: 0.7792\n",
      "Epoch 62/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.6247 - acc: 0.6254 - val_loss: 0.5473 - val_acc: 0.7792\n",
      "Epoch 63/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.6164 - acc: 0.6324 - val_loss: 0.5462 - val_acc: 0.7792\n",
      "Epoch 64/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6281 - acc: 0.6271 - val_loss: 0.5453 - val_acc: 0.7807\n",
      "Epoch 65/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6086 - acc: 0.6419 - val_loss: 0.5438 - val_acc: 0.7792\n",
      "Epoch 66/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6173 - acc: 0.6356 - val_loss: 0.5428 - val_acc: 0.7792\n",
      "Epoch 67/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6166 - acc: 0.6308 - val_loss: 0.5419 - val_acc: 0.7807\n",
      "Epoch 68/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.6080 - acc: 0.6382 - val_loss: 0.5405 - val_acc: 0.7807\n",
      "Epoch 69/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.6104 - acc: 0.6398 - val_loss: 0.5390 - val_acc: 0.7807\n",
      "Epoch 70/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.6165 - acc: 0.6358 - val_loss: 0.5380 - val_acc: 0.7807\n",
      "Epoch 71/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.6168 - acc: 0.6340 - val_loss: 0.5372 - val_acc: 0.7821\n",
      "Epoch 72/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.6103 - acc: 0.6368 - val_loss: 0.5364 - val_acc: 0.7821\n",
      "Epoch 73/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.6122 - acc: 0.6435 - val_loss: 0.5354 - val_acc: 0.7821\n",
      "Epoch 74/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.6063 - acc: 0.6490 - val_loss: 0.5344 - val_acc: 0.7821\n",
      "Epoch 75/2000\n",
      "6233/6233 [==============================] - 1s 81us/sample - loss: 0.6037 - acc: 0.6482 - val_loss: 0.5328 - val_acc: 0.7807\n",
      "Epoch 76/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.6030 - acc: 0.6466 - val_loss: 0.5317 - val_acc: 0.7778\n",
      "Epoch 77/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.6082 - acc: 0.6422 - val_loss: 0.5306 - val_acc: 0.7792\n",
      "Epoch 78/2000\n",
      "6233/6233 [==============================] - 0s 80us/sample - loss: 0.6064 - acc: 0.6392 - val_loss: 0.5298 - val_acc: 0.7778\n",
      "Epoch 79/2000\n",
      "6233/6233 [==============================] - 1s 85us/sample - loss: 0.6139 - acc: 0.6422 - val_loss: 0.5290 - val_acc: 0.7778\n",
      "Epoch 80/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6059 - acc: 0.6482 - val_loss: 0.5280 - val_acc: 0.7763\n",
      "Epoch 81/2000\n",
      "6233/6233 [==============================] - 1s 84us/sample - loss: 0.6041 - acc: 0.6421 - val_loss: 0.5269 - val_acc: 0.7763\n",
      "Epoch 82/2000\n",
      "6233/6233 [==============================] - 1s 82us/sample - loss: 0.6175 - acc: 0.6366 - val_loss: 0.5263 - val_acc: 0.7778\n",
      "Epoch 83/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.6060 - acc: 0.6486 - val_loss: 0.5255 - val_acc: 0.7778\n",
      "Epoch 84/2000\n",
      "6233/6233 [==============================] - 1s 81us/sample - loss: 0.6069 - acc: 0.6459 - val_loss: 0.5248 - val_acc: 0.7792\n",
      "Epoch 85/2000\n",
      "6233/6233 [==============================] - 1s 86us/sample - loss: 0.5941 - acc: 0.6478 - val_loss: 0.5235 - val_acc: 0.7792\n",
      "Epoch 86/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.6022 - acc: 0.6422 - val_loss: 0.5226 - val_acc: 0.7807\n",
      "Epoch 87/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.6027 - acc: 0.6421 - val_loss: 0.5219 - val_acc: 0.7792\n",
      "Epoch 88/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.6068 - acc: 0.6360 - val_loss: 0.5212 - val_acc: 0.7792\n",
      "Epoch 89/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5913 - acc: 0.6535 - val_loss: 0.5200 - val_acc: 0.7807\n",
      "Epoch 90/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5975 - acc: 0.6522 - val_loss: 0.5190 - val_acc: 0.7807\n",
      "Epoch 91/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.5963 - acc: 0.6543 - val_loss: 0.5182 - val_acc: 0.7807\n",
      "Epoch 92/2000\n",
      "6233/6233 [==============================] - 1s 88us/sample - loss: 0.5957 - acc: 0.6586 - val_loss: 0.5174 - val_acc: 0.7807\n",
      "Epoch 93/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5925 - acc: 0.6519 - val_loss: 0.5164 - val_acc: 0.7807\n",
      "Epoch 94/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5962 - acc: 0.6607 - val_loss: 0.5152 - val_acc: 0.7807\n",
      "Epoch 95/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5820 - acc: 0.6592 - val_loss: 0.5138 - val_acc: 0.7807\n",
      "Epoch 96/2000\n",
      "6233/6233 [==============================] - 0s 79us/sample - loss: 0.5848 - acc: 0.6629 - val_loss: 0.5127 - val_acc: 0.7835\n",
      "Epoch 97/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.5972 - acc: 0.6546 - val_loss: 0.5120 - val_acc: 0.7807\n",
      "Epoch 98/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.5899 - acc: 0.6575 - val_loss: 0.5111 - val_acc: 0.7792\n",
      "Epoch 99/2000\n",
      "6233/6233 [==============================] - 0s 79us/sample - loss: 0.5903 - acc: 0.6612 - val_loss: 0.5102 - val_acc: 0.7792\n",
      "Epoch 100/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5861 - acc: 0.6579 - val_loss: 0.5094 - val_acc: 0.7807\n",
      "Epoch 101/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5924 - acc: 0.6559 - val_loss: 0.5086 - val_acc: 0.7792\n",
      "Epoch 102/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5843 - acc: 0.6626 - val_loss: 0.5073 - val_acc: 0.7792\n",
      "Epoch 103/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.5855 - acc: 0.6676 - val_loss: 0.5066 - val_acc: 0.7778\n",
      "Epoch 104/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5816 - acc: 0.6716 - val_loss: 0.5054 - val_acc: 0.7778\n",
      "Epoch 105/2000\n",
      "6233/6233 [==============================] - 0s 78us/sample - loss: 0.5923 - acc: 0.6669 - val_loss: 0.5049 - val_acc: 0.7778\n",
      "Epoch 106/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.5850 - acc: 0.6626 - val_loss: 0.5044 - val_acc: 0.7763\n",
      "Epoch 107/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5866 - acc: 0.6594 - val_loss: 0.5037 - val_acc: 0.7763\n",
      "Epoch 108/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5942 - acc: 0.6573 - val_loss: 0.5032 - val_acc: 0.7778\n",
      "Epoch 109/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5825 - acc: 0.6685 - val_loss: 0.5024 - val_acc: 0.7778\n",
      "Epoch 110/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5916 - acc: 0.6605 - val_loss: 0.5020 - val_acc: 0.7778\n",
      "Epoch 111/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5859 - acc: 0.6610 - val_loss: 0.5015 - val_acc: 0.7778\n",
      "Epoch 112/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5742 - acc: 0.6772 - val_loss: 0.5006 - val_acc: 0.7778\n",
      "Epoch 113/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.5884 - acc: 0.6666 - val_loss: 0.5003 - val_acc: 0.7792\n",
      "Epoch 114/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5826 - acc: 0.6661 - val_loss: 0.4995 - val_acc: 0.7807\n",
      "Epoch 115/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5887 - acc: 0.6671 - val_loss: 0.4991 - val_acc: 0.7792\n",
      "Epoch 116/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5820 - acc: 0.6525 - val_loss: 0.4984 - val_acc: 0.7807\n",
      "Epoch 117/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5884 - acc: 0.6653 - val_loss: 0.4980 - val_acc: 0.7807\n",
      "Epoch 118/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5793 - acc: 0.6725 - val_loss: 0.4971 - val_acc: 0.7821\n",
      "Epoch 119/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5861 - acc: 0.6652 - val_loss: 0.4965 - val_acc: 0.7821\n",
      "Epoch 120/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5749 - acc: 0.6717 - val_loss: 0.4954 - val_acc: 0.7821\n",
      "Epoch 121/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5826 - acc: 0.6671 - val_loss: 0.4949 - val_acc: 0.7821\n",
      "Epoch 122/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5819 - acc: 0.6668 - val_loss: 0.4941 - val_acc: 0.7821\n",
      "Epoch 123/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5780 - acc: 0.6722 - val_loss: 0.4935 - val_acc: 0.7821\n",
      "Epoch 124/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5742 - acc: 0.6758 - val_loss: 0.4928 - val_acc: 0.7821\n",
      "Epoch 125/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5765 - acc: 0.6727 - val_loss: 0.4923 - val_acc: 0.7821\n",
      "Epoch 126/2000\n",
      "6233/6233 [==============================] - 1s 85us/sample - loss: 0.5691 - acc: 0.6823 - val_loss: 0.4914 - val_acc: 0.7821\n",
      "Epoch 127/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5731 - acc: 0.6831 - val_loss: 0.4906 - val_acc: 0.7821\n",
      "Epoch 128/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5695 - acc: 0.6740 - val_loss: 0.4900 - val_acc: 0.7821\n",
      "Epoch 129/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5696 - acc: 0.6748 - val_loss: 0.4891 - val_acc: 0.7807\n",
      "Epoch 130/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5701 - acc: 0.6775 - val_loss: 0.4885 - val_acc: 0.7835\n",
      "Epoch 131/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.5652 - acc: 0.6796 - val_loss: 0.4876 - val_acc: 0.7835\n",
      "Epoch 132/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5697 - acc: 0.6814 - val_loss: 0.4871 - val_acc: 0.7835\n",
      "Epoch 133/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5646 - acc: 0.6844 - val_loss: 0.4860 - val_acc: 0.7864\n",
      "Epoch 134/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5696 - acc: 0.6796 - val_loss: 0.4851 - val_acc: 0.7879\n",
      "Epoch 135/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5698 - acc: 0.6774 - val_loss: 0.4846 - val_acc: 0.7879\n",
      "Epoch 136/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5765 - acc: 0.6719 - val_loss: 0.4844 - val_acc: 0.7879\n",
      "Epoch 137/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5569 - acc: 0.6918 - val_loss: 0.4836 - val_acc: 0.7879\n",
      "Epoch 138/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5658 - acc: 0.6804 - val_loss: 0.4829 - val_acc: 0.7879\n",
      "Epoch 139/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5618 - acc: 0.6958 - val_loss: 0.4820 - val_acc: 0.7879\n",
      "Epoch 140/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5624 - acc: 0.6812 - val_loss: 0.4812 - val_acc: 0.7879\n",
      "Epoch 141/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5660 - acc: 0.6932 - val_loss: 0.4805 - val_acc: 0.7879\n",
      "Epoch 142/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5642 - acc: 0.6880 - val_loss: 0.4799 - val_acc: 0.7879\n",
      "Epoch 143/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5610 - acc: 0.6868 - val_loss: 0.4793 - val_acc: 0.7879\n",
      "Epoch 144/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5598 - acc: 0.6884 - val_loss: 0.4786 - val_acc: 0.7893\n",
      "Epoch 145/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5650 - acc: 0.6831 - val_loss: 0.4781 - val_acc: 0.7893\n",
      "Epoch 146/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5516 - acc: 0.6828 - val_loss: 0.4772 - val_acc: 0.7908\n",
      "Epoch 147/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5562 - acc: 0.6944 - val_loss: 0.4765 - val_acc: 0.7908\n",
      "Epoch 148/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5566 - acc: 0.6867 - val_loss: 0.4758 - val_acc: 0.7937\n",
      "Epoch 149/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5545 - acc: 0.6985 - val_loss: 0.4751 - val_acc: 0.7937\n",
      "Epoch 150/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5558 - acc: 0.6952 - val_loss: 0.4743 - val_acc: 0.7937\n",
      "Epoch 151/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5567 - acc: 0.6937 - val_loss: 0.4738 - val_acc: 0.7937\n",
      "Epoch 152/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5533 - acc: 0.6904 - val_loss: 0.4731 - val_acc: 0.7937\n",
      "Epoch 153/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5534 - acc: 0.6880 - val_loss: 0.4727 - val_acc: 0.7937\n",
      "Epoch 154/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5518 - acc: 0.6944 - val_loss: 0.4720 - val_acc: 0.7937\n",
      "Epoch 155/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5578 - acc: 0.6952 - val_loss: 0.4712 - val_acc: 0.7951\n",
      "Epoch 156/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5479 - acc: 0.6817 - val_loss: 0.4707 - val_acc: 0.7951\n",
      "Epoch 157/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5511 - acc: 0.6942 - val_loss: 0.4700 - val_acc: 0.7937\n",
      "Epoch 158/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5536 - acc: 0.6973 - val_loss: 0.4695 - val_acc: 0.7937\n",
      "Epoch 159/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5603 - acc: 0.6871 - val_loss: 0.4691 - val_acc: 0.7922\n",
      "Epoch 160/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5555 - acc: 0.6960 - val_loss: 0.4686 - val_acc: 0.7951\n",
      "Epoch 161/2000\n",
      "6233/6233 [==============================] - 1s 86us/sample - loss: 0.5460 - acc: 0.6998 - val_loss: 0.4679 - val_acc: 0.7937\n",
      "Epoch 162/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5460 - acc: 0.6974 - val_loss: 0.4671 - val_acc: 0.7922\n",
      "Epoch 163/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5528 - acc: 0.6945 - val_loss: 0.4665 - val_acc: 0.7951\n",
      "Epoch 164/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5430 - acc: 0.7062 - val_loss: 0.4654 - val_acc: 0.7937\n",
      "Epoch 165/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5488 - acc: 0.6973 - val_loss: 0.4649 - val_acc: 0.7922\n",
      "Epoch 166/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.5457 - acc: 0.6993 - val_loss: 0.4643 - val_acc: 0.7937\n",
      "Epoch 167/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5523 - acc: 0.7016 - val_loss: 0.4636 - val_acc: 0.7937\n",
      "Epoch 168/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5390 - acc: 0.7086 - val_loss: 0.4627 - val_acc: 0.7937\n",
      "Epoch 169/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5447 - acc: 0.7082 - val_loss: 0.4621 - val_acc: 0.7951\n",
      "Epoch 170/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5411 - acc: 0.7051 - val_loss: 0.4617 - val_acc: 0.7937\n",
      "Epoch 171/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5422 - acc: 0.7083 - val_loss: 0.4609 - val_acc: 0.7937\n",
      "Epoch 172/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5363 - acc: 0.7048 - val_loss: 0.4602 - val_acc: 0.7937\n",
      "Epoch 173/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5436 - acc: 0.7040 - val_loss: 0.4597 - val_acc: 0.7937\n",
      "Epoch 174/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5384 - acc: 0.7008 - val_loss: 0.4592 - val_acc: 0.7937\n",
      "Epoch 175/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5436 - acc: 0.7016 - val_loss: 0.4587 - val_acc: 0.7951\n",
      "Epoch 176/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5425 - acc: 0.7066 - val_loss: 0.4581 - val_acc: 0.7951\n",
      "Epoch 177/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5463 - acc: 0.7016 - val_loss: 0.4579 - val_acc: 0.7965\n",
      "Epoch 178/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5371 - acc: 0.7136 - val_loss: 0.4572 - val_acc: 0.7965\n",
      "Epoch 179/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5401 - acc: 0.7016 - val_loss: 0.4563 - val_acc: 0.7965\n",
      "Epoch 180/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5364 - acc: 0.7074 - val_loss: 0.4558 - val_acc: 0.7965\n",
      "Epoch 181/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5323 - acc: 0.7106 - val_loss: 0.4551 - val_acc: 0.7965\n",
      "Epoch 182/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5214 - acc: 0.7213 - val_loss: 0.4544 - val_acc: 0.7965\n",
      "Epoch 183/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5326 - acc: 0.7183 - val_loss: 0.4539 - val_acc: 0.7965\n",
      "Epoch 184/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5429 - acc: 0.7135 - val_loss: 0.4535 - val_acc: 0.7980\n",
      "Epoch 185/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5338 - acc: 0.7115 - val_loss: 0.4528 - val_acc: 0.7994\n",
      "Epoch 186/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5379 - acc: 0.7086 - val_loss: 0.4524 - val_acc: 0.7980\n",
      "Epoch 187/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5306 - acc: 0.7180 - val_loss: 0.4520 - val_acc: 0.7994\n",
      "Epoch 188/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5322 - acc: 0.7149 - val_loss: 0.4512 - val_acc: 0.7980\n",
      "Epoch 189/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5294 - acc: 0.7191 - val_loss: 0.4505 - val_acc: 0.8023\n",
      "Epoch 190/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5277 - acc: 0.7149 - val_loss: 0.4499 - val_acc: 0.8009\n",
      "Epoch 191/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5328 - acc: 0.7202 - val_loss: 0.4493 - val_acc: 0.8009\n",
      "Epoch 192/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.5342 - acc: 0.7112 - val_loss: 0.4488 - val_acc: 0.8023\n",
      "Epoch 193/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5259 - acc: 0.7184 - val_loss: 0.4481 - val_acc: 0.8009\n",
      "Epoch 194/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5313 - acc: 0.7205 - val_loss: 0.4477 - val_acc: 0.8009\n",
      "Epoch 195/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.5306 - acc: 0.7147 - val_loss: 0.4475 - val_acc: 0.8023\n",
      "Epoch 196/2000\n",
      "6233/6233 [==============================] - 1s 84us/sample - loss: 0.5248 - acc: 0.7194 - val_loss: 0.4469 - val_acc: 0.8052\n",
      "Epoch 197/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5268 - acc: 0.7245 - val_loss: 0.4463 - val_acc: 0.8038\n",
      "Epoch 198/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5224 - acc: 0.7252 - val_loss: 0.4460 - val_acc: 0.8066\n",
      "Epoch 199/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5249 - acc: 0.7268 - val_loss: 0.4455 - val_acc: 0.8038\n",
      "Epoch 200/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5242 - acc: 0.7229 - val_loss: 0.4451 - val_acc: 0.8052\n",
      "Epoch 201/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.5199 - acc: 0.7240 - val_loss: 0.4446 - val_acc: 0.8052\n",
      "Epoch 202/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5288 - acc: 0.7244 - val_loss: 0.4443 - val_acc: 0.8038\n",
      "Epoch 203/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5250 - acc: 0.7303 - val_loss: 0.4439 - val_acc: 0.8023\n",
      "Epoch 204/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5125 - acc: 0.7303 - val_loss: 0.4430 - val_acc: 0.8023\n",
      "Epoch 205/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5212 - acc: 0.7297 - val_loss: 0.4425 - val_acc: 0.8038\n",
      "Epoch 206/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5195 - acc: 0.7258 - val_loss: 0.4418 - val_acc: 0.8038\n",
      "Epoch 207/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5164 - acc: 0.7311 - val_loss: 0.4413 - val_acc: 0.8052\n",
      "Epoch 208/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5087 - acc: 0.7335 - val_loss: 0.4403 - val_acc: 0.8038\n",
      "Epoch 209/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5196 - acc: 0.7223 - val_loss: 0.4395 - val_acc: 0.8038\n",
      "Epoch 210/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5124 - acc: 0.7348 - val_loss: 0.4386 - val_acc: 0.8038\n",
      "Epoch 211/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5200 - acc: 0.7303 - val_loss: 0.4380 - val_acc: 0.8038\n",
      "Epoch 212/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5168 - acc: 0.7300 - val_loss: 0.4376 - val_acc: 0.8038\n",
      "Epoch 213/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5113 - acc: 0.7327 - val_loss: 0.4372 - val_acc: 0.8052\n",
      "Epoch 214/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5161 - acc: 0.7258 - val_loss: 0.4366 - val_acc: 0.8038\n",
      "Epoch 215/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5164 - acc: 0.7340 - val_loss: 0.4360 - val_acc: 0.8038\n",
      "Epoch 216/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5139 - acc: 0.7295 - val_loss: 0.4356 - val_acc: 0.8038\n",
      "Epoch 217/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5137 - acc: 0.7293 - val_loss: 0.4352 - val_acc: 0.8038\n",
      "Epoch 218/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5140 - acc: 0.7346 - val_loss: 0.4346 - val_acc: 0.8081\n",
      "Epoch 219/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5124 - acc: 0.7362 - val_loss: 0.4342 - val_acc: 0.8081\n",
      "Epoch 220/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5092 - acc: 0.7467 - val_loss: 0.4337 - val_acc: 0.8081\n",
      "Epoch 221/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5144 - acc: 0.7303 - val_loss: 0.4335 - val_acc: 0.8095\n",
      "Epoch 222/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5139 - acc: 0.7337 - val_loss: 0.4332 - val_acc: 0.8095\n",
      "Epoch 223/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5131 - acc: 0.7351 - val_loss: 0.4325 - val_acc: 0.8081\n",
      "Epoch 224/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5062 - acc: 0.7374 - val_loss: 0.4319 - val_acc: 0.8095\n",
      "Epoch 225/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5099 - acc: 0.7340 - val_loss: 0.4316 - val_acc: 0.8038\n",
      "Epoch 226/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.5046 - acc: 0.7417 - val_loss: 0.4310 - val_acc: 0.8066\n",
      "Epoch 227/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5004 - acc: 0.7404 - val_loss: 0.4305 - val_acc: 0.8066\n",
      "Epoch 228/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.5050 - acc: 0.7375 - val_loss: 0.4301 - val_acc: 0.8066\n",
      "Epoch 229/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5011 - acc: 0.7305 - val_loss: 0.4295 - val_acc: 0.8066\n",
      "Epoch 230/2000\n",
      "6233/6233 [==============================] - 0s 79us/sample - loss: 0.5043 - acc: 0.7380 - val_loss: 0.4289 - val_acc: 0.8066\n",
      "Epoch 231/2000\n",
      "6233/6233 [==============================] - 1s 82us/sample - loss: 0.5019 - acc: 0.7361 - val_loss: 0.4285 - val_acc: 0.8066\n",
      "Epoch 232/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4964 - acc: 0.7444 - val_loss: 0.4280 - val_acc: 0.8066\n",
      "Epoch 233/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.5060 - acc: 0.7340 - val_loss: 0.4275 - val_acc: 0.8066\n",
      "Epoch 234/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.5027 - acc: 0.7446 - val_loss: 0.4271 - val_acc: 0.8038\n",
      "Epoch 235/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4984 - acc: 0.7367 - val_loss: 0.4267 - val_acc: 0.8052\n",
      "Epoch 236/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.4909 - acc: 0.7465 - val_loss: 0.4262 - val_acc: 0.8052\n",
      "Epoch 237/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5020 - acc: 0.7417 - val_loss: 0.4258 - val_acc: 0.8066\n",
      "Epoch 238/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4946 - acc: 0.7431 - val_loss: 0.4252 - val_acc: 0.8066\n",
      "Epoch 239/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.5058 - acc: 0.7382 - val_loss: 0.4247 - val_acc: 0.8066\n",
      "Epoch 240/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4947 - acc: 0.7491 - val_loss: 0.4243 - val_acc: 0.8081\n",
      "Epoch 241/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4919 - acc: 0.7537 - val_loss: 0.4238 - val_acc: 0.8066\n",
      "Epoch 242/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4972 - acc: 0.7491 - val_loss: 0.4235 - val_acc: 0.8081\n",
      "Epoch 243/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4906 - acc: 0.7518 - val_loss: 0.4230 - val_acc: 0.8066\n",
      "Epoch 244/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.5008 - acc: 0.7441 - val_loss: 0.4224 - val_acc: 0.8066\n",
      "Epoch 245/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.4947 - acc: 0.7500 - val_loss: 0.4218 - val_acc: 0.8081\n",
      "Epoch 246/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4993 - acc: 0.7531 - val_loss: 0.4213 - val_acc: 0.8081\n",
      "Epoch 247/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4915 - acc: 0.7462 - val_loss: 0.4209 - val_acc: 0.8081\n",
      "Epoch 248/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4968 - acc: 0.7454 - val_loss: 0.4204 - val_acc: 0.8095\n",
      "Epoch 249/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4899 - acc: 0.7468 - val_loss: 0.4198 - val_acc: 0.8095\n",
      "Epoch 250/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4960 - acc: 0.7496 - val_loss: 0.4196 - val_acc: 0.8095\n",
      "Epoch 251/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4941 - acc: 0.7502 - val_loss: 0.4192 - val_acc: 0.8095\n",
      "Epoch 252/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4874 - acc: 0.7452 - val_loss: 0.4187 - val_acc: 0.8110\n",
      "Epoch 253/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4802 - acc: 0.7608 - val_loss: 0.4180 - val_acc: 0.8110\n",
      "Epoch 254/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4870 - acc: 0.7423 - val_loss: 0.4176 - val_acc: 0.8110\n",
      "Epoch 255/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4852 - acc: 0.7526 - val_loss: 0.4172 - val_acc: 0.8095\n",
      "Epoch 256/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4836 - acc: 0.7547 - val_loss: 0.4167 - val_acc: 0.8081\n",
      "Epoch 257/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4868 - acc: 0.7486 - val_loss: 0.4162 - val_acc: 0.8081\n",
      "Epoch 258/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.4864 - acc: 0.7593 - val_loss: 0.4158 - val_acc: 0.8081\n",
      "Epoch 259/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4853 - acc: 0.7565 - val_loss: 0.4154 - val_acc: 0.8081\n",
      "Epoch 260/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.4888 - acc: 0.7560 - val_loss: 0.4150 - val_acc: 0.8081\n",
      "Epoch 261/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4885 - acc: 0.7507 - val_loss: 0.4146 - val_acc: 0.8095\n",
      "Epoch 262/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4868 - acc: 0.7521 - val_loss: 0.4142 - val_acc: 0.8095\n",
      "Epoch 263/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4843 - acc: 0.7553 - val_loss: 0.4138 - val_acc: 0.8095\n",
      "Epoch 264/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.4765 - acc: 0.7579 - val_loss: 0.4133 - val_acc: 0.8095\n",
      "Epoch 265/2000\n",
      "6233/6233 [==============================] - 1s 81us/sample - loss: 0.4722 - acc: 0.7650 - val_loss: 0.4129 - val_acc: 0.8095\n",
      "Epoch 266/2000\n",
      "6233/6233 [==============================] - 1s 82us/sample - loss: 0.4818 - acc: 0.7600 - val_loss: 0.4125 - val_acc: 0.8110\n",
      "Epoch 267/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4800 - acc: 0.7502 - val_loss: 0.4120 - val_acc: 0.8110\n",
      "Epoch 268/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4837 - acc: 0.7520 - val_loss: 0.4117 - val_acc: 0.8110\n",
      "Epoch 269/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4778 - acc: 0.7557 - val_loss: 0.4112 - val_acc: 0.8095\n",
      "Epoch 270/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4757 - acc: 0.7553 - val_loss: 0.4108 - val_acc: 0.8110\n",
      "Epoch 271/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.4876 - acc: 0.7500 - val_loss: 0.4107 - val_acc: 0.8110\n",
      "Epoch 272/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4759 - acc: 0.7579 - val_loss: 0.4101 - val_acc: 0.8110\n",
      "Epoch 273/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4784 - acc: 0.7609 - val_loss: 0.4097 - val_acc: 0.8110\n",
      "Epoch 274/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4768 - acc: 0.7555 - val_loss: 0.4093 - val_acc: 0.8124\n",
      "Epoch 275/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4744 - acc: 0.7531 - val_loss: 0.4088 - val_acc: 0.8110\n",
      "Epoch 276/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4757 - acc: 0.7627 - val_loss: 0.4084 - val_acc: 0.8110\n",
      "Epoch 277/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4675 - acc: 0.7719 - val_loss: 0.4081 - val_acc: 0.8095\n",
      "Epoch 278/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4757 - acc: 0.7645 - val_loss: 0.4078 - val_acc: 0.8081\n",
      "Epoch 279/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4783 - acc: 0.7579 - val_loss: 0.4075 - val_acc: 0.8095\n",
      "Epoch 280/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4693 - acc: 0.7693 - val_loss: 0.4070 - val_acc: 0.8095\n",
      "Epoch 281/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4755 - acc: 0.7653 - val_loss: 0.4066 - val_acc: 0.8095\n",
      "Epoch 282/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4640 - acc: 0.7606 - val_loss: 0.4062 - val_acc: 0.8095\n",
      "Epoch 283/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4691 - acc: 0.7691 - val_loss: 0.4060 - val_acc: 0.8095\n",
      "Epoch 284/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4689 - acc: 0.7648 - val_loss: 0.4055 - val_acc: 0.8139\n",
      "Epoch 285/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4670 - acc: 0.7670 - val_loss: 0.4051 - val_acc: 0.8139\n",
      "Epoch 286/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4708 - acc: 0.7659 - val_loss: 0.4048 - val_acc: 0.8139\n",
      "Epoch 287/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4658 - acc: 0.7728 - val_loss: 0.4044 - val_acc: 0.8153\n",
      "Epoch 288/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4657 - acc: 0.7695 - val_loss: 0.4041 - val_acc: 0.8167\n",
      "Epoch 289/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4708 - acc: 0.7616 - val_loss: 0.4037 - val_acc: 0.8167\n",
      "Epoch 290/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4624 - acc: 0.7662 - val_loss: 0.4032 - val_acc: 0.8167\n",
      "Epoch 291/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4664 - acc: 0.7685 - val_loss: 0.4030 - val_acc: 0.8153\n",
      "Epoch 292/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4674 - acc: 0.7695 - val_loss: 0.4026 - val_acc: 0.8167\n",
      "Epoch 293/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4581 - acc: 0.7738 - val_loss: 0.4021 - val_acc: 0.8167\n",
      "Epoch 294/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4678 - acc: 0.7621 - val_loss: 0.4017 - val_acc: 0.8139\n",
      "Epoch 295/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.4682 - acc: 0.7715 - val_loss: 0.4014 - val_acc: 0.8153\n",
      "Epoch 296/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4564 - acc: 0.7805 - val_loss: 0.4009 - val_acc: 0.8139\n",
      "Epoch 297/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.4669 - acc: 0.7706 - val_loss: 0.4007 - val_acc: 0.8153\n",
      "Epoch 298/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4600 - acc: 0.7744 - val_loss: 0.4002 - val_acc: 0.8153\n",
      "Epoch 299/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4608 - acc: 0.7670 - val_loss: 0.3999 - val_acc: 0.8167\n",
      "Epoch 300/2000\n",
      "6233/6233 [==============================] - 1s 87us/sample - loss: 0.4561 - acc: 0.7699 - val_loss: 0.3995 - val_acc: 0.8153\n",
      "Epoch 301/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.4628 - acc: 0.7659 - val_loss: 0.3992 - val_acc: 0.8167\n",
      "Epoch 302/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4608 - acc: 0.7746 - val_loss: 0.3988 - val_acc: 0.8196\n",
      "Epoch 303/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4609 - acc: 0.7654 - val_loss: 0.3985 - val_acc: 0.8196\n",
      "Epoch 304/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4573 - acc: 0.7760 - val_loss: 0.3982 - val_acc: 0.8182\n",
      "Epoch 305/2000\n",
      "6233/6233 [==============================] - 0s 79us/sample - loss: 0.4663 - acc: 0.7627 - val_loss: 0.3978 - val_acc: 0.8211\n",
      "Epoch 306/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4583 - acc: 0.7666 - val_loss: 0.3974 - val_acc: 0.8211\n",
      "Epoch 307/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4511 - acc: 0.7751 - val_loss: 0.3969 - val_acc: 0.8211\n",
      "Epoch 308/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4565 - acc: 0.7731 - val_loss: 0.3968 - val_acc: 0.8211\n",
      "Epoch 309/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4546 - acc: 0.7658 - val_loss: 0.3962 - val_acc: 0.8211\n",
      "Epoch 310/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4605 - acc: 0.7735 - val_loss: 0.3959 - val_acc: 0.8225\n",
      "Epoch 311/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4572 - acc: 0.7714 - val_loss: 0.3955 - val_acc: 0.8225\n",
      "Epoch 312/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4482 - acc: 0.7810 - val_loss: 0.3952 - val_acc: 0.8225\n",
      "Epoch 313/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.4577 - acc: 0.7759 - val_loss: 0.3948 - val_acc: 0.8225\n",
      "Epoch 314/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4492 - acc: 0.7770 - val_loss: 0.3943 - val_acc: 0.8211\n",
      "Epoch 315/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4558 - acc: 0.7719 - val_loss: 0.3939 - val_acc: 0.8240\n",
      "Epoch 316/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4538 - acc: 0.7731 - val_loss: 0.3935 - val_acc: 0.8240\n",
      "Epoch 317/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4481 - acc: 0.7826 - val_loss: 0.3933 - val_acc: 0.8240\n",
      "Epoch 318/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4533 - acc: 0.7707 - val_loss: 0.3929 - val_acc: 0.8225\n",
      "Epoch 319/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4476 - acc: 0.7789 - val_loss: 0.3925 - val_acc: 0.8225\n",
      "Epoch 320/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4594 - acc: 0.7711 - val_loss: 0.3924 - val_acc: 0.8225\n",
      "Epoch 321/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4477 - acc: 0.7802 - val_loss: 0.3921 - val_acc: 0.8225\n",
      "Epoch 322/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4461 - acc: 0.7767 - val_loss: 0.3918 - val_acc: 0.8225\n",
      "Epoch 323/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4461 - acc: 0.7739 - val_loss: 0.3916 - val_acc: 0.8225\n",
      "Epoch 324/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4487 - acc: 0.7687 - val_loss: 0.3914 - val_acc: 0.8225\n",
      "Epoch 325/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4428 - acc: 0.7775 - val_loss: 0.3910 - val_acc: 0.8225\n",
      "Epoch 326/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.4456 - acc: 0.7773 - val_loss: 0.3906 - val_acc: 0.8225\n",
      "Epoch 327/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4393 - acc: 0.7865 - val_loss: 0.3902 - val_acc: 0.8225\n",
      "Epoch 328/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4449 - acc: 0.7736 - val_loss: 0.3898 - val_acc: 0.8225\n",
      "Epoch 329/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4439 - acc: 0.7800 - val_loss: 0.3895 - val_acc: 0.8225\n",
      "Epoch 330/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4432 - acc: 0.7791 - val_loss: 0.3892 - val_acc: 0.8225\n",
      "Epoch 331/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4391 - acc: 0.7823 - val_loss: 0.3888 - val_acc: 0.8240\n",
      "Epoch 332/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4417 - acc: 0.7897 - val_loss: 0.3887 - val_acc: 0.8240\n",
      "Epoch 333/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.4385 - acc: 0.7836 - val_loss: 0.3883 - val_acc: 0.8240\n",
      "Epoch 334/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.4455 - acc: 0.7755 - val_loss: 0.3881 - val_acc: 0.8240\n",
      "Epoch 335/2000\n",
      "6233/6233 [==============================] - 1s 85us/sample - loss: 0.4356 - acc: 0.7853 - val_loss: 0.3878 - val_acc: 0.8240\n",
      "Epoch 336/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4478 - acc: 0.7772 - val_loss: 0.3874 - val_acc: 0.8240\n",
      "Epoch 337/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4325 - acc: 0.7889 - val_loss: 0.3871 - val_acc: 0.8240\n",
      "Epoch 338/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4441 - acc: 0.7821 - val_loss: 0.3868 - val_acc: 0.8240\n",
      "Epoch 339/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.4365 - acc: 0.7858 - val_loss: 0.3863 - val_acc: 0.8254\n",
      "Epoch 340/2000\n",
      "6233/6233 [==============================] - 0s 79us/sample - loss: 0.4391 - acc: 0.7833 - val_loss: 0.3859 - val_acc: 0.8254\n",
      "Epoch 341/2000\n",
      "6233/6233 [==============================] - 1s 85us/sample - loss: 0.4387 - acc: 0.7858 - val_loss: 0.3855 - val_acc: 0.8312\n",
      "Epoch 342/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4331 - acc: 0.7863 - val_loss: 0.3852 - val_acc: 0.8312\n",
      "Epoch 343/2000\n",
      "6233/6233 [==============================] - 0s 79us/sample - loss: 0.4335 - acc: 0.7881 - val_loss: 0.3852 - val_acc: 0.8283\n",
      "Epoch 344/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.4298 - acc: 0.7884 - val_loss: 0.3848 - val_acc: 0.8297\n",
      "Epoch 345/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4390 - acc: 0.7824 - val_loss: 0.3847 - val_acc: 0.8268\n",
      "Epoch 346/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4357 - acc: 0.7802 - val_loss: 0.3843 - val_acc: 0.8297\n",
      "Epoch 347/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4327 - acc: 0.7868 - val_loss: 0.3839 - val_acc: 0.8312\n",
      "Epoch 348/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4371 - acc: 0.7847 - val_loss: 0.3834 - val_acc: 0.8326\n",
      "Epoch 349/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4222 - acc: 0.7919 - val_loss: 0.3831 - val_acc: 0.8326\n",
      "Epoch 350/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4315 - acc: 0.7910 - val_loss: 0.3827 - val_acc: 0.8341\n",
      "Epoch 351/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4364 - acc: 0.7842 - val_loss: 0.3824 - val_acc: 0.8341\n",
      "Epoch 352/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4337 - acc: 0.7956 - val_loss: 0.3821 - val_acc: 0.8341\n",
      "Epoch 353/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4306 - acc: 0.7874 - val_loss: 0.3818 - val_acc: 0.8341\n",
      "Epoch 354/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4367 - acc: 0.7866 - val_loss: 0.3813 - val_acc: 0.8341\n",
      "Epoch 355/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4314 - acc: 0.7850 - val_loss: 0.3811 - val_acc: 0.8341\n",
      "Epoch 356/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4249 - acc: 0.7935 - val_loss: 0.3806 - val_acc: 0.8341\n",
      "Epoch 357/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4232 - acc: 0.7930 - val_loss: 0.3803 - val_acc: 0.8341\n",
      "Epoch 358/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4232 - acc: 0.7938 - val_loss: 0.3800 - val_acc: 0.8341\n",
      "Epoch 359/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4267 - acc: 0.7919 - val_loss: 0.3797 - val_acc: 0.8341\n",
      "Epoch 360/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4302 - acc: 0.7874 - val_loss: 0.3793 - val_acc: 0.8341\n",
      "Epoch 361/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.4255 - acc: 0.7918 - val_loss: 0.3790 - val_acc: 0.8326\n",
      "Epoch 362/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4264 - acc: 0.7937 - val_loss: 0.3788 - val_acc: 0.8326\n",
      "Epoch 363/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4261 - acc: 0.7914 - val_loss: 0.3785 - val_acc: 0.8312\n",
      "Epoch 364/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4239 - acc: 0.7951 - val_loss: 0.3781 - val_acc: 0.8341\n",
      "Epoch 365/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4211 - acc: 0.7943 - val_loss: 0.3778 - val_acc: 0.8341\n",
      "Epoch 366/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4165 - acc: 0.7982 - val_loss: 0.3775 - val_acc: 0.8341\n",
      "Epoch 367/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4286 - acc: 0.7910 - val_loss: 0.3773 - val_acc: 0.8341\n",
      "Epoch 368/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4283 - acc: 0.7926 - val_loss: 0.3772 - val_acc: 0.8341\n",
      "Epoch 369/2000\n",
      "6233/6233 [==============================] - 1s 87us/sample - loss: 0.4231 - acc: 0.7951 - val_loss: 0.3768 - val_acc: 0.8341\n",
      "Epoch 370/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.4218 - acc: 0.7881 - val_loss: 0.3764 - val_acc: 0.8341\n",
      "Epoch 371/2000\n",
      "6233/6233 [==============================] - 1s 93us/sample - loss: 0.4209 - acc: 0.7970 - val_loss: 0.3763 - val_acc: 0.8341\n",
      "Epoch 372/2000\n",
      "6233/6233 [==============================] - 1s 85us/sample - loss: 0.4181 - acc: 0.8023 - val_loss: 0.3760 - val_acc: 0.8369\n",
      "Epoch 373/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4203 - acc: 0.7882 - val_loss: 0.3756 - val_acc: 0.8355\n",
      "Epoch 374/2000\n",
      "6233/6233 [==============================] - 1s 87us/sample - loss: 0.4184 - acc: 0.7956 - val_loss: 0.3752 - val_acc: 0.8369\n",
      "Epoch 375/2000\n",
      "6233/6233 [==============================] - 1s 84us/sample - loss: 0.4123 - acc: 0.8022 - val_loss: 0.3750 - val_acc: 0.8369\n",
      "Epoch 376/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4156 - acc: 0.7942 - val_loss: 0.3746 - val_acc: 0.8369\n",
      "Epoch 377/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4119 - acc: 0.8052 - val_loss: 0.3743 - val_acc: 0.8355\n",
      "Epoch 378/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4173 - acc: 0.7987 - val_loss: 0.3742 - val_acc: 0.8355\n",
      "Epoch 379/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4111 - acc: 0.8019 - val_loss: 0.3738 - val_acc: 0.8355\n",
      "Epoch 380/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4182 - acc: 0.8041 - val_loss: 0.3734 - val_acc: 0.8355\n",
      "Epoch 381/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4205 - acc: 0.7929 - val_loss: 0.3732 - val_acc: 0.8369\n",
      "Epoch 382/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4068 - acc: 0.8051 - val_loss: 0.3730 - val_acc: 0.8384\n",
      "Epoch 383/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4185 - acc: 0.7987 - val_loss: 0.3727 - val_acc: 0.8369\n",
      "Epoch 384/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4159 - acc: 0.7911 - val_loss: 0.3725 - val_acc: 0.8398\n",
      "Epoch 385/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4154 - acc: 0.8014 - val_loss: 0.3723 - val_acc: 0.8384\n",
      "Epoch 386/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4068 - acc: 0.8056 - val_loss: 0.3720 - val_acc: 0.8384\n",
      "Epoch 387/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4074 - acc: 0.8057 - val_loss: 0.3719 - val_acc: 0.8369\n",
      "Epoch 388/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4107 - acc: 0.8060 - val_loss: 0.3716 - val_acc: 0.8384\n",
      "Epoch 389/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4055 - acc: 0.8068 - val_loss: 0.3713 - val_acc: 0.8384\n",
      "Epoch 390/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4081 - acc: 0.8099 - val_loss: 0.3712 - val_acc: 0.8384\n",
      "Epoch 391/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4097 - acc: 0.7956 - val_loss: 0.3708 - val_acc: 0.8369\n",
      "Epoch 392/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4055 - acc: 0.8104 - val_loss: 0.3705 - val_acc: 0.8369\n",
      "Epoch 393/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4028 - acc: 0.8027 - val_loss: 0.3703 - val_acc: 0.8369\n",
      "Epoch 394/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4053 - acc: 0.8039 - val_loss: 0.3701 - val_acc: 0.8384\n",
      "Epoch 395/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4041 - acc: 0.8068 - val_loss: 0.3698 - val_acc: 0.8369\n",
      "Epoch 396/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3995 - acc: 0.8078 - val_loss: 0.3696 - val_acc: 0.8369\n",
      "Epoch 397/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4104 - acc: 0.7987 - val_loss: 0.3695 - val_acc: 0.8384\n",
      "Epoch 398/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4003 - acc: 0.8126 - val_loss: 0.3693 - val_acc: 0.8384\n",
      "Epoch 399/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4047 - acc: 0.8064 - val_loss: 0.3689 - val_acc: 0.8398\n",
      "Epoch 400/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4084 - acc: 0.8057 - val_loss: 0.3686 - val_acc: 0.8398\n",
      "Epoch 401/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4010 - acc: 0.8070 - val_loss: 0.3685 - val_acc: 0.8384\n",
      "Epoch 402/2000\n",
      "6233/6233 [==============================] - 0s 78us/sample - loss: 0.3995 - acc: 0.8123 - val_loss: 0.3681 - val_acc: 0.8384\n",
      "Epoch 403/2000\n",
      "6233/6233 [==============================] - 1s 87us/sample - loss: 0.4040 - acc: 0.8099 - val_loss: 0.3679 - val_acc: 0.8398\n",
      "Epoch 404/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4001 - acc: 0.8041 - val_loss: 0.3677 - val_acc: 0.8369\n",
      "Epoch 405/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.4037 - acc: 0.7972 - val_loss: 0.3676 - val_acc: 0.8355\n",
      "Epoch 406/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.4055 - acc: 0.8116 - val_loss: 0.3674 - val_acc: 0.8384\n",
      "Epoch 407/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3952 - acc: 0.8100 - val_loss: 0.3668 - val_acc: 0.8369\n",
      "Epoch 408/2000\n",
      "6233/6233 [==============================] - 0s 78us/sample - loss: 0.4007 - acc: 0.8083 - val_loss: 0.3666 - val_acc: 0.8384\n",
      "Epoch 409/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.4001 - acc: 0.8185 - val_loss: 0.3664 - val_acc: 0.8384\n",
      "Epoch 410/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3974 - acc: 0.8115 - val_loss: 0.3665 - val_acc: 0.8384\n",
      "Epoch 411/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3993 - acc: 0.8133 - val_loss: 0.3665 - val_acc: 0.8398\n",
      "Epoch 412/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3937 - acc: 0.8173 - val_loss: 0.3666 - val_acc: 0.8384\n",
      "Epoch 413/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3937 - acc: 0.8144 - val_loss: 0.3661 - val_acc: 0.8384\n",
      "Epoch 414/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3949 - acc: 0.8096 - val_loss: 0.3659 - val_acc: 0.8384\n",
      "Epoch 415/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3995 - acc: 0.8038 - val_loss: 0.3655 - val_acc: 0.8384\n",
      "Epoch 416/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3936 - acc: 0.8099 - val_loss: 0.3653 - val_acc: 0.8384\n",
      "Epoch 417/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3941 - acc: 0.8158 - val_loss: 0.3651 - val_acc: 0.8384\n",
      "Epoch 418/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3872 - acc: 0.8141 - val_loss: 0.3647 - val_acc: 0.8413\n",
      "Epoch 419/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3993 - acc: 0.8038 - val_loss: 0.3643 - val_acc: 0.8398\n",
      "Epoch 420/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.4005 - acc: 0.8097 - val_loss: 0.3641 - val_acc: 0.8398\n",
      "Epoch 421/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3956 - acc: 0.8092 - val_loss: 0.3639 - val_acc: 0.8398\n",
      "Epoch 422/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3908 - acc: 0.8168 - val_loss: 0.3637 - val_acc: 0.8398\n",
      "Epoch 423/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3906 - acc: 0.8171 - val_loss: 0.3636 - val_acc: 0.8384\n",
      "Epoch 424/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3899 - acc: 0.8161 - val_loss: 0.3632 - val_acc: 0.8398\n",
      "Epoch 425/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3829 - acc: 0.8185 - val_loss: 0.3629 - val_acc: 0.8398\n",
      "Epoch 426/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3841 - acc: 0.8177 - val_loss: 0.3626 - val_acc: 0.8398\n",
      "Epoch 427/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3842 - acc: 0.8187 - val_loss: 0.3621 - val_acc: 0.8398\n",
      "Epoch 428/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3942 - acc: 0.8158 - val_loss: 0.3617 - val_acc: 0.8398\n",
      "Epoch 429/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3822 - acc: 0.8181 - val_loss: 0.3616 - val_acc: 0.8369\n",
      "Epoch 430/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3896 - acc: 0.8171 - val_loss: 0.3613 - val_acc: 0.8384\n",
      "Epoch 431/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3850 - acc: 0.8173 - val_loss: 0.3611 - val_acc: 0.8398\n",
      "Epoch 432/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3876 - acc: 0.8126 - val_loss: 0.3608 - val_acc: 0.8398\n",
      "Epoch 433/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3822 - acc: 0.8213 - val_loss: 0.3607 - val_acc: 0.8398\n",
      "Epoch 434/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3811 - acc: 0.8190 - val_loss: 0.3606 - val_acc: 0.8413\n",
      "Epoch 435/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3831 - acc: 0.8184 - val_loss: 0.3605 - val_acc: 0.8398\n",
      "Epoch 436/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3858 - acc: 0.8190 - val_loss: 0.3599 - val_acc: 0.8398\n",
      "Epoch 437/2000\n",
      "6233/6233 [==============================] - 1s 81us/sample - loss: 0.3745 - acc: 0.8279 - val_loss: 0.3597 - val_acc: 0.8413\n",
      "Epoch 438/2000\n",
      "6233/6233 [==============================] - 1s 82us/sample - loss: 0.3836 - acc: 0.8203 - val_loss: 0.3596 - val_acc: 0.8398\n",
      "Epoch 439/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3841 - acc: 0.8192 - val_loss: 0.3597 - val_acc: 0.8398\n",
      "Epoch 440/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3883 - acc: 0.8134 - val_loss: 0.3593 - val_acc: 0.8413\n",
      "Epoch 441/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3766 - acc: 0.8205 - val_loss: 0.3591 - val_acc: 0.8413\n",
      "Epoch 442/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3784 - acc: 0.8227 - val_loss: 0.3590 - val_acc: 0.8398\n",
      "Epoch 443/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3803 - acc: 0.8185 - val_loss: 0.3589 - val_acc: 0.8398\n",
      "Epoch 444/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3805 - acc: 0.8211 - val_loss: 0.3590 - val_acc: 0.8398\n",
      "Epoch 445/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3749 - acc: 0.8155 - val_loss: 0.3589 - val_acc: 0.8413\n",
      "Epoch 446/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3757 - acc: 0.8211 - val_loss: 0.3589 - val_acc: 0.8427\n",
      "Epoch 447/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3710 - acc: 0.8293 - val_loss: 0.3591 - val_acc: 0.8442\n",
      "Epoch 448/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3750 - acc: 0.8210 - val_loss: 0.3591 - val_acc: 0.8427\n",
      "Epoch 449/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3780 - acc: 0.8152 - val_loss: 0.3588 - val_acc: 0.8413\n",
      "Epoch 450/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3753 - acc: 0.8218 - val_loss: 0.3589 - val_acc: 0.8427\n",
      "Epoch 451/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3776 - acc: 0.8283 - val_loss: 0.3586 - val_acc: 0.8442\n",
      "Epoch 452/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3745 - acc: 0.8240 - val_loss: 0.3585 - val_acc: 0.8427\n",
      "Epoch 453/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3772 - acc: 0.8200 - val_loss: 0.3585 - val_acc: 0.8442\n",
      "Epoch 454/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3738 - acc: 0.8193 - val_loss: 0.3582 - val_acc: 0.8442\n",
      "Epoch 455/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3820 - acc: 0.8129 - val_loss: 0.3578 - val_acc: 0.8456\n",
      "Epoch 456/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3757 - acc: 0.8279 - val_loss: 0.3576 - val_acc: 0.8456\n",
      "Epoch 457/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3748 - acc: 0.8208 - val_loss: 0.3576 - val_acc: 0.8442\n",
      "Epoch 458/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3675 - acc: 0.8221 - val_loss: 0.3573 - val_acc: 0.8442\n",
      "Epoch 459/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3730 - acc: 0.8238 - val_loss: 0.3569 - val_acc: 0.8456\n",
      "Epoch 460/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3684 - acc: 0.8240 - val_loss: 0.3565 - val_acc: 0.8456\n",
      "Epoch 461/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3725 - acc: 0.8202 - val_loss: 0.3565 - val_acc: 0.8456\n",
      "Epoch 462/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3748 - acc: 0.8174 - val_loss: 0.3562 - val_acc: 0.8456\n",
      "Epoch 463/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3739 - acc: 0.8208 - val_loss: 0.3557 - val_acc: 0.8427\n",
      "Epoch 464/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3675 - acc: 0.8317 - val_loss: 0.3557 - val_acc: 0.8456\n",
      "Epoch 465/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3627 - acc: 0.8293 - val_loss: 0.3557 - val_acc: 0.8470\n",
      "Epoch 466/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3671 - acc: 0.8234 - val_loss: 0.3557 - val_acc: 0.8470\n",
      "Epoch 467/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3635 - acc: 0.8303 - val_loss: 0.3556 - val_acc: 0.8442\n",
      "Epoch 468/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3668 - acc: 0.8315 - val_loss: 0.3555 - val_acc: 0.8427\n",
      "Epoch 469/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3697 - acc: 0.8254 - val_loss: 0.3555 - val_acc: 0.8442\n",
      "Epoch 470/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3683 - acc: 0.8299 - val_loss: 0.3556 - val_acc: 0.8442\n",
      "Epoch 471/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3649 - acc: 0.8354 - val_loss: 0.3552 - val_acc: 0.8427\n",
      "Epoch 472/2000\n",
      "6233/6233 [==============================] - 1s 89us/sample - loss: 0.3647 - acc: 0.8360 - val_loss: 0.3553 - val_acc: 0.8398\n",
      "Epoch 473/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3657 - acc: 0.8205 - val_loss: 0.3551 - val_acc: 0.8398\n",
      "Epoch 474/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3607 - acc: 0.8261 - val_loss: 0.3548 - val_acc: 0.8413\n",
      "Epoch 475/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3596 - acc: 0.8362 - val_loss: 0.3549 - val_acc: 0.8384\n",
      "Epoch 476/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3634 - acc: 0.8280 - val_loss: 0.3549 - val_acc: 0.8384\n",
      "Epoch 477/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.3635 - acc: 0.8319 - val_loss: 0.3546 - val_acc: 0.8369\n",
      "Epoch 478/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3662 - acc: 0.8240 - val_loss: 0.3542 - val_acc: 0.8398\n",
      "Epoch 479/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3589 - acc: 0.8280 - val_loss: 0.3542 - val_acc: 0.8413\n",
      "Epoch 480/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3675 - acc: 0.8266 - val_loss: 0.3544 - val_acc: 0.8456\n",
      "Epoch 481/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3623 - acc: 0.8280 - val_loss: 0.3542 - val_acc: 0.8456\n",
      "Epoch 482/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3647 - acc: 0.8277 - val_loss: 0.3540 - val_acc: 0.8442\n",
      "Epoch 483/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3536 - acc: 0.8335 - val_loss: 0.3540 - val_acc: 0.8442\n",
      "Epoch 484/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3535 - acc: 0.8373 - val_loss: 0.3541 - val_acc: 0.8427\n",
      "Epoch 485/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3557 - acc: 0.8356 - val_loss: 0.3541 - val_acc: 0.8456\n",
      "Epoch 486/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3592 - acc: 0.8343 - val_loss: 0.3539 - val_acc: 0.8413\n",
      "Epoch 487/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3580 - acc: 0.8317 - val_loss: 0.3539 - val_acc: 0.8427\n",
      "Epoch 488/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3553 - acc: 0.8338 - val_loss: 0.3537 - val_acc: 0.8427\n",
      "Epoch 489/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3576 - acc: 0.8336 - val_loss: 0.3539 - val_acc: 0.8427\n",
      "Epoch 490/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3516 - acc: 0.8339 - val_loss: 0.3537 - val_acc: 0.8442\n",
      "Epoch 491/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3528 - acc: 0.8368 - val_loss: 0.3538 - val_acc: 0.8413\n",
      "Epoch 492/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3648 - acc: 0.8245 - val_loss: 0.3537 - val_acc: 0.8413\n",
      "Epoch 493/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3549 - acc: 0.8344 - val_loss: 0.3535 - val_acc: 0.8398\n",
      "Epoch 494/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3626 - acc: 0.8303 - val_loss: 0.3535 - val_acc: 0.8413\n",
      "Epoch 495/2000\n",
      "6233/6233 [==============================] - 0s 71us/sample - loss: 0.3474 - acc: 0.8391 - val_loss: 0.3536 - val_acc: 0.8398\n",
      "Epoch 496/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3439 - acc: 0.8434 - val_loss: 0.3536 - val_acc: 0.8442\n",
      "Epoch 497/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3532 - acc: 0.8373 - val_loss: 0.3539 - val_acc: 0.8427\n",
      "Epoch 498/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3534 - acc: 0.8372 - val_loss: 0.3538 - val_acc: 0.8442\n",
      "Epoch 499/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3469 - acc: 0.8410 - val_loss: 0.3535 - val_acc: 0.8456\n",
      "Epoch 500/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3572 - acc: 0.8317 - val_loss: 0.3530 - val_acc: 0.8470\n",
      "Epoch 501/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3489 - acc: 0.8325 - val_loss: 0.3526 - val_acc: 0.8470\n",
      "Epoch 502/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3511 - acc: 0.8349 - val_loss: 0.3524 - val_acc: 0.8456\n",
      "Epoch 503/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3405 - acc: 0.8457 - val_loss: 0.3526 - val_acc: 0.8485\n",
      "Epoch 504/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3519 - acc: 0.8364 - val_loss: 0.3528 - val_acc: 0.8485\n",
      "Epoch 505/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3482 - acc: 0.8338 - val_loss: 0.3529 - val_acc: 0.8485\n",
      "Epoch 506/2000\n",
      "6233/6233 [==============================] - 1s 81us/sample - loss: 0.3450 - acc: 0.8367 - val_loss: 0.3527 - val_acc: 0.8470\n",
      "Epoch 507/2000\n",
      "6233/6233 [==============================] - 1s 84us/sample - loss: 0.3446 - acc: 0.8431 - val_loss: 0.3530 - val_acc: 0.8470\n",
      "Epoch 508/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3403 - acc: 0.8441 - val_loss: 0.3531 - val_acc: 0.8456\n",
      "Epoch 509/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3411 - acc: 0.8434 - val_loss: 0.3531 - val_acc: 0.8456\n",
      "Epoch 510/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3521 - acc: 0.8356 - val_loss: 0.3531 - val_acc: 0.8456\n",
      "Epoch 511/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3442 - acc: 0.8384 - val_loss: 0.3528 - val_acc: 0.8456\n",
      "Epoch 512/2000\n",
      "6233/6233 [==============================] - 0s 79us/sample - loss: 0.3399 - acc: 0.8457 - val_loss: 0.3525 - val_acc: 0.8456\n",
      "Epoch 513/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3428 - acc: 0.8400 - val_loss: 0.3523 - val_acc: 0.8485\n",
      "Epoch 514/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3448 - acc: 0.8421 - val_loss: 0.3521 - val_acc: 0.8470\n",
      "Epoch 515/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3478 - acc: 0.8408 - val_loss: 0.3522 - val_acc: 0.8485\n",
      "Epoch 516/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3401 - acc: 0.8343 - val_loss: 0.3522 - val_acc: 0.8485\n",
      "Epoch 517/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3405 - acc: 0.8449 - val_loss: 0.3523 - val_acc: 0.8485\n",
      "Epoch 518/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3415 - acc: 0.8402 - val_loss: 0.3519 - val_acc: 0.8456\n",
      "Epoch 519/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3447 - acc: 0.8399 - val_loss: 0.3522 - val_acc: 0.8442\n",
      "Epoch 520/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3435 - acc: 0.8399 - val_loss: 0.3521 - val_acc: 0.8456\n",
      "Epoch 521/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3454 - acc: 0.8412 - val_loss: 0.3523 - val_acc: 0.8485\n",
      "Epoch 522/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3395 - acc: 0.8420 - val_loss: 0.3522 - val_acc: 0.8470\n",
      "Epoch 523/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3393 - acc: 0.8487 - val_loss: 0.3518 - val_acc: 0.8456\n",
      "Epoch 524/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3391 - acc: 0.8405 - val_loss: 0.3518 - val_acc: 0.8470\n",
      "Epoch 525/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3355 - acc: 0.8413 - val_loss: 0.3519 - val_acc: 0.8470\n",
      "Epoch 526/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3476 - acc: 0.8341 - val_loss: 0.3518 - val_acc: 0.8456\n",
      "Epoch 527/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3346 - acc: 0.8473 - val_loss: 0.3514 - val_acc: 0.8470\n",
      "Epoch 528/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3371 - acc: 0.8420 - val_loss: 0.3512 - val_acc: 0.8442\n",
      "Epoch 529/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3337 - acc: 0.8477 - val_loss: 0.3515 - val_acc: 0.8456\n",
      "Epoch 530/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3346 - acc: 0.8442 - val_loss: 0.3514 - val_acc: 0.8442\n",
      "Epoch 531/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3332 - acc: 0.8434 - val_loss: 0.3509 - val_acc: 0.8456\n",
      "Epoch 532/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3416 - acc: 0.8473 - val_loss: 0.3511 - val_acc: 0.8427\n",
      "Epoch 533/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3456 - acc: 0.8357 - val_loss: 0.3509 - val_acc: 0.8442\n",
      "Epoch 534/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3255 - acc: 0.8494 - val_loss: 0.3509 - val_acc: 0.8442\n",
      "Epoch 535/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3361 - acc: 0.8420 - val_loss: 0.3506 - val_acc: 0.8456\n",
      "Epoch 536/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3275 - acc: 0.8524 - val_loss: 0.3505 - val_acc: 0.8485\n",
      "Epoch 537/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3369 - acc: 0.8447 - val_loss: 0.3506 - val_acc: 0.8485\n",
      "Epoch 538/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3296 - acc: 0.8458 - val_loss: 0.3502 - val_acc: 0.8470\n",
      "Epoch 539/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3311 - acc: 0.8469 - val_loss: 0.3499 - val_acc: 0.8470\n",
      "Epoch 540/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.3265 - acc: 0.8519 - val_loss: 0.3500 - val_acc: 0.8470\n",
      "Epoch 541/2000\n",
      "6233/6233 [==============================] - 1s 87us/sample - loss: 0.3241 - acc: 0.8567 - val_loss: 0.3502 - val_acc: 0.8442\n",
      "Epoch 542/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3234 - acc: 0.8548 - val_loss: 0.3505 - val_acc: 0.8470\n",
      "Epoch 543/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3248 - acc: 0.8487 - val_loss: 0.3511 - val_acc: 0.8470\n",
      "Epoch 544/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3349 - acc: 0.8376 - val_loss: 0.3508 - val_acc: 0.8470\n",
      "Epoch 545/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3300 - acc: 0.8466 - val_loss: 0.3506 - val_acc: 0.8470\n",
      "Epoch 546/2000\n",
      "6233/6233 [==============================] - 0s 78us/sample - loss: 0.3300 - acc: 0.8458 - val_loss: 0.3507 - val_acc: 0.8470\n",
      "Epoch 547/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3252 - acc: 0.8510 - val_loss: 0.3513 - val_acc: 0.8470\n",
      "Epoch 548/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3281 - acc: 0.8468 - val_loss: 0.3515 - val_acc: 0.8470\n",
      "Epoch 549/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3201 - acc: 0.8495 - val_loss: 0.3517 - val_acc: 0.8499\n",
      "Epoch 550/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3244 - acc: 0.8535 - val_loss: 0.3519 - val_acc: 0.8485\n",
      "Epoch 551/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3229 - acc: 0.8463 - val_loss: 0.3520 - val_acc: 0.8470\n",
      "Epoch 552/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3249 - acc: 0.8500 - val_loss: 0.3518 - val_acc: 0.8485\n",
      "Epoch 553/2000\n",
      "6233/6233 [==============================] - 0s 72us/sample - loss: 0.3320 - acc: 0.8473 - val_loss: 0.3516 - val_acc: 0.8485\n",
      "Epoch 554/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3281 - acc: 0.8482 - val_loss: 0.3516 - val_acc: 0.8514\n",
      "Epoch 555/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3321 - acc: 0.8457 - val_loss: 0.3513 - val_acc: 0.8499\n",
      "Epoch 556/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3241 - acc: 0.8485 - val_loss: 0.3518 - val_acc: 0.8514\n",
      "Epoch 557/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3236 - acc: 0.8469 - val_loss: 0.3520 - val_acc: 0.8499\n",
      "Epoch 558/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3237 - acc: 0.8518 - val_loss: 0.3518 - val_acc: 0.8499\n",
      "Epoch 559/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3206 - acc: 0.8540 - val_loss: 0.3517 - val_acc: 0.8485\n",
      "Epoch 560/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3198 - acc: 0.8543 - val_loss: 0.3521 - val_acc: 0.8499\n",
      "Epoch 561/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3203 - acc: 0.8476 - val_loss: 0.3520 - val_acc: 0.8485\n",
      "Epoch 562/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3211 - acc: 0.8527 - val_loss: 0.3513 - val_acc: 0.8470\n",
      "Epoch 563/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3218 - acc: 0.8521 - val_loss: 0.3525 - val_acc: 0.8499\n",
      "Epoch 564/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3101 - acc: 0.8571 - val_loss: 0.3525 - val_acc: 0.8499\n",
      "Epoch 565/2000\n",
      "6233/6233 [==============================] - 0s 77us/sample - loss: 0.3120 - acc: 0.8619 - val_loss: 0.3524 - val_acc: 0.8514\n",
      "Epoch 566/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3119 - acc: 0.8526 - val_loss: 0.3519 - val_acc: 0.8514\n",
      "Epoch 567/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3123 - acc: 0.8534 - val_loss: 0.3519 - val_acc: 0.8499\n",
      "Epoch 568/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3212 - acc: 0.8510 - val_loss: 0.3516 - val_acc: 0.8470\n",
      "Epoch 569/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3150 - acc: 0.8580 - val_loss: 0.3515 - val_acc: 0.8470\n",
      "Epoch 570/2000\n",
      "6233/6233 [==============================] - 0s 73us/sample - loss: 0.3114 - acc: 0.8609 - val_loss: 0.3523 - val_acc: 0.8470\n",
      "Epoch 571/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3247 - acc: 0.8492 - val_loss: 0.3515 - val_acc: 0.8470\n",
      "Epoch 572/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3149 - acc: 0.8569 - val_loss: 0.3516 - val_acc: 0.8470\n",
      "Epoch 573/2000\n",
      "6233/6233 [==============================] - 0s 75us/sample - loss: 0.3140 - acc: 0.8558 - val_loss: 0.3521 - val_acc: 0.8456\n",
      "Epoch 574/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3203 - acc: 0.8461 - val_loss: 0.3519 - val_acc: 0.8485\n",
      "Epoch 575/2000\n",
      "6233/6233 [==============================] - 1s 87us/sample - loss: 0.3173 - acc: 0.8516 - val_loss: 0.3521 - val_acc: 0.8499\n",
      "Epoch 576/2000\n",
      "6233/6233 [==============================] - 0s 78us/sample - loss: 0.3109 - acc: 0.8561 - val_loss: 0.3523 - val_acc: 0.8499\n",
      "Epoch 577/2000\n",
      "6233/6233 [==============================] - 0s 74us/sample - loss: 0.3162 - acc: 0.8510 - val_loss: 0.3522 - val_acc: 0.8499\n",
      "Epoch 578/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3150 - acc: 0.8551 - val_loss: 0.3526 - val_acc: 0.8485\n",
      "Epoch 579/2000\n",
      "6233/6233 [==============================] - 0s 76us/sample - loss: 0.3217 - acc: 0.8484 - val_loss: 0.3529 - val_acc: 0.8499\n"
     ]
    }
   ],
   "source": [
    "#history = model.fit(x=X_train, y=y_train, batch_size=32, epochs=400, verbose=1, validation_split=0.1)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40)\n",
    "history = model.fit(x=[X1_train, X2_train], y=y_train, batch_size=32, epochs=2000, verbose=1, validation_split=0.1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/770 [==============================] - 0s 56us/sample - loss: 0.4335 - acc: 0.8312\n",
      "Test Score: 0.4335043683454588\n",
      "Test ACC: 0.83116883\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test ACC:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
