{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa: E265\n",
    "from pathlib import Path\n",
    "\n",
    "import click\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import applications as keras_applications\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras_fsl.dataframe.operators import ToKShotDataset\n",
    "from keras_fsl.layers import Classification, GramMatrix\n",
    "from keras_fsl.losses import ClippedBinaryCrossentropy, class_consistency_loss, MaxBinaryCrossentropy, StdBinaryCrossentropy\n",
    "from keras_fsl.metrics import accuracy, classification_accuracy, same_image_score\n",
    "\n",
    "\n",
    "#%% Toggle some config if required\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "# tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "\n",
    "#%% CLI args\n",
    "@click.option(\"--base_dir\", help=\"Base directory for the training\", type=Path, default=\"\")\n",
    "@click.command()\n",
    "def train(base_dir):\n",
    "    #%% Init model\n",
    "    encoder = keras_applications.Xception(input_shape=(299, 299, 3), include_top=True, pooling=\"avg\")\n",
    "    support_layer = GramMatrix(\n",
    "        kernel={\n",
    "            \"name\": \"MixedNorms\",\n",
    "            \"init\": {\n",
    "                \"norms\": [\n",
    "                    lambda x: 1 - tf.nn.l2_normalize(x[0]) * tf.nn.l2_normalize(x[1]),\n",
    "                    lambda x: tf.math.abs(x[0] - x[1]),\n",
    "                    lambda x: tf.nn.softmax(tf.math.abs(x[0] - x[1])),\n",
    "                    lambda x: tf.square(x[0] - x[1]),\n",
    "                ],\n",
    "                \"use_bias\": True,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    model = Sequential([encoder, support_layer])\n",
    "\n",
    "    #%% Init training\n",
    "    callbacks = [\n",
    "        TensorBoard(base_dir, write_images=True, histogram_freq=1),\n",
    "        ModelCheckpoint(str(base_dir / \"best_loss.h5\"), save_best_only=True),\n",
    "        ReduceLROnPlateau(),\n",
    "    ]\n",
    "\n",
    "    #%% Init data\n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[None, None, 3], dtype=tf.uint8),))\n",
    "    def preprocessing(input_tensor):\n",
    "        output_tensor = tf.cast(input_tensor, dtype=tf.float32)\n",
    "        output_tensor = tf.image.resize_with_pad(output_tensor, target_height=299, target_width=299)\n",
    "        output_tensor = keras_applications.mobilenet.preprocess_input(output_tensor, data_format=\"channels_last\")\n",
    "        return output_tensor\n",
    "\n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[None, None, 3], dtype=tf.float32),))\n",
    "    def data_augmentation(input_tensor):\n",
    "        output_tensor = tf.image.random_flip_left_right(input_tensor)\n",
    "        output_tensor = tf.image.random_flip_up_down(output_tensor)\n",
    "        output_tensor = tf.image.random_brightness(output_tensor, max_delta=0.25)\n",
    "        return preprocessing(output_tensor)\n",
    "\n",
    "    all_annotations = pd.read_csv(base_dir / \"annotations\" / \"all_annotations.csv\")\n",
    "    class_count = all_annotations.groupby(\"split\").apply(lambda group: group.label.value_counts())\n",
    "\n",
    "    #%% Train model\n",
    "    margin = 0.05\n",
    "    k_shot = 11\n",
    "    cache = base_dir / \"cache\"\n",
    "    datasets = {\n",
    "        split: all_annotations.loc[lambda df: df.split == split].pipe(\n",
    "            ToKShotDataset(k_shot=k_shot, preprocessing=data_augmentation, cache=str(cache / split), reset_cache=False)\n",
    "        )\n",
    "        for split in set(all_annotations.split)\n",
    "    }\n",
    "\n",
    "    batch_size = 8\n",
    "    encoder.trainable = False\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=class_consistency_loss,\n",
    "        metrics=[\n",
    "            accuracy(margin),\n",
    "            ClippedBinaryCrossentropy(),\n",
    "            MaxBinaryCrossentropy(),\n",
    "            StdBinaryCrossentropy(),\n",
    "            same_image_score,\n",
    "            classification_accuracy(),\n",
    "        ],\n",
    "    )\n",
    "    model.fit(\n",
    "        datasets[\"train\"].batch(batch_size).repeat(),\n",
    "        steps_per_epoch=len(class_count[\"train\"]) * k_shot // batch_size * 150,\n",
    "        validation_data=datasets[\"val\"].batch(batch_size).repeat(),\n",
    "        validation_steps=max(len(class_count[\"val\"]) * k_shot // batch_size, 100),\n",
    "        initial_epoch=0,\n",
    "        epochs=3,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    encoder.trainable = True\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=class_consistency_loss,\n",
    "        metrics=[\n",
    "            accuracy(margin),\n",
    "            ClippedBinaryCrossentropy(),\n",
    "            MaxBinaryCrossentropy(),\n",
    "            StdBinaryCrossentropy(),\n",
    "            same_image_score,\n",
    "            classification_accuracy(),\n",
    "        ],\n",
    "    )\n",
    "    model.fit(\n",
    "        datasets[\"train\"].batch(batch_size).repeat(),\n",
    "        steps_per_epoch=len(class_count[\"train\"]) * k_shot // batch_size * 150,\n",
    "        validation_data=datasets[\"val\"].batch(batch_size).repeat(),\n",
    "        validation_steps=max(len(class_count[\"val\"]) * k_shot // batch_size, 100),\n",
    "        initial_epoch=3,\n",
    "        epochs=30,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    #%% Evaluate on test set. Each batch is a k_shot, n_way=batch_size / k_shot task\n",
    "    model.load_weights(str(base_dir / \"best_loss.h5\"))\n",
    "    model.evaluate(\n",
    "        datasets[\"test\"].batch(batch_size).repeat(), steps=max(len(class_count[\"test\"]) * k_shot // batch_size, 100)\n",
    "    )\n",
    "\n",
    "    #%% Export artifacts\n",
    "    classifier = Sequential([encoder, Classification(support_layer.kernel)])\n",
    "    tf.saved_model.save(classifier, \"siamese_nets_classifier/1\", signatures={\"preprocessing\": preprocessing})\n",
    "\n",
    "\n",
    "#%% Run command\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import click\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import applications as keras_applications\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras_fsl.dataframe.operators import ToKShotDataset\n",
    "from keras_fsl.layers import Classification, GramMatrix\n",
    "from keras_fsl.losses import ClippedBinaryCrossentropy, class_consistency_loss, MaxBinaryCrossentropy, StdBinaryCrossentropy\n",
    "from keras_fsl.metrics import accuracy, classification_accuracy, same_image_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(base_dir):\n",
    "    #%% Init model\n",
    "    encoder = keras_applications.Xception(input_shape=(299, 299, 3), include_top=True, pooling=\"avg\")\n",
    "    support_layer = GramMatrix(\n",
    "        kernel={\n",
    "            \"name\": \"MixedNorms\",\n",
    "            \"init\": {\n",
    "                \"norms\": [\n",
    "                    lambda x: 1 - tf.nn.l2_normalize(x[0]) * tf.nn.l2_normalize(x[1]),\n",
    "                    lambda x: tf.math.abs(x[0] - x[1]),\n",
    "                    lambda x: tf.nn.softmax(tf.math.abs(x[0] - x[1])),\n",
    "                    lambda x: tf.square(x[0] - x[1]),\n",
    "                ],\n",
    "                \"use_bias\": True,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    model = Sequential([encoder, support_layer])\n",
    "\n",
    "    #%% Init training\n",
    "    callbacks = [\n",
    "        TensorBoard(base_dir, write_images=True, histogram_freq=1),\n",
    "        ModelCheckpoint(str(base_dir / \"best_loss.h5\"), save_best_only=True),\n",
    "        ReduceLROnPlateau(),\n",
    "    ]\n",
    "    \n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[None, None, 3], dtype=tf.uint8),))\n",
    "    def preprocessing(input_tensor):\n",
    "        output_tensor = tf.cast(input_tensor, dtype=tf.float32)\n",
    "        output_tensor = tf.image.resize_with_pad(output_tensor, target_height=299, target_width=299)\n",
    "        output_tensor = keras_applications.mobilenet.preprocess_input(output_tensor, data_format=\"channels_last\")\n",
    "        return output_tensor\n",
    "    \n",
    "    \n",
    "    batch_size = 8\n",
    "    encoder.trainable = False\n",
    "    optimizer = Adam(lr=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=class_consistency_loss,\n",
    "        metrics=[\n",
    "            accuracy(margin),\n",
    "            ClippedBinaryCrossentropy(),\n",
    "            MaxBinaryCrossentropy(),\n",
    "            StdBinaryCrossentropy(),\n",
    "            same_image_score,\n",
    "            classification_accuracy(),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.)\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.)\n",
    "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "    train_set = train_datagen.flow_from_directory('image8/train',\n",
    "                                                     target_size = (299, 299),\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     class_mode = 'categorical')\n",
    "\n",
    "    val_set = val_datagen.flow_from_directory('image8/val',\n",
    "                                                     target_size = (299, 299),\n",
    "                                                     batch_size = batch_size,\n",
    "                                                     class_mode = 'categorical')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.fit_generator(\n",
    "        train_set,\n",
    "        steps_per_epoch= 4742 // batch_size,\n",
    "        validation_data=val_set,\n",
    "        validation_steps=527 // batch_size,\n",
    "        initial_epoch=0,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    encoder.trainable = True\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=class_consistency_loss,\n",
    "        metrics=[\n",
    "            accuracy(margin),\n",
    "            ClippedBinaryCrossentropy(),\n",
    "            MaxBinaryCrossentropy(),\n",
    "            StdBinaryCrossentropy(),\n",
    "            same_image_score,\n",
    "            classification_accuracy(),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    model.fit_generator(\n",
    "        train_set,\n",
    "        steps_per_epoch= 4742 // batch_size,\n",
    "        validation_data=val_set,\n",
    "        validation_steps=527 // batch_size,\n",
    "        initial_epoch=3,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    #%% Evaluate on test set. Each batch is a k_shot, n_way=batch_size / k_shot task\n",
    "    model.load_weights(str(base_dir / \"best_loss.h5\"))\n",
    "    model.evaluate(\n",
    "        datasets[\"test\"].batch(batch_size).repeat(), steps=max(len(class_count[\"test\"]) * k_shot // batch_size, 100)\n",
    "    )\n",
    "\n",
    "    #%% Export artifacts\n",
    "    classifier = Sequential([encoder, Classification(support_layer.kernel)])\n",
    "    tf.saved_model.save(classifier, \"siamese_nets_classifier/1\", signatures={\"preprocessing\": preprocessing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(r\"C:\\Users\\A\\Documents\\GitHub\\scdc2020-cck\\fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs.models import head_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fs.models.head_models' (namespace)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
